Traceback (most recent call last):
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/nbclient/client.py", line 1263, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/nbclient/util.py", line 85, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/nbclient/util.py", line 60, in just_run
    return loop.run_until_complete(coro)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/nbclient/client.py", line 701, in async_execute
    await self.async_execute_cell(
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/nbclient/client.py", line 1019, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/home/danbot/Documents/code/data_analysis/lib/python3.10/site-packages/nbclient/client.py", line 913, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# the 'process' variable is here so jupyter doesn't go computing 
# a million rows per iteration when the book is built for pushing to github pages.  

process = True
if process:
    for use_partial_counts in partial_counts[::-1]:
        for bitrate in bitrates:
            print(f'Processing pairs at {bitrate} bits quantization (concurrent data={use_partial_counts})')
            results_fname = f'DKL_results_{bitrate}bits_{revision_date}.csv'
            if use_partial_counts == True:
                results_fname = results_fname.replace('.csv', '_partial_counts.csv')

            out_fpath = os.path.join('data/', results_fname)
            existing_results = dpf.check_processed_results(out_fpath)

            if existing_results.empty:
                id_pairs_filtered = id_pairs
            else:
                id_pairs_filtered = dpf.filter_processed_pairs(existing_results, id_pairs)
                print(f'    {len(existing_results)} existing results loaded.')
            
            # define the input array for multiprocessing
            inputs = [
                (
                    proxy_stn, target_stn, bitrate, completeness_threshold, 
                    min_years, use_partial_counts, attr_cols, climate_cols, 
                    pseudo_counts
                ) for proxy_stn, target_stn in id_pairs_filtered
            ]

            batch_files = dpf.process_pairwise_comparisons(inputs, bitrate, results_fname, batch_size)

            print(f'    Processed {len(sample_pairs)} pairs at ({bitrate} bits) in {time() - t0:.1f} seconds')
            print(f'    Concatenating {len(batch_files)} batch files.')

            if len(batch_files) > 0:
                all_results = pd.concat([pd.read_csv(f, engine='pyarrow') for f in batch_files], axis=0)
                all_results.to_csv(out_fpath, index=False)
                if os.path.exists(out_fpath):
                    for f in batch_files:
                        os.remove(f)
                print(f'    Wrote {len(all_results)} results to {out_fpath}')
            else:
                print('    No new results to write to file.')
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRemoteTraceback[0m                           Traceback (most recent call last)
[0;31mRemoteTraceback[0m: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib/python3.10/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/home/danbot/Documents/code/24/divergence_measures/docs/notebooks/data_processing_functions.py", line 1671, in process_batch
    result["nested_catchments"] = check_if_nested(
  File "/home/danbot/Documents/code/24/divergence_measures/docs/notebooks/data_processing_functions.py", line 1034, in check_if_nested
    print(asdf)
NameError: name 'asdf' is not defined
"""

The above exception was the direct cause of the following exception:

[0;31mNameError[0m                                 Traceback (most recent call last)
Cell [0;32mIn [16], line 31[0m
[1;32m     22[0m [38;5;66;03m# define the input array for multiprocessing[39;00m
[1;32m     23[0m inputs [38;5;241m=[39m [
[1;32m     24[0m     (
[1;32m     25[0m         proxy_stn, target_stn, bitrate, completeness_threshold, 
[0;32m   (...)[0m
[1;32m     28[0m     ) [38;5;28;01mfor[39;00m proxy_stn, target_stn [38;5;129;01min[39;00m id_pairs_filtered
[1;32m     29[0m ]
[0;32m---> 31[0m batch_files [38;5;241m=[39m [43mdpf[49m[38;5;241;43m.[39;49m[43mprocess_pairwise_comparisons[49m[43m([49m[43minputs[49m[43m,[49m[43m [49m[43mbitrate[49m[43m,[49m[43m [49m[43mresults_fname[49m[43m,[49m[43m [49m[43mbatch_size[49m[43m)[49m
[1;32m     33[0m [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m'[39m[38;5;124m    Processed [39m[38;5;132;01m{[39;00m[38;5;28mlen[39m(sample_pairs)[38;5;132;01m}[39;00m[38;5;124m pairs at ([39m[38;5;132;01m{[39;00mbitrate[38;5;132;01m}[39;00m[38;5;124m bits) in [39m[38;5;132;01m{[39;00mtime()[38;5;250m [39m[38;5;241m-[39m[38;5;250m [39mt0[38;5;132;01m:[39;00m[38;5;124m.1f[39m[38;5;132;01m}[39;00m[38;5;124m seconds[39m[38;5;124m'[39m)
[1;32m     34[0m [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m'[39m[38;5;124m    Concatenating [39m[38;5;132;01m{[39;00m[38;5;28mlen[39m(batch_files)[38;5;132;01m}[39;00m[38;5;124m batch files.[39m[38;5;124m'[39m)

File [0;32m~/Documents/code/24/divergence_measures/docs/notebooks/data_processing_functions.py:993[0m, in [0;36mprocess_pairwise_comparisons[0;34m(inputs, bitrate, out_fname, batch_size)[0m
[1;32m    987[0m [38;5;66;03m# results = [][39;00m
[1;32m    988[0m [38;5;66;03m# for batch_inputs in batch[:2]:[39;00m
[1;32m    989[0m [38;5;66;03m#     result = process_batch(batch_inputs)[39;00m
[1;32m    990[0m [38;5;66;03m#     results.append(result)[39;00m
[1;32m    992[0m [38;5;28;01mwith[39;00m mp[38;5;241m.[39mPool([38;5;241m4[39m) [38;5;28;01mas[39;00m pool:
[0;32m--> 993[0m    results [38;5;241m=[39m [43mpool[49m[38;5;241;43m.[39;49m[43mmap[49m[43m([49m[43mprocess_batch[49m[43m,[49m[43m [49m[43mbatch[49m[43m)[49m
[1;32m    994[0m    results [38;5;241m=[39m [r [38;5;28;01mfor[39;00m r [38;5;129;01min[39;00m results [38;5;28;01mif[39;00m r [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m]
[1;32m    996[0m [38;5;28;01mif[39;00m [38;5;28mlen[39m(results) [38;5;241m==[39m [38;5;241m0[39m:

File [0;32m/usr/lib/python3.10/multiprocessing/pool.py:367[0m, in [0;36mPool.map[0;34m(self, func, iterable, chunksize)[0m
[1;32m    362[0m [38;5;28;01mdef[39;00m [38;5;21mmap[39m([38;5;28mself[39m, func, iterable, chunksize[38;5;241m=[39m[38;5;28;01mNone[39;00m):
[1;32m    363[0m [38;5;250m    [39m[38;5;124;03m'''[39;00m
[1;32m    364[0m [38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results[39;00m
[1;32m    365[0m [38;5;124;03m    in a list that is returned.[39;00m
[1;32m    366[0m [38;5;124;03m    '''[39;00m
[0;32m--> 367[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_map_async[49m[43m([49m[43mfunc[49m[43m,[49m[43m [49m[43miterable[49m[43m,[49m[43m [49m[43mmapstar[49m[43m,[49m[43m [49m[43mchunksize[49m[43m)[49m[38;5;241;43m.[39;49m[43mget[49m[43m([49m[43m)[49m

File [0;32m/usr/lib/python3.10/multiprocessing/pool.py:774[0m, in [0;36mApplyResult.get[0;34m(self, timeout)[0m
[1;32m    772[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_value
[1;32m    773[0m [38;5;28;01melse[39;00m:
[0;32m--> 774[0m     [38;5;28;01mraise[39;00m [38;5;28mself[39m[38;5;241m.[39m_value

[0;31mNameError[0m: name 'asdf' is not defined
NameError: name 'asdf' is not defined

