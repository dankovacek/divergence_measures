{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec6f101-baba-407b-bbd0-da0b48b55d58",
   "metadata": {},
   "source": [
    "# Process Target Variables\n",
    "\n",
    "After updating the catchment attributes using revised catchment bounds from USGS and WSC where available, or delineating them from processed rasters where not available, the target variables are the last input data to be processed before we can train the gradient boosted decision tree (GBDT) models to take in attributes and predict the various target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15319a-251b-4963-8a52-c40154aae3e3",
   "metadata": {},
   "source": [
    "## Shannon entropy processing\n",
    "\n",
    "Compute the Shannon entropy of individual streamflow time series.  The Shannon entropy is given by: \n",
    "\n",
    "$$H(X) = \\sum P \\log P$$\n",
    "\n",
    "The entropy is computed for various quantization bit depths (`bitrate` parameter).  No prior is applied here.\n",
    "\n",
    "From the scipy.stats [docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html):\n",
    ">*\"If messages consisting of sequences of symbols from a set are to be encoded and transmitted over a noiseless channel, then the Shannon entropy H(pk) gives a tight lower bound for the average number of units of information [bits] needed per symbol if the symbols occur with frequencies governed by the discrete distribution pk.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c41fda-caf4-4e8c-8ff8-e3d4ded409f0",
   "metadata": {},
   "source": [
    "Let's run through an example computation to see the difference between 4, 6, and 8 bit quantization, how each represents the total measurement range, and how each quantization aligns with your own expectation of heteroscedastic rating curve uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7cb74-1b3a-448d-be6e-50656934d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from scipy.stats import entropy\n",
    "import data_processing_functions as dpf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# visualize the catchment centroid locations\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import Colorblind, Sunset10\n",
    "output_notebook()\n",
    "\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9c7d2-7159-46a2-9bdf-e561930c9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "stn = '05010500'\n",
    "test_df = dpf.get_timeseries_data(stn)\n",
    "\n",
    "test_fig = figure(title=f'Sample Distribution by Dictionary Size (Station ID {stn})',\n",
    "                 width=800, height=300)\n",
    "n = 0\n",
    "for b in [4, 6, 8, 10, 12]:\n",
    "    test_df.dropna(subset=[stn], inplace=True)\n",
    "    # add a very small margin on the range to ensure the values are contained \n",
    "    # within the specified dictionary size because the right edge is closed\n",
    "    # by default and will return 2**b + 1 for values equal to the max\n",
    "    min_q, max_q = test_df[stn].min() - 1e-9, test_df[stn].max() + 1e-9\n",
    "    assert min_q > 0\n",
    "    # use equal width bins in log10 space\n",
    "    log_edges = np.linspace(np.log10(min_q), np.log10(max_q), 2**b)\n",
    "    linear_edges = [10**e for e in log_edges]\n",
    "    test_df[f'{b}_bits_quantized'] = np.digitize(test_df[stn], linear_edges)\n",
    "    unique, counts = np.unique(test_df[f'{b}_bits_quantized'], return_counts=True)\n",
    "    count_dict = {k: 1/v for k, v in zip(unique, counts)}\n",
    "    frequencies = [count_dict[e] if e in count_dict else 0 for e in range(1, 2**b)]\n",
    "    normed_frequencies = frequencies / sum(frequencies)\n",
    "    H = entropy(normed_frequencies, base=2)\n",
    "    bin_midpoints = (linear_edges[1:] + linear_edges[-1]) / 2\n",
    "    bottoms = [0 for _ in normed_frequencies]\n",
    "    test_fig.quad(left=linear_edges[:-1], right=linear_edges[1:], top=normed_frequencies, bottom=bottoms, \n",
    "                  legend_label=f'{b} bits (H={H:.2f})', color=Sunset10[n], fill_alpha=0.5)\n",
    "    \n",
    "    test_fig.xaxis.axis_label = r'$$\\text{Log}_{10}\\text{Flow} \\left[ m^3/s \\right]$$'\n",
    "    test_fig.yaxis.axis_label = r'P(X)'\n",
    "    test_fig.legend.location = 'top_left'\n",
    "    test_fig.legend.click_policy = 'hide'\n",
    "    n += 2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2502c-0ea7-4f5f-bb45-2b962cfa0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(test_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49e95e-c8e6-48dd-ad74-226926358051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new output filename \n",
    "attributes_filename = 'BCUB_watershed_attributes_updated.csv'\n",
    "attributes_fpath = os.path.join('data', attributes_filename)\n",
    "df = pd.read_csv(attributes_fpath)\n",
    "df.columns = [e.lower() for e in df.columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095bd89-dc23-4d99-a49c-af8a3710cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bitrate dictates the number of quantization levels = 2**b, i.e. 4 bits = 16 levels\n",
    "quant_labels = []\n",
    "filename = 'BCUB_watershed_attributes_updated.csv'\n",
    "entropy_fpath = os.path.join(BASE_DIR, 'data', 'processed_divergence_inputs', attributes_filename)\n",
    "if not os.path.exists(entropy_fpath):\n",
    "    bitrates = [4, 6, 8, 9, 10, 11, 12]\n",
    "    for bitrate in bitrates:\n",
    "        label = f'H_{bitrate}_bits'\n",
    "        print(f'Processing {bitrate} bit entropy')\n",
    "        df[label] = df.apply(lambda row: dpf.compute_observed_series_entropy(row, bitrate), axis=1)\n",
    "        quant_labels.append(label)\n",
    "    # save the results\n",
    "    df.to_csv(entropy_fpath, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(entropy_fpath)\n",
    "    quant_labels = [e for e in df.columns if e.startswith('H')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cee928-210f-4637-ba56-e6a37883073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the CDFs of entropy by quantization\n",
    "fig = figure(width=600, height=350, x_axis_label=r'$$H(X)$$', y_axis_label=r'$$P(H)$$')\n",
    "n = 0\n",
    "for l in quant_labels:\n",
    "    x, y = dpf.compute_cdf(df[l])\n",
    "    fig.line(x, y, legend_label=' '.join(l.split('_')[1:]), line_width=2, color=Colorblind[len(quant_labels)][n])\n",
    "    n += 1\n",
    "fig.legend.location = 'top_left'\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a6e20-f505-4b9e-bf25-b9432b2ce388",
   "metadata": {},
   "source": [
    "## Pairwise f-divergence processing\n",
    "\n",
    "There are about 1.3 million pairings.  To speed up the processing and avoid losing progress, we process these in parallel in batches and save the results intermittently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919715e-91cd-4e99-846a-514036fa69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "unique_stations = list(set(df['official_id'].values))\n",
    "# generate all combinations of pairs of station ids\n",
    "id_pairs = list(itertools.combinations(unique_stations, 2))\n",
    "print(f' There are {len(id_pairs)} unique pairings in the dataset')\n",
    "# shuffle the pairs to make testing smaller batches more robust\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(id_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da8cba-f12e-418d-aa7a-57b95d408832",
   "metadata": {},
   "source": [
    "### Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d74c8-47f1-48d0-967f-42593ba088d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, the observation counts will incorporate observation error\n",
    "# and divide observation counts based on proportion of bin covered \n",
    "# by the error range\n",
    "partial_counts = [False, True]\n",
    "# set a revision date for the results output file\n",
    "revision_date = '20240812'\n",
    "\n",
    "# how many pairs to compute in each batch\n",
    "batch_size = 5000\n",
    "\n",
    "# what percentage of 365 observations in a year counts as a \"complete\" year\n",
    "completeness_threshold = 0.9\n",
    "min_observations = 365 * 0.9\n",
    "\n",
    "# station pairs with less than min_years concurrent years of data are excluded (for concurrent analysis),\n",
    "# stations with less than min_years are excluded (for non-concurrent analysis),\n",
    "min_years = 1 #[2, 3, 4, 5, 10]\n",
    "\n",
    "# a prior is applied to q in the form of a uniform array of 10**c pseudo-counts \"c\"\n",
    "# this prior is used to test the effect of the choice of prior on the model\n",
    "pseudo_counts = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# set the number of quantization levels to test, equal to 2^bitrate\n",
    "bitrates = [4, 6, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ce523-2c3e-4d57-a008-b9a45e724f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the attributes file with catchment geometries\n",
    "geom_file = 'BCUB_watershed_attributes_updated.geojson'\n",
    "bcub_gdf = gpd.read_file(os.path.join(os.getcwd(), 'data', geom_file))\n",
    "bcub_gdf.columns = [c.lower() for c in bcub_gdf.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9924d-9b0e-4c65-ae76-9228955ecfd1",
   "metadata": {},
   "source": [
    "Review, organize, and separate the attribute and metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65949aea-dff3-4549-85f0-f0775655d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_cols = [\n",
    "    'drainage_area_km2', 'elevation_m', 'slope_deg', 'aspect_deg', \n",
    "    'land_use_forest_frac_2010','land_use_grass_frac_2010', 'land_use_wetland_frac_2010',\n",
    "    'land_use_water_frac_2010', 'land_use_urban_frac_2010', 'land_use_shrubs_frac_2010', \n",
    "    'land_use_crops_frac_2010', 'land_use_snow_ice_frac_2010', 'logk_ice_x100', 'porosity_x100'\n",
    "]\n",
    "\n",
    "climate_cols = [\n",
    "    'tmax', 'tmin', 'prcp', 'srad', 'swe', 'vp', \n",
    "    'high_prcp_freq', 'low_prcp_freq', 'high_prcp_duration', 'low_prcp_duration',\n",
    "]\n",
    "\n",
    "flag_cols = ['flag_shape_extraction', 'flag_terrain_extraction', 'flag_subsoil_extraction', 'flag_gsim_boundaries', 'flag_artificial_boundaries', 'flag_land_use_extraction']\n",
    "metadata_cols = [e for e in df.columns if e not in climate_cols + attr_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227acc48-47a5-45cd-a148-d28ca53ac234",
   "metadata": {},
   "source": [
    "## Process the data \n",
    "\n",
    "\n",
    "```{note}\n",
    "This step is very time consuming, you can skip by downloading the processed files as described at the [top of the page](https://dankovacek.github.io/divergence_measures/notebooks/1_data.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e244e35-4c54-4ec4-8882-5f6d09b490ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_batch_generator(df, id_pairs_filtered, bitrate, completeness_threshold, \n",
    "                    min_years, use_partial_counts, attr_cols, climate_cols, pseudo_counts):\n",
    "    batch_inputs = []\n",
    "    for proxy, target in id_pairs_filtered:\n",
    "        proxy_dict = bcub_gdf.loc[bcub_gdf['official_id'] == proxy].to_dict(orient='records')[0]\n",
    "        target_dict = bcub_gdf.loc[bcub_gdf['official_id'] == target].to_dict(orient='records')[0]\n",
    "\n",
    "        batch = [\n",
    "            proxy_dict, \n",
    "            target_dict, \n",
    "            bitrate, completeness_threshold, \n",
    "            min_years, use_partial_counts, attr_cols, climate_cols, \n",
    "            pseudo_counts\n",
    "        ]\n",
    "        batch_inputs.append(batch)\n",
    "    return batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f237d66-6881-47cf-b44f-dc174a335411",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(os.getcwd(), 'data/', 'temp')\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0987c-0b4a-4726-9498-93e626d52995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the 'process' variable is here so jupyter doesn't go computing \n",
    "# a million rows per iteration when the book is built for pushing to github pages.  \n",
    "process = True\n",
    "if process:\n",
    "    for use_partial_counts in partial_counts:\n",
    "        for bitrate in bitrates:\n",
    "            print(f'Processing pairs at {bitrate} bits quantization (concurrent data={use_partial_counts})')\n",
    "            results_fname = f'KL_results_{bitrate}bits_{revision_date}.csv'\n",
    "            if use_partial_counts == True:\n",
    "                results_fname = results_fname.replace('.csv', '_partial_counts.csv')\n",
    "\n",
    "            out_fpath = os.path.join('data/', 'processed_divergence_inputs', results_fname)\n",
    "            existing_results = dpf.check_processed_results(out_fpath)\n",
    "\n",
    "            if existing_results.empty:\n",
    "                id_pairs_filtered = id_pairs\n",
    "            else:\n",
    "                id_pairs_filtered = dpf.filter_processed_pairs(existing_results, id_pairs)\n",
    "                print(f'    {len(existing_results)} existing results loaded.')\n",
    "\n",
    "            # set some number of batches to create inputs for multiprocessing\n",
    "            n_batches = max(len(id_pairs_filtered) // batch_size, 1)\n",
    "            batches = np.array_split(np.array(id_pairs_filtered, dtype=object), n_batches)\n",
    "\n",
    "            n_pairs = len(id_pairs_filtered)\n",
    "            print(\n",
    "                f\"    Processing {n_pairs} pairs in {n_batches} batches at {bitrate} bits (partial_counts={use_partial_counts})\"\n",
    "            )\n",
    "            batch_no = 1\n",
    "            batch_files = []\n",
    "            t0 = time()\n",
    "            for batch_ids in batches:\n",
    "                print(f'Starting batch {batch_no}/{len(batches)} processing.')\n",
    "                batch_fname = results_fname.replace('.csv', f'_batch_{batch_no:03d}.csv')\n",
    "                batch_output_fpath = os.path.join(temp_dir, batch_fname)\n",
    "                if os.path.exists(batch_output_fpath):\n",
    "                    batch_files.append(batch_output_fpath)\n",
    "                    batch_no += 1\n",
    "                    continue\n",
    "                \n",
    "                # define the input array for multiprocessing\n",
    "                inputs = input_batch_generator(bcub_gdf, batch_ids, bitrate, completeness_threshold, \n",
    "                         min_years, use_partial_counts, attr_cols, climate_cols, pseudo_counts)\n",
    "                batch_result = dpf.process_pairwise_comparisons(inputs, bitrate)\n",
    "                if batch_result.empty:\n",
    "                    print('Empty batch.  Skipping')\n",
    "                else:\n",
    "                    batch_result.to_csv(batch_output_fpath, index=False)\n",
    "                    print(f\"    Saved {len(batch_result)} new results to file.\")\n",
    "                \n",
    "                batch_files.append(batch_output_fpath)\n",
    "                t2 = time()\n",
    "                print(f'    Processed {len(batch_ids)} pairs at ({bitrate} bits) in {t2 - t0:.1f} seconds')\n",
    "                batch_no += 1\n",
    "                \n",
    "            print(f'    Concatenating {len(batch_files)} batch files.')\n",
    "            if len(batch_files) > 0:\n",
    "                all_results = pd.concat([pd.read_csv(f, engine='pyarrow') for f in batch_files], axis=0)\n",
    "                all_results.to_csv(out_fpath, index=False)\n",
    "                if os.path.exists(out_fpath):\n",
    "                    for f in batch_files:\n",
    "                        os.remove(f)\n",
    "                print(f'    Wrote {len(all_results)} results to {out_fpath}')\n",
    "            else:\n",
    "                print('    No new results to write to file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c61f5c-5df5-41f5-908b-b81fac43ea1f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71186c1a-6368-4978-a0a3-48321811552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f17218-aa35-420e-a9db-2a5360efdcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
