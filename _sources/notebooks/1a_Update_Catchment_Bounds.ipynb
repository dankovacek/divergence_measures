{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5ce998-903d-4b91-9dbf-e9e69e9f2f42",
   "metadata": {},
   "source": [
    "# Update HYSETS Catchment Polygons\n",
    "\n",
    "The HYSETS dataset {cite}`arsenault2020comprehensive` contains an \"artificial boundaries\" flag to indicate where the catchment boundary for the monitoring location is approximated due to either missing data or uncertainty in catchment delineation, in general due to small drainage area.  Approximately 25% of the catchments in the study region (British Columbia and surrounding areas) feature this flag.  \n",
    "\n",
    "In July 2022, the Water Survey of Canada (WSC) published updated polygons for over 8000 catchments in Canada.  We find updated catchment boundaries from WSC and USGS where available.  Polygons updated from USGS and WSC official sources represent over 3/4 of the \"artificial bounds\" flagged catchments, and the remaining 92 are updated using reprocessed USGS 3DEP DEM and an algorithm matching the closest stream pixel (required for catchment delineation) to the reported drainage area. \n",
    "\n",
    "To start, download the HYSETS catchment polygons (`HYSETS_watershed_boundaries.zip`) from [that dataset's open data repository](https://osf.io/rpc3w/).\n",
    "\n",
    "The resulting updated polygons are used in the next chapter/section to extract and validate attributes as part of the supporting information for technical validation of the associated publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331b3ee-44ba-4bc0-9d19-07f3a05ca2a1",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "The following files are needed for this notebook:\n",
    "\n",
    "* **HYSETS catchment polygons**: \n",
    "* **HYSETS station points**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4c4e3-dbac-4cee-9765-50ac29902287",
   "metadata": {},
   "source": [
    "### Import HYSETS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3129c-b011-4ba1-8b7d-e64fadbad4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff63057-4946-4322-9bd7-7c0f1fef0a59",
   "metadata": {},
   "source": [
    "### Set up working folders to organize temporary and final files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200a52d-b960-4aaf-a6f5-9efcec546daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_folder = os.path.join(os.getcwd(), 'data', 'catchment_polygons')\n",
    "temp_folder = os.path.join(polygon_folder, 'temp')\n",
    "updated_catchment_folder = os.path.join(polygon_folder, 'updated_catchment_set')\n",
    "os.path.exists(temp_folder)\n",
    "for f in [temp_folder, updated_catchment_folder]:\n",
    "    if not os.path.exists(f):\n",
    "        os.makedirs(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d3f33-da10-4237-8c7c-15c4b8ccd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the HYSETS attributes data\n",
    "hysets_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hysets_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634249dd-bd4f-4a1c-b260-c7b61cb9d41b",
   "metadata": {},
   "source": [
    "### Import (BCUB) study region bounds\n",
    "\n",
    "Get the region bounds from the BCUB dataset [https://doi.org/10.5683/SP3/JNKZVT](https://doi.org/10.5683/SP3/JNKZVT) or just skip this step and use the pre-processed file (`data/data/study_region_stations.geojson`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca6dc1-cae6-40d9-a1a5-86821e8cc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BCUB (study) region boundary\n",
    "region_gdf = gpd.read_file('data/BCUB_regions_4326.geojson')\n",
    "region_gdf = region_gdf.to_crs(3005)\n",
    "# simplify the geometries (100m threshold) and add a small buffer (250m) to capture HYSETS station points recorded with low accuracy near boundaries\n",
    "region_gdf.geometry = region_gdf.simplify(100).buffer(500)\n",
    "# region_gdf = region_gdf.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9040e7-32ef-42f0-8fdc-53715f829085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stations contained in the study region\n",
    "centroids = hysets_df.apply(lambda x: Point(x['Centroid_Lon_deg_E'], x['Centroid_Lat_deg_N']), axis=1)\n",
    "hysets_points = gpd.GeoDataFrame(hysets_df, geometry=centroids, crs='EPSG:4326')\n",
    "hysets_points.to_crs(3005, inplace=True)\n",
    "hysets_points.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292928ee-aca4-4d1f-b8b0-de84c9c551c6",
   "metadata": {},
   "source": [
    "Note that these are just the artificial bounds flagged rows, below we check for other corrections/updates from official sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c739937-1d9b-4e54-9012-04ada424834e",
   "metadata": {},
   "source": [
    "### Load the original HYSETS polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13af91-986f-49ac-85c3-bc441c6a5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_path = 'data/catchment_polygons/HYSETS_watershed_boundaries/HYSETS_watershed_boundaries_20200730.shp'\n",
    "hs_polygons = gpd.read_file(hs_path)\n",
    "hs_polygons = hs_polygons.set_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d18b5-21b8-488c-817c-57a39bcfa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_gdf = gpd.read_file('data/study_region_stations.geojson')\n",
    "print(bcub_gdf.crs)\n",
    "# the bcub geometries are (centroid) points from the HYSETS properties, \n",
    "# set the geometry to the HYSETS polygons instead\n",
    "catchment_geometries = bcub_gdf.apply(lambda x: hs_polygons.loc[hs_polygons['OfficialID'] == x['Official_ID'], 'geometry'].values[0], axis=1)\n",
    "bcub_gdf['geometry'] = catchment_geometries\n",
    "bcub_gdf.to_crs(3005, inplace=True)\n",
    "hs_polygons.to_crs(3005, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97508d3d-e8ac-4bfe-b680-311d7af9df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df = bcub_gdf.loc[bcub_gdf['Flag_Artificial_Boundaries']== 1, :].copy()\n",
    "print(f'{len(ab_df)}/{len(bcub_gdf)} catchment geometries are flagged \"artificial bounds\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf2a96-9d55-453c-9d33-a97fb0e3fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsim_df = bcub_gdf.loc[bcub_gdf['Flag_GSIM_boundaries']== 1, :].copy()\n",
    "print(f'{len(gsim_df)}/{len(bcub_gdf)} catchment geometries are flagged as using GSIM boundaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879db7d-519b-4687-9111-d86421c7a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc_ab = ab_df.loc[ab_df['Source'] == 'HYDAT', :].copy()\n",
    "usgs_ab = ab_df.loc[ab_df['Source'] == 'USGS', :].copy()\n",
    "print(f'{len(wsc_ab)}/{len(usgs_ab)} WSC/USGS artificial bounds flags ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9539f5-a40d-4d90-a601-131e89c7fda4",
   "metadata": {},
   "source": [
    "### Check for updated WSC polygons\n",
    "\n",
    "The updated WSC catchments can be accessed at the Environment and Climate Change Canada (ECCC) [hydrometrics page](https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/HydrometricNetworkBasinPolygons/).  We only need to download the region files associated with the study region, corresponding to the first two digits of the station identifier (official id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde59e4-b53c-4c33-b0d9-3b3e34f18b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc_stn_df = bcub_gdf.loc[bcub_gdf['Source'] == 'HYDAT'].copy()\n",
    "wsc_stns = wsc_stn_df['Official_ID']\n",
    "prefixes = sorted(list(set([e[:2] for e in wsc_stns])))\n",
    "prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5271c-413d-438f-aaa7-0f35699d1961",
   "metadata": {},
   "source": [
    "Download the above files `<2-digit identifier>.zip` and extract them in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f964b91-5e06-477a-8a60-10639ab4c18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find updated catchment polygons\n",
    "def retrieve_and_update_WSC_polygon(stn_id):\n",
    "    \"\"\"\n",
    "    Returns an updated WSC polygon if it exists or returns an empty (geo)dataframe.\n",
    "    \"\"\"\n",
    "    stn_id = row['Official_ID']\n",
    "    stn_prefix = stn_id[:2]\n",
    "    catchment_path = f'data/catchment_polygons/{stn_prefix}/{stn_id}/{stn_id}_DrainageBasin_BassinDeDrainage.shp'\n",
    "    if os.path.exists(catchment_path):\n",
    "        print(f'Updated polygon found for {stn_id}')\n",
    "        updated_polygon = gpd.read_file(catchment_path)\n",
    "        updated_polygon.to_crs(3005, inplace=True)\n",
    "        return updated_polygon\n",
    "    return gpd.GeoDataFrame()\n",
    "\n",
    "def retrieve_and_update_WSC_station_location(stn_id):\n",
    "    \"\"\"\n",
    "    Returns an updated WSC station location if it exists or returns an empty (geo)dataframe.\n",
    "    \"\"\"\n",
    "    stn_id = row['Official_ID']\n",
    "    stn_prefix = stn_id[:2]\n",
    "    file_path = f'data/catchment_polygons/{stn_prefix}/{stn_id}/{stn_id}_Station.shp'\n",
    "    if os.path.exists(file_path):\n",
    "        print(f'Updated station location found for {stn_id}')\n",
    "        updated_pt = gpd.read_file(file_path)\n",
    "        updated_pt.to_crs(3005, inplace=True)\n",
    "        return updated_pt\n",
    "    return gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c500c31-1e60-4f78-a42c-ee9ebd6f093a",
   "metadata": {},
   "source": [
    "### Check for updated USGS polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42498fa-f699-4683-b457-e220ac13b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api url for nwis sites\n",
    "usgs_api_url = 'https://labs.waterdata.usgs.gov/api/nldi/linked-data/nwissite/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397b462-e777-49d4-a189-99b04229268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_usgs_stn_data(stn):\n",
    "    # query the NWIS with the station number to get the station coordinates    \n",
    "    try:\n",
    "        query_url = usgs_api_url + f'USGS-{stn}'\n",
    "        usgs_data = pd.read_json(query_url)\n",
    "        usgs_stn_loc = usgs_data['features'][0]['geometry']['coordinates']\n",
    "        stn_pt = Point(*usgs_stn_loc)\n",
    "        return gpd.GeoDataFrame(geometry=[stn_pt], crs=4326)\n",
    "    except Exception as ex:\n",
    "        msg = f'USGS station query failed for {stn}. {ex}'\n",
    "        print(msg)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "\n",
    "def usgs_basin_polygon_query(url):\n",
    "    \"\"\"USGS polygons are in EPSG:4326 crs\"\"\"\n",
    "    response = urlopen(url)\n",
    "    json_data = response.read().decode('utf-8', 'replace')\n",
    "    d = json.loads(json_data)\n",
    "    return gpd.GeoDataFrame.from_features(d['features'], crs='EPSG:4326')\n",
    "\n",
    "\n",
    "def retrieve_usgs_basin_data(stn):\n",
    "    \"\"\"Retrieve the USGS basin polygon and station location from the NLDI API. \n",
    "    If there is no basin for the station, use the NLDI to retrieve upstream \n",
    "    and downstream boundaries.  \n",
    "    Pick the one closest in (HYSETS published) area to the station location.\"\"\"    \n",
    "    \n",
    "    # query the basin polygon from USGS\n",
    "    basin_query = usgs_api_url + f'USGS-{stn}/basin?simplified=false&splitCatchment=false'    \n",
    "    try:\n",
    "        usgs_basin_df = usgs_basin_polygon_query(basin_query)\n",
    "        # dissolve the basin polygons\n",
    "        usgs_basin_df = usgs_basin_df.dissolve()\n",
    "        usgs_basin_df = usgs_basin_df.to_crs(3005)\n",
    "        # check if geometry is multipolygon\n",
    "        if usgs_basin_df.geometry.type.values[0] == 'MultiPolygon':\n",
    "            print(f'   ...MultiPolygon detected, attemping to make geometry valid.')\n",
    "            usgs_basin_df = usgs_basin_df.explode()\n",
    "            usgs_basin_df['area'] = usgs_basin_df.geometry.area / 1E6\n",
    "            usgs_basin_df['area_pct'] = usgs_basin_df['area'] / usgs_basin_df['area'].sum()\n",
    "            usgs_basin_df = usgs_basin_df[usgs_basin_df['area_pct'] > 0.95]\n",
    "            if len(usgs_basin_df) > 1:\n",
    "                raise Exception('USGS basin polygon query returned multiple polygons.')\n",
    "\n",
    "        return usgs_basin_df\n",
    "    except Exception as ex:\n",
    "        print(f'USGS basin polygon query failed for {stn}.  {ex}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382731fc-9f79-4a70-b252-aee083a0bfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intermediate_step_path = 'data/updated_geometries_intermediate.geojson'\n",
    "if os.path.exists(intermediate_step_path):\n",
    "    print('loading existing file')\n",
    "    bcub_gdf = gpd.read_file(intermediate_step_path)\n",
    "else:\n",
    "    bcub_gdf['geometry_updated'] = False\n",
    "    # set a variable to prevent jupyter book from \n",
    "    # processing during book building step\n",
    "    process_step = True\n",
    "    if process_step:\n",
    "        for i, row in bcub_gdf.iterrows():\n",
    "            stn_id = row['Official_ID']\n",
    "            source = row['Source']\n",
    "            if source == 'HYDAT':\n",
    "                # pt = retrieve_and_update_WSC_station_location(stn_id)\n",
    "                polygon = retrieve_and_update_WSC_polygon(stn_id)            \n",
    "            elif source == 'USGS':\n",
    "                # pt = retrieve_usgs_stn_data(stn_id)\n",
    "                polygon = retrieve_usgs_basin_data(stn_id)    \n",
    "            if not polygon.empty:\n",
    "                assert polygon.crs == 'EPSG:3005'\n",
    "                pt = polygon.geometry.centroid\n",
    "                pt = pt.to_crs(4326)\n",
    "                lat, lon = pt.geometry.x[0], pt.geometry.y[0]\n",
    "\n",
    "                bcub_gdf.loc[i, 'Centroid_Lat_deg_N'] = lat\n",
    "                bcub_gdf.loc[i, 'Centroid_Lon_deg_E'] = lon\n",
    "                bcub_gdf.loc[i, 'geometry'] = polygon.geometry.values[0]\n",
    "                bcub_gdf.loc[i, 'geometry_updated'] = True\n",
    "                bcub_gdf.loc[i, 'Flag_Artificial_Boundaries'] = 0\n",
    "                bcub_gdf.loc[i, 'Flag_GSIM_boundaries'] = 0\n",
    "            else:\n",
    "                polygon = hs_polygons.loc[hs_polygons['OfficialID'] == stn_id, 'geometry'].copy()\n",
    "                bcub_gdf.loc[i, 'geometry'] = polygon.geometry.values[0]\n",
    "    \n",
    "    updated_official = bcub_gdf.copy()\n",
    "    updated_official.to_file(intermediate_step_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629d070-adf2-417a-ad2b-3f275e595563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df_remaining = bcub_gdf.loc[bcub_gdf['Flag_Artificial_Boundaries'] == 1, :].copy()\n",
    "gsim_remaining = bcub_gdf.loc[bcub_gdf['Flag_GSIM_boundaries'] == 1, :].copy()\n",
    "print(f'{len(ab_df_remaining)}/{len(bcub_gdf)} catchment geometries remain flagged \"artificial bounds\" ({len(ab_df) - len(ab_df_remaining)} updated.)')\n",
    "print(f'{len(gsim_remaining)}/{len(bcub_gdf)} catchment geometries remain flagged as using GSIM bounds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151f419-4d73-431c-bcab-4171083ef224",
   "metadata": {},
   "source": [
    "### Delineate the remaining catchments from 1 arc-second USGS 3DEP DEM.\n",
    "\n",
    "For all of the remaining rows labeled \"FLAG_Artificial_Boundaries\" or \"FLAG_GSIM_boundaries\", follow the methodology in the [BCUB dataset demo](https://dankovacek.github.io/bcub_demo/notebooks/2_DEM_Preprocessing.html) and reprocess the catchment bounds.\n",
    "\n",
    "The code below assumes the underlying DEM has been hydraulically conditioned and flow direction, flow accumulation, and stream rasters have been generated for the area corresponding to each monitoring station catchment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083a640-fe38-4a98-908f-5fce78fe11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the station location, get the corresponding (sub-)region code\n",
    "# since rasters are broken up into regions for dem processing and \n",
    "# catchment delineation\n",
    "\n",
    "if 'index_right' in ab_df_remaining.columns:\n",
    "    ab_df_remaining.drop(columns=['index_right'], axis=1, inplace=True)\n",
    "\n",
    "# ensure that all remaining points have a region code\n",
    "ab_stns = ab_df_remaining['Official_ID'].values\n",
    "gsim_stns = gsim_remaining['Official_ID'].values\n",
    "\n",
    "# make sure no stations are included twice\n",
    "assert len(np.intersect1d(ab_stns, gsim_stns)) == 0\n",
    "remaining_stns = gpd.GeoDataFrame(pd.concat([ab_df_remaining, gsim_remaining]), crs=ab_df_remaining.crs)\n",
    "print(f'{len(remaining_stns)} stations remain to re-process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40945ac-9868-44c6-9595-9cf947c2bc68",
   "metadata": {},
   "source": [
    "## Process stations individually\n",
    "\n",
    "In order to facilitate an iterative validation process for delineating catchment bounds that are uncertain, the stations will be processed one by one as follows:\n",
    "\n",
    ":::{prf:algorithm} Catchment delineation\n",
    ":label: catchment-validation\n",
    "\n",
    "**Inputs** Given an approximate streamflow monitoring station location $L$ and a flow accumulation raster ($C$) with sufficient spatial coverage, an expected flow accumulation within some allowable range ($[\\text{acc}_{min}, \\text{acc}_{max}]$) and some allowable spatial distance ($d_{max}$) from the recorded station location.\n",
    "\n",
    "**Output** a) HYSETS pour pt (station location), b) HYSETS \"artificial bounds\" polygon c) adjusted pour pt, d) adjusted (approximate) catchment boundary, and e) (nearby) stream network vector lines.  These geometries are rendered in standalone interactive HTML documents for each station to facilitate manual validation and iteration.\n",
    "\n",
    "1. For each station location $L$\n",
    "    1. Find the stream pixel $C*_{ij}$ corresponding to the smallest distance from $L$ between all valid flow accumulation raster cells $\\text{acc}_{min} >= C_{ij} < \\text{acc}_{max}$\n",
    "    2. Delineate the catchment from $C*_{ij}$ using the \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac68c4e-6e86-4b9b-b0db-4b16b047356f",
   "metadata": {},
   "source": [
    "### Find the nearest pixel in the stream raster corresponding to the point geometry\n",
    "\n",
    "Note that where the catchment geometry is missing in HYSETS, the centroid point is just the station location, and the geometry is simply a square centred at the station location and with an area equal to the drainage area reported by the official source {cite}`arsenault2020comprehensive`.  We then find the nearest stream pixel to the \"centroid\" which is not actually the centroid but the reported station location.  \n",
    "\n",
    "Historical (discontinued) stations are difficult to align automatically with the correct stream location because often their geographic locations (representing pour points) were not recorded precisely.  Catchment delineation requires an x,y input that aligns precisely with the stream network raster, meaning greater pour point precision is required as resolution increases.  There are less than 100 catchments in the study region remaining with a `Artificial_Bounds_Flag`, and this is a reasonable number to validate \"manually\", which is done here.\n",
    "\n",
    "The process of validating these final catchments is as follows:\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab3d14-9eac-4a0e-b5b6-48ae7fd17dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import Point, box\n",
    "\n",
    "from attribute_processing_functions import clip_raster_to_basin\n",
    "\n",
    "import whitebox \n",
    "wbt = whitebox.WhiteboxTools()\n",
    "wbt.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da956d0-e124-4767-89f9-24b73b24d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_raster(filename):\n",
    "    \"\"\"\n",
    "    Take in a file name and return the raster data, \n",
    "    the coordinate reference system, and the affine transform.\n",
    "    \"\"\"\n",
    "    raster = rxr.open_rasterio(filename, mask_and_scale=True)\n",
    "    crs = raster.rio.crs\n",
    "    affine = raster.rio.transform(recalc=False)\n",
    "    return raster, crs.to_epsg(), affine\n",
    "\n",
    "\n",
    "def affine_map_vec(affine, x, y):\n",
    "    a, b, c, d, e, f, _, _, _ = affine\n",
    "    n = x.size\n",
    "    new_x = np.zeros(n, dtype=np.float64)\n",
    "    new_y = np.zeros(n, dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        new_x[i] = x[i] * a + y[i] * b + c\n",
    "        new_y[i] = x[i] * d + y[i] * e + f\n",
    "    return new_x, new_y\n",
    "\n",
    "\n",
    "def snap_pour_point(raster_filepath, pt, area, distance_tol=250):\n",
    "    raster, crs, affine = retrieve_raster(raster_filepath)     \n",
    "    acc = raster.squeeze()\n",
    "\n",
    "    dx, dy = abs(raster.rio.resolution()[0]), abs(raster.rio.resolution()[1])\n",
    "    \n",
    "    # Determine area fraction\n",
    "    area_frac = 2 if area > 10 else 5\n",
    "    expected_cells = int((area * 1e6) / (dx * dy))\n",
    "    \n",
    "    # Calculate min and max cells\n",
    "    min_cells = int((1 / area_frac) * expected_cells)\n",
    "    max_cells = int(area_frac * expected_cells)\n",
    "        \n",
    "    # Get potential stream cells within the expected range\n",
    "    yi, xi = np.where((acc >= min_cells) & (acc <= max_cells))\n",
    "        \n",
    "    if len(yi) == 0 or len(xi) == 0:\n",
    "        print('No points returned meeting accumulation criteria.')\n",
    "        return None, np.inf\n",
    "    \n",
    "    # Convert to coordinates\n",
    "    affine_tup = tuple(raster.rio.transform(recalc=False))\n",
    "    x_coords, y_coords = affine_map_vec(affine_tup, xi, yi)\n",
    "\n",
    "    # Calculate distances and find the nearest stream cell\n",
    "    stn_coords = (pt.geometry.x.values[0], pt.geometry.y.values[0])\n",
    "    dists = np.sqrt((x_coords - stn_coords[0])**2 + (y_coords - stn_coords[1])**2)\n",
    "        \n",
    "    if len(dists) == 0 or np.min(dists) > distance_tol:\n",
    "        print('No points found within distance tolerance.')\n",
    "        return None, np.inf\n",
    "    \n",
    "    min_idx = np.argmin(dists)\n",
    "    x_snap, y_snap = x_coords[min_idx] + 0.5 * dx, y_coords[min_idx] - 0.5 * dy\n",
    "    \n",
    "    return Point(x_snap, y_snap), np.min(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1a50f-57d5-4c29-9a3a-8f6cbabe4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub/processed_data/processed_dem/'\n",
    "# dem_folder = '/home/danbot/Documents/code/23/bcub/processed_data/processed_dem/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce2871-c8b1-4b96-bfe8-cc220abf39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delineate_new_catchment(pour_pt_path, rc, stn_id, stn_folder):\n",
    "    d8_pntr = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005_fdir.tif')\n",
    "    assert os.path.exists(d8_pntr)\n",
    "    output = os.path.join(stn_folder, f'{stn_id}_basin.tif')\n",
    "    \n",
    "    if not os.path.exists(output):        \n",
    "        wbt.watershed(\n",
    "            d8_pntr, \n",
    "            pour_pt_path, \n",
    "            output, \n",
    "            esri_pntr=False, \n",
    "        )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90601ff-8f4f-4b9d-9add-97fbeac27401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stream_vectors(stn_data, adjusted_catchment_path, stn_folder):\n",
    "    catchment = gpd.read_file(adjusted_catchment_path)\n",
    "    # add a buffer to the clip to get a wider picture of streams\n",
    "    catchment.geometry = catchment.geometry.buffer(500)\n",
    "    stn_id = stn_data['Official_ID'].values[0]\n",
    "    rc = stn_data['region_code'].values[0]\n",
    "    \n",
    "    # open and clip the stream raster\n",
    "    acc_file = f'{rc}_USGS_3DEP_3005_accum.tif'\n",
    "    acc_fpath = os.path.join(dem_folder, acc_file)\n",
    "    acc_raster, crs, _ = retrieve_raster(acc_fpath)\n",
    "    nodata_value = acc_raster.rio.nodata\n",
    "    \n",
    "    raster_res = acc_raster.rio.resolution()\n",
    "    cell_area = abs(raster_res[0] * raster_res[1])\n",
    "    \n",
    "    assert acc_raster.rio.crs == catchment.crs\n",
    "\n",
    "    geoms = [stn_data.geometry.values[0], catchment.geometry.values[0]]\n",
    "    combined_geom = gpd.GeoDataFrame(geometry=geoms, crs=stn_data.crs)\n",
    "    # cvx_hull = combined_geom.dissolve().convex_hull\n",
    "    # Get the bounding box of the combined geometries\n",
    "    bbox = combined_geom.total_bounds  # returns (minx, miny, maxx, maxy)\n",
    "\n",
    "    # Create a Polygon from the bounding box and buffer it by 500 meters\n",
    "    bbox_polygon = box(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    bbox_buffered = bbox_polygon.buffer(500)\n",
    "    \n",
    "    # Create a GeoDataFrame for the buffered bounding box\n",
    "    bbox_mask = gpd.GeoDataFrame(geometry=[bbox_buffered], crs=stn_data.crs)\n",
    "    \n",
    "    # clip the raster with the catchment polygon    \n",
    "    clip_ok, clipped_acc_raster = clip_raster_to_basin(bbox_mask, acc_raster)   \n",
    "    \n",
    "    # Save the clipped raster to a file\n",
    "    acc_clip_fpath = os.path.join(stn_folder, f'{stn_id}_clipped_acc.tif')\n",
    "    clipped_acc_raster.rio.to_raster(acc_clip_fpath, crs=acc_raster.rio.crs, nodata=np.nan)\n",
    "    \n",
    "    if clip_ok:\n",
    "        # drop the raster to save memory\n",
    "        del acc_raster\n",
    "    # set the minimum area to 1 km^2 for filtering \n",
    "    # the accumulation for stream pixels \n",
    "    min_cells = int(1e6 / cell_area) \n",
    "    \n",
    "    # render the streams raster from the clip\n",
    "    streams_temp = os.path.join(stn_folder, f'{stn_id}_streams.tif')\n",
    "    wbt.extract_streams(\n",
    "        acc_clip_fpath, \n",
    "        streams_temp, \n",
    "        min_cells, \n",
    "        zero_background=False, \n",
    "    )\n",
    "    \n",
    "    assert os.path.exists(streams_temp)\n",
    "    \n",
    "    stream_vector_output = os.path.join(stn_folder, f'{stn_id}_stream_vectors.shp')\n",
    "    \n",
    "    wbt.raster_to_vector_lines(\n",
    "        streams_temp, \n",
    "        stream_vector_output,\n",
    "    )\n",
    "    \n",
    "    assert os.path.exists(stream_vector_output)\n",
    "        \n",
    "    return stream_vector_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aeeae5-f797-4311-8882-7c6e30803c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_vector_basin(rc, catchment_raster_fpath, stn_id, stn_folder):\n",
    "    \"\"\"\n",
    "    If we send too many pour points per batch to the Whitebox \"unnest\" function, \n",
    "    we generate a huge number of temporary raster files that could easily \n",
    "    exceed current SSD disk capacities.\n",
    "    \"\"\"\n",
    "    raster, crs, affine = retrieve_raster(catchment_raster_fpath)\n",
    "    polygon_path = os.path.join(stn_folder, f'{rc}_temp_polygon.shp')\n",
    "\n",
    "    # this function creates rasters of ordered \n",
    "    # sets of non-overlapping basins\n",
    "    if not os.path.exists(polygon_path):\n",
    "        wbt.raster_to_vector_polygons(\n",
    "            catchment_raster_fpath,\n",
    "            polygon_path,\n",
    "        )\n",
    "    \n",
    "    gdf = gpd.read_file(polygon_path, crs=crs)\n",
    "    gdf = gdf.explode(index_parts=False)\n",
    "    gdf.reset_index(inplace=True)\n",
    "    gdf['area'] = gdf.geometry.area\n",
    "    gdf = gdf[gdf.index == gdf['area'].idxmax()]\n",
    "    # drop the raster from memory\n",
    "    del raster\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d77c2-82b8-487c-b79d-3d38639a1860",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "remaining_stns = remaining_stns.sort_values(by=['region_code'])\n",
    "remaining_stns.reset_index(inplace=True, drop=True)\n",
    "pour_pt_filenames = []\n",
    "\n",
    "for i, row in remaining_stns.iterrows():\n",
    "    \n",
    "    stn_id = row['Official_ID']\n",
    "    rc = row['region_code']\n",
    "    area = row['Drainage_Area_km2']\n",
    "    \n",
    "    stn_folder = os.path.join(updated_catchment_folder, stn_id)\n",
    "    if not os.path.exists(stn_folder):\n",
    "        os.makedirs(stn_folder)\n",
    "    # accumulation raster path\n",
    "    acc_dem_path = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005_accum.tif')\n",
    "    if not os.path.exists(acc_dem_path):\n",
    "        print('missing ', acc_dem_path)\n",
    "        continue\n",
    "    assert os.path.exists(acc_dem_path), f'{acc_dem_path} not found'\n",
    "    \n",
    "    print(f'processing {stn_id} in {rc} region ({area:.2f} km²)')\n",
    "    stn_data = remaining_stns.loc[[i]].copy()\n",
    "\n",
    "    # 1) save the original HYSETS catchment geometry \n",
    "    hs_polygon_path = os.path.join(stn_folder, f'{stn_id}_HYSETS_polygon.geojson')\n",
    "    if not os.path.exists(hs_polygon_path):\n",
    "        stn_data.to_file(hs_polygon_path)\n",
    "\n",
    "    # 2) save the original HYSETS station location\n",
    "    hs_pt_path = os.path.join(stn_folder, f'{stn_id}_HYSETS_pt.geojson')\n",
    "    if not os.path.exists(hs_pt_path):\n",
    "        print('    ...processing HYSETS pour point')\n",
    "        hs_pt = Point(stn_data['Centroid_Lon_deg_E'], stn_data['Centroid_Lat_deg_N'])\n",
    "        hs_pt_df = gpd.GeoDataFrame(geometry=[hs_pt], crs='4326')\n",
    "        hs_pt_df.to_crs(3005, inplace=True)\n",
    "        hs_pt_df.to_file(hs_pt_path)\n",
    "    else:\n",
    "        hs_pt_df = gpd.read_file(hs_pt_path)\n",
    "\n",
    "    # 3) find the nearest stream cell to the reported station location\n",
    "    adjusted_ppt_path = os.path.join(stn_folder, f'{stn_id}_adjusted_ppt.geojson')\n",
    "    adjusted_ppt_path_shp = os.path.join(stn_folder, f'{stn_id}_adjusted_ppt.shp')    \n",
    "    if not os.path.exists(adjusted_ppt_path) | os.path.exists(adjusted_ppt_path_shp):\n",
    "        print('    ...processing adjusted pour point')\n",
    "        nearest_pt, distance = snap_pour_point(acc_dem_path, hs_pt_df, area, distance_tol=1000)\n",
    "        if nearest_pt is None:\n",
    "            print(f'{stn_id}: no point returned within expected drainage area range.')\n",
    "            continue\n",
    "        adj_pt = gpd.GeoDataFrame(geometry=[nearest_pt], crs=remaining_stns.crs)\n",
    "        adj_pt['Official_ID'] = stn_id\n",
    "        adj_pt.to_file(adjusted_ppt_path)\n",
    "        adj_pt.to_file(adjusted_ppt_path_shp)\n",
    "        \n",
    "        \n",
    "    # 4) delineate a new catchment from the adjusted point\n",
    "    adjusted_catchment_path = os.path.join(stn_folder, f'{stn_id}_adjusted_catchment.geojson')\n",
    "    if not os.path.exists(adjusted_catchment_path):\n",
    "        print('    ...delineating basin raster')\n",
    "        catchment_raster_fpath = delineate_new_catchment(adjusted_ppt_path_shp, rc, stn_id, stn_folder)\n",
    "        adjusted_catchment = raster_to_vector_basin(rc, catchment_raster_fpath, stn_id, stn_folder)\n",
    "        adjusted_catchment.to_file(adjusted_catchment_path)\n",
    "        \n",
    "    # 5) save streamlines as a vector within some distance of the catchment polygon / ppt.\n",
    "    streams_path = os.path.join(stn_folder, f'{stn_id}_streams.geojson')\n",
    "    if not os.path.exists(streams_path):\n",
    "        print('    ...processing streams vectors')\n",
    "        streams_temp_path = generate_stream_vectors(stn_data, adjusted_catchment_path, stn_folder)\n",
    "        gdf = gpd.read_file(streams_temp_path)\n",
    "        gdf.to_file(streams_path)\n",
    "        \n",
    "        remove_extensions = ['.dbf', '.prj', '.shp', '.shx', '.tif', '.cpg']\n",
    "        if os.path.exists(adjusted_catchment_path):\n",
    "            for f in os.listdir(stn_folder):\n",
    "                if any([f.endswith(e) for e in remove_extensions]):\n",
    "                    os.remove(os.path.join(stn_folder, f))\n",
    "        \n",
    "        print(f'   ...processed {stn_id}, saved to {streams_path}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245bc4d-cdf8-4170-86e4-119dbbc95c9d",
   "metadata": {},
   "source": [
    "## Create plots to visualize the results of adjusting pour points to the derived stream network\n",
    "\n",
    "### Create validation plots set\n",
    "\n",
    "Create and save interactive html plots to compare HYSETS \"artificial bounds\" with catchment boundaries delineated by snapping pour points to nearest flow accumulation cell within 5% of expected area (for basins $> 10 \\text{km}^2$ and 2.5% otherwise.  Also set a maximum search distance tolerance of 250m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20483cb7-de10-40a2-ac1c-771a2f25bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "import xyzservices.providers as xyz\n",
    "# output_notebook()\n",
    "\n",
    "usgs_tiles = xyz['USGS']['USTopo']\n",
    "\n",
    "# dir(tiles)\n",
    "tiles = usgs_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5587a-dbf7-4079-9cb3-5951f3292bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pt_for_plotting(pt_df):\n",
    "    df = pt_df.copy()\n",
    "    df.to_crs(3857, inplace=True)\n",
    "    x, y = df.geometry.values[0].x, df.geometry.values[0].y\n",
    "    return x, y\n",
    "\n",
    "def format_poly_for_plotting(poly_df):\n",
    "    df = poly_df.copy()\n",
    "    df = df.explode(index_parts=False)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df['area'] = df.geometry.area / 1e6\n",
    "    df = df[df.index == df['area'].idxmax()]\n",
    "    df.to_crs(3857, inplace=True)\n",
    "    x, y = df.exterior.geometry.values[0].coords.xy\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def format_linestring_for_plotting(line_df):\n",
    "    # Prepare data for Bokeh\n",
    "    df = line_df.copy()\n",
    "    df.to_crs(3857, inplace=True)\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for geom in df.geometry:\n",
    "        if geom.geom_type == 'MultiLineString':\n",
    "            for line in geom:\n",
    "                xs.append(list(line.xy[0]))\n",
    "                ys.append(list(line.xy[1]))\n",
    "        elif geom.geom_type == 'LineString':\n",
    "            xs.append(list(geom.xy[0]))\n",
    "            ys.append(list(geom.xy[1]))\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def create_new_pt(x, y, pt_crs):\n",
    "    pt = Point(x, y)\n",
    "    return gpd.GeoDataFrame(geometry=[pt], crs=pt_crs)\n",
    "\n",
    "    \n",
    "def pour_point_plot(stn_id, hs_pt, hs_poly, adj_pt, adj_poly, streams, add_pt=None, pt_crs='EPSG:4326', adjusted=False):\n",
    "    \"\"\"\n",
    "    Note if you add a point, it should be in decimal degrees (EPSG:4326) unless otherwise specified with pt_crs.\n",
    "    \"\"\"\n",
    "    \n",
    "    name = hysets_df[hysets_df['Official_ID'] == stn_id]['Name'].values[0]\n",
    "    \n",
    "        \n",
    "    p = figure(title=f'{stn_id} ({name})', x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
    "    \n",
    "    ppt_x, ppt_y = format_pt_for_plotting(adj_pt)\n",
    "    stn_x, stn_y = format_pt_for_plotting(hs_pt)\n",
    "    new_poly_x, new_poly_y = format_poly_for_plotting(adj_poly)\n",
    "    hs_poly_x, hs_poly_y = format_poly_for_plotting(hs_poly)\n",
    "    \n",
    "    stream_x, stream_y = format_linestring_for_plotting(streams)\n",
    "    \n",
    "    hs_da = hs_poly.geometry.area.values[0] / 1e6\n",
    "    new_area = adj_poly.geometry.area.values[0] / 1e6\n",
    "    \n",
    "    # Add the transparent polygon to the plot\n",
    "    p.patch(hs_poly_x, hs_poly_y, fill_alpha=0.3, line_color='green', fill_color='green',\n",
    "           legend_label=f'HYSETS ({hs_da:.1f} km²)')\n",
    "    p.patch(new_poly_x, new_poly_y, fill_alpha=0.3, line_color='orange', fill_color='orange',\n",
    "           legend_label=f'BCUB ({new_area:.1f} km²)')\n",
    "    \n",
    "    p.scatter([stn_x], [stn_y], marker='o', size=14, color='green',\n",
    "              legend_label=f'HYSETS stn')\n",
    "    p.scatter([ppt_x], [ppt_y], marker='star', size=16, color='orange', line_color='black',\n",
    "              legend_label=f'BCUB ppt')\n",
    "    \n",
    "    if add_pt is not None:\n",
    "        print(add_pt)\n",
    "        add_x, add_y = add_pt\n",
    "        add_pt_df = create_pt(add_x, add_y, pt_crs)\n",
    "        pt_x, pt_y = format_pt_for_plotting(add_pt_df)\n",
    "        p.scatter([pt_x], [pt_y], marker='triangle', size=18, color='salmon', line_color='black',\n",
    "              legend_label=f'Added pt')\n",
    "        \n",
    "    \n",
    "    # Add a MultiLine glyph\n",
    "    if new_area < 1000:\n",
    "        p.multi_line(xs=stream_x, ys=stream_y, line_width=2, color='blueviolet', line_dash='dashed',\n",
    "                    legend_label='3DEP streamline')    \n",
    "\n",
    "    p.add_tile(tiles, retina=True)\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.legend.click_policy = 'hide'\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c37e6-c2bf-4119-9446-cca994ead09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pour_pt_filenames = []\n",
    "total = 0\n",
    "plot = []\n",
    "\n",
    "output_plot_folder = 'data/validation_plots'\n",
    "stn_plot_data_folder = f'data/catchment_polygons/updated_catchment_set/'\n",
    "\n",
    "print(f'Processing {len(os.listdir(stn_plot_data_folder))} plots')\n",
    "\n",
    "def process_plot(stn_id, adjusted=False):\n",
    "    fpath = os.path.join(output_plot_folder, f'{stn_id}.html')\n",
    "    # print(stn_id)\n",
    "    if os.path.exists(fpath) & ~adjusted:\n",
    "        # print(f'    {stn_id} already processed.')\n",
    "        return None\n",
    "    if stn_id == None:\n",
    "        print('phantom station with no name')\n",
    "        return None\n",
    "        \n",
    "    stn_folder = os.path.join(updated_catchment_folder, stn_id)\n",
    "    hs_pt_file = os.path.join(stn_folder, f'{stn_id}_HYSETS_pt.geojson')\n",
    "    hs_poly_file = os.path.join(stn_folder, f'{stn_id}_HYSETS_polygon.geojson')\n",
    "    if adjusted:\n",
    "        adj_pt_file = os.path.join(stn_folder, f'{stn_id}_REadjusted_ppt.geojson')\n",
    "        adj_poly_file = os.path.join(stn_folder, f'{stn_id}_REadjusted_catchment.geojson')\n",
    "        streams_file = os.path.join(stn_folder, f'{stn_id}_streams_adjusted.geojson')\n",
    "    \n",
    "    # try:\n",
    "    hs_pt = gpd.read_file(hs_pt_file)\n",
    "    hs_poly = gpd.read_file(hs_poly_file)\n",
    "    adj_pt = gpd.read_file(adj_pt_file)\n",
    "    adj_poly = gpd.read_file(adj_poly_file)\n",
    "    streams = gpd.read_file(streams_file)\n",
    "    plt = pour_point_plot(stn_id, hs_pt, hs_poly, adj_pt, adj_poly, streams, adjusted=adjusted)\n",
    "    fpath = os.path.join(output_plot_folder, f'{stn_id}.html')\n",
    "    # Specify the output file\n",
    "    output_file(fpath)\n",
    "    # Save the plot to the HTML file\n",
    "    save(plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acded11-1662-4f02-8282-98e9598c1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stn_id in sorted(os.listdir(stn_plot_data_folder)):\n",
    "    process_plot(stn_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca23525-2cbf-4b16-b180-b6ae3e31555b",
   "metadata": {},
   "source": [
    "## Show an example validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a5726-7221-4814-94df-161593bc29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4a955-36be-495a-94fd-420a57410d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = [e.split('.')[0] for e in os.listdir('data/validation_plots/recheck')]\n",
    "pt_crs = 'EPSG:4326'\n",
    "checked_stns = {\n",
    "    '15081800': create_new_pt(-132.87492, 55.36158, pt_crs), # catch trib just DS of stn or no?\n",
    "    '15039900': create_new_pt(-133.98292, 58.24594, pt_crs), # reposition at lake outlet\n",
    "    '10CC001': create_new_pt(-122.53885, 58.835, pt_crs), # reposition below Muskwa Creek\n",
    "    '15109048': create_new_pt(-134.67127, 58.286815, pt_crs), # cature more of North Fork (NF)\n",
    "    '15056095': create_new_pt(-135.18648, 59.52632, pt_crs), # reposition at lake outlet\n",
    "    '15031000': create_new_pt(-133.88392, 58.18253, pt_crs), # reposition to capture three upper tribs\n",
    "    '12212430': create_new_pt(-122.49944, 48.99979, pt_crs), # reposition to capture upper tribs\n",
    "    '15054000': create_new_pt(-134.64325, 58.38685, pt_crs), # better isolate Auke Creek\n",
    "    '15087500': create_new_pt(-132.87254, 56.79361, pt_crs), # isolate east fork of Hobo Creek\n",
    "    '12110400': create_new_pt(-122.08512, 47.35607, pt_crs), # reposition on south fork Jenkins (Cranmar Creek)\n",
    "    '15056030': create_new_pt(-135.19131, 59.009803, pt_crs), # not great resolving of stream but captures upper basin well\n",
    "    '05AE040': create_new_pt(-113.54587, 49.01493, pt_crs), # isolate East branch of Lee Creek\n",
    "    '15087200': None, # Hammer Slough at Petersburg doesn't render from effect of bypass road\n",
    "    '07FD913': None, # Young Drainage Project near Spirit River -- no idea!\n",
    "    '07FD912': None, # Whitburn Drainage Project Near Spirit River -- no idea!\n",
    "    '15053200': None, # Duck Creek doesn't resolve in 30m DEM due to urban development\n",
    "    '12113349': None, # Mill Creek doesn't render well with 1 arcsecond DEM in urban area\n",
    "    '15052475': None, # Jordan Creek doesn't render well with 1 arcsecond DEM in urban area\n",
    "    # '08EG013': create_new_pt(-130.0851, 54.19629, pt_crs), # reposition at lake outlet\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b682a0f-1f7e-4fa7-9e52-eb60b04c4aef",
   "metadata": {},
   "source": [
    "### Additional validation notes\n",
    "\n",
    "* **15024750: Goat Creek Near Wrangell AK**  \n",
    "    -there is a large (5-7km^2) trib just US of the station that in basemaps appears to drain to the north  \n",
    "    -there's a low point where debris jam could cause flow to divert, or maybe past flows diverted  \n",
    "* **15129600: Ophir Creek NR Yakutat AK**   \n",
    "    - the 3DEP dem doesn't align with USGS basemapping but the area captured is a reasonable approximation\n",
    "* **15129510: Old Situk River Nr Yakutat AK**  \n",
    "    - significantly larger catchment delineated from very close to the reported location.   \n",
    "    - does the north fork exist / should it be included?  \n",
    "* **10CC001: Fort Nelson River at Fort Nelson**\n",
    "    - the reported location doesn't include North Branch (HYSETS does) -- Muskwa River\n",
    "    - 10CC002 is named \"above Muskwa River\" which coincides with this location,  \n",
    "    - the much greater flow magnitude of 10CC001 suggests it was downstream of the Muskwa confluence.\n",
    "* **12212430: Unnamed Trib to Bertrand Cr. Near H St.**\n",
    "    - naming isn't very descriptive, base maps look like monitoring station is road ditch\n",
    "    - pour point shifted away from reported location\n",
    "    - however upstream network matches well with USGS base mapping despite pour point locat\n",
    "* **08EG013: Boneyard Creek at Outlet of Rainbow Lake**\n",
    "    - naming is helpful to identify outlet of Rainbow Lake\n",
    "    - very different drainage area compared to WSC reported value\n",
    "    - record is very short (~ 2 seasons 1962-64)\n",
    "    - appears to have a dam at the outlet which may have caused lake connectivity and doubling of catchment area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b923e9d-069a-4d3a-8860-694e0d9c6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a station id\n",
    "# stn_id = [e for e in to_check if e not in checked_stns][0]\n",
    "stn_id = '15024750'\n",
    "fpath = os.path.join(output_plot_folder, f'{stn_id}.html')\n",
    "stn_folder = os.path.join(updated_catchment_folder, stn_id)\n",
    "\n",
    "hs_pt_file = os.path.join(stn_folder, f'{stn_id}_HYSETS_pt.geojson')\n",
    "hs_poly_file = os.path.join(stn_folder, f'{stn_id}_HYSETS_polygon.geojson')\n",
    "adj_pt_file = os.path.join(stn_folder, f'{stn_id}_adjusted_ppt.geojson')\n",
    "adj_poly_file = os.path.join(stn_folder, f'{stn_id}_adjusted_catchment.geojson')\n",
    "streams_file = os.path.join(stn_folder, f'{stn_id}_streams.geojson')\n",
    "\n",
    "hs_pt = gpd.read_file(hs_pt_file)\n",
    "hs_poly = gpd.read_file(hs_poly_file)\n",
    "adj_pt = gpd.read_file(adj_pt_file)\n",
    "adj_poly = gpd.read_file(adj_poly_file)\n",
    "streams = gpd.read_file(streams_file)\n",
    "plt = pour_point_plot(stn_id, hs_pt, hs_poly, adj_pt, adj_poly, streams)\n",
    "show(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d9cc3-9d69-4cfb-8e7b-85b23a7c5e13",
   "metadata": {},
   "source": [
    "## Re-delineate basins with adjusted pour points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cee732-4157-4dad-9e67-6b7c3523bc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stn_id, pt in checked_stns.items():\n",
    "    if pt is None:\n",
    "        continue\n",
    "    \n",
    "    bcub_data = bcub_gdf[bcub_gdf['Official_ID'] == stn_id].copy()\n",
    "    rc = bcub_data['region_code'].values[0]\n",
    "    area = bcub_data['Drainage_Area_km2'].values[0]\n",
    "        \n",
    "    pt.to_crs(3005, inplace=True)\n",
    "    \n",
    "    stn_folder = os.path.join(updated_catchment_folder, stn_id)\n",
    "    if not os.path.exists(stn_folder):\n",
    "        os.makedirs(stn_folder)\n",
    "        \n",
    "    # accumulation raster path\n",
    "    acc_dem_path = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005_accum.tif')\n",
    "    if not os.path.exists(acc_dem_path):\n",
    "        print('missing ', acc_dem_path)\n",
    "        continue\n",
    "    assert os.path.exists(acc_dem_path), f'{acc_dem_path} not found'\n",
    "    \n",
    "    print(f'processing {stn_id} in {rc} region ({area:.2f} km²)')\n",
    "    stn_data = bcub_gdf[bcub_gdf['Official_ID'] == stn_id].copy()\n",
    "\n",
    "    # 3) find the nearest stream cell to the UPDATED station location\n",
    "    adjusted_ppt_path = os.path.join(stn_folder, f'{stn_id}_REadjusted_ppt.geojson')\n",
    "    adjusted_ppt_path_shp = os.path.join(stn_folder, f'{stn_id}_REadjusted_ppt.shp')    \n",
    "    if not os.path.exists(adjusted_ppt_path) | os.path.exists(adjusted_ppt_path_shp):\n",
    "        print('    ...processing adjusted pour point')\n",
    "        nearest_pt, distance = snap_pour_point(acc_dem_path, pt, area, distance_tol=1000)\n",
    "        adj_pt = gpd.GeoDataFrame(geometry=[nearest_pt], crs=pt.crs)\n",
    "        adj_pt['Official_ID'] = stn_id\n",
    "        adj_pt.to_file(adjusted_ppt_path)\n",
    "        adj_pt.to_file(adjusted_ppt_path_shp)\n",
    "        \n",
    "    # 4) delineate a new catchment from the adjusted point\n",
    "    adjusted_catchment_path = os.path.join(stn_folder, f'{stn_id}_REadjusted_catchment.geojson')\n",
    "    if not os.path.exists(adjusted_catchment_path):\n",
    "        print('    ...delineating basin raster')\n",
    "        catchment_raster_fpath = delineate_new_catchment(adjusted_ppt_path_shp, rc, stn_id, stn_folder)\n",
    "        adjusted_catchment = raster_to_vector_basin(rc, catchment_raster_fpath, stn_id, stn_folder)\n",
    "        adjusted_catchment.to_file(adjusted_catchment_path)\n",
    "        \n",
    "    # 5) save streamlines as a vector within some distance of the catchment polygon / ppt.\n",
    "    streams_path = os.path.join(stn_folder, f'{stn_id}_streams_adjusted.geojson')\n",
    "    if not os.path.exists(streams_path):\n",
    "        print('    ...processing streams vectors')\n",
    "        streams_temp_path = generate_stream_vectors(stn_data, adjusted_catchment_path, stn_folder)\n",
    "        gdf = gpd.read_file(streams_temp_path)\n",
    "        gdf.to_file(streams_path)\n",
    "        \n",
    "        remove_extensions = ['.dbf', '.prj', '.shp', '.shx', '.tif', '.cpg']\n",
    "        if os.path.exists(adjusted_catchment_path):\n",
    "            for f in os.listdir(stn_folder):\n",
    "                if any([f.endswith(e) for e in remove_extensions]):\n",
    "                    os.remove(os.path.join(stn_folder, f))\n",
    "        \n",
    "        print(f'   ...processed {stn_id}, saved to {streams_path}')  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3d7d0-166e-4bc0-a25f-4c0d72a9e455",
   "metadata": {},
   "source": [
    "### Revise the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96cedb-bb62-4de3-8659-e2f51df2eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stn_id, pt in checked_stns.items():\n",
    "    if pt is None:\n",
    "        continue\n",
    "    process_plot(stn_id, adjusted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c22ebd-396b-4c51-bf07-8a99785846a6",
   "metadata": {},
   "source": [
    "### Final review notes (excluded stations)\n",
    "\n",
    "1. **15087200**: Hammer Slough at Petersburg doesn't render from effect of bypass road  \n",
    "2. **07FD913**: Young Drainage Project near Spirit River -- no idea!  \n",
    "3. **07FD912**: Whitburn Drainage Project Near Spirit River -- no idea!\n",
    "4. **12113349**: Mill Creek doesn't resolve with 1 arcsecond DEM in urban area\n",
    "5. **15052475**: Jordan Creek doesn't resolve with 1 arcsecond DEM in urban area\n",
    "6. **12110400**: Jenkins Creek doesn't resolve with 1 arcsecond DEM in urban area\n",
    "7. **15053200**: Duck Creek doesn't resolve with 1 arcsecond DEM in urban area\n",
    "8. **12212430**: Unnamed Trip to Bertrand Creek Creek doesn't resolve with 1 arcsecond DEM in urban area \n",
    "9. **08EG013**: Boneyard Creek at Outlet of Rainbow Lake is lakes connected by outlet dams raising water level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee07a5-226c-4037-a93b-c505c45a98d6",
   "metadata": {},
   "source": [
    "## Assemble final catchment bounds into a dataframe to be used in subsequent computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7082e-608c-41d5-96ba-f06187b91a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_geometry_fname = f'BCUB_watershed_bounds_updated.geojson'\n",
    "\n",
    "excluded_stns = ['15087200', '07FD913', '07FD912', '12113349', '15052475',\n",
    "                '12110400', '15053200', '12212430', '08EG013']\n",
    "\n",
    "revised_geometry_folder = os.path.join(os.getcwd(), 'data/catchment_polygons/updated_catchment_set')\n",
    "revised_stns = [e for e in remaining_stns['Official_ID'] if e not in excluded_stns]\n",
    "print(f'{len(revised_stns)} revised catchment bounds')\n",
    "bcub_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i, row in bcub_gdf.iterrows():\n",
    "    stn_id = row['Official_ID']\n",
    "    if stn_id in revised_stns:\n",
    "        stn_geometry_folder = os.path.join(revised_geometry_folder, stn_id)\n",
    "    \n",
    "        # if the station has been revised, there should be new geometry\n",
    "        # either it worked the first time, or the pour point was adjusted and\n",
    "        # the catchment file is ...REadjusted_catchment.geojson\n",
    "        if stn_id in revised_stns:\n",
    "            revised_ppt_catchment_fpath = os.path.join(stn_geometry_folder, f'{stn_id}_REadjusted_catchment.geojson')\n",
    "            if os.path.exists(revised_ppt_catchment_fpath):\n",
    "                revised_catchment_fpath = revised_ppt_catchment_fpath\n",
    "            else:\n",
    "                revised_catchment_fpath = os.path.join(stn_geometry_folder, f'{stn_id}_adjusted_catchment.geojson')\n",
    "            \n",
    "            assert os.path.exists(revised_catchment_fpath), f'revised catchment file not found for {stn_id}'\n",
    "            \n",
    "            catchment_df = gpd.read_file(revised_catchment_fpath)\n",
    "            # also need to update the centroid geom\n",
    "            centroid_df = catchment_df.copy()\n",
    "            # need to compute centroid in projected crs to get coords\n",
    "            centroid_x, centroid_y = centroid_df.geometry.centroid.x.values[0], centroid_df.geometry.centroid.y.values[0]\n",
    "            # then create a dataframe to reproject to \n",
    "            centroid_df = gpd.GeoDataFrame(geometry=[Point(centroid_x, centroid_y)], crs=catchment_df.crs)\n",
    "            # then reproject back to decimal degrees (geographic crs)\n",
    "            centroid_df.to_crs(4326, inplace=True)\n",
    "            centroid_x, centroid_y = centroid_df.geometry.x.values[0], centroid_df.geometry.y.values[0]\n",
    "            bcub_gdf.loc[i, 'geometry'] = catchment_df.geometry.values[0]\n",
    "            bcub_gdf.loc[i, 'Centroid_Lat_deg_N'] = centroid_y\n",
    "            bcub_gdf.loc[i, 'Centroid_Lon_deg_E'] = centroid_x\n",
    "            bcub_gdf.loc[i, 'geometry_updated'] = True\n",
    "            \n",
    "\n",
    "bcub_gdf = bcub_gdf[~bcub_gdf['Official_ID'].isin(excluded_stns)]\n",
    "bcub_gdf.to_file(os.path.join('data', revised_geometry_fname))\n",
    "print(f'file saved to {revised_geometry_fname}')\n",
    "print(len(bcub_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f55512-0340-4b33-8bdb-e538b2358ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_gdf[bcub_gdf['Official_ID'].isin(revised_stns)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7bf5c-a2f3-4249-a340-2bd2e9d4b933",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291becb6-22b4-4b5e-8e40-49f44f3b7257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
