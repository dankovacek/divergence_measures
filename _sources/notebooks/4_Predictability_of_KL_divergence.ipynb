{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bd3e2d-4130-4fd8-8d0e-aebdadd4d447",
   "metadata": {},
   "source": [
    "# Predictability of Kullback-Leibler (KL) Divergence\n",
    "\n",
    "In the first steps, we looked at the predictability of a common hydrological signature and an uncommon information measure, both computed on individual distributions.  If the entropy measure is reasonably predictable from attributes, then the uncertainty of unmonitored locations might be predictable enough for some applications.  \n",
    "\n",
    "Here we make a comparison of runoff between **many pairs of locations** and ask if the **Kullback-Leibler Divergence** (KLD) of the distribution of unit area runoff between two locations can be predicted from the attributes of both catchments (and their differences). We will again use the gradient boosted decision tree method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9864ab-af00-457b-bfe0-2e69ac18b2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fdc7bcd-88cd-45b1-8c47-3a3b0a204046",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
