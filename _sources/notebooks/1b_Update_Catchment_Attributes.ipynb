{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec6f101-baba-407b-bbd0-da0b48b55d58",
   "metadata": {},
   "source": [
    "# Update HYSETS Catchment Attributes\n",
    "\n",
    "After checking for updated catchment polygons, the attributes associated with monitored station polygons are updated.  The majority of the preprocessing work is detailed in {cite}`kovacek2024bcub` and example notebooks are provided in the [jupyter book associated with that publication](https://dankovacek.github.io/bcub_demo/0_intro.html) detailing all of these steps. Where more recent catchment boundary information was found for monitoring stations in the preceding chapter, the updated polygons are used to revise catchment attributes.  The effect is most pronounced where the HYSETS \"artificial boundaries\" flagged catchments represented attributes with the nearest raster pixel or where the polygon was simply a square of area equal to that reported in official sources centred at the reported station location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a177c0f-f2aa-4de6-b7b0-157fc24a1f49",
   "metadata": {},
   "source": [
    "## Compare updated results with HYSETS attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307468b6-9e51-4a62-ab31-c2502e35fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import Point\n",
    "from time import time\n",
    "from attribute_processing_functions import *\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ca7c9-f759-4664-a1c8-947ea056d212",
   "metadata": {},
   "source": [
    "### Load the original hysets data and the pre-processed results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ab0d0-e47c-440e-b93b-2a552f7dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_attributes = ['prcp', 'tmin', 'tmax', 'vp', 'swe', 'srad', 'low_prcp_duration', 'low_prcp_freq', 'high_prcp_duration', 'high_prcp_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e2c85-078c-4b19-a1b7-eb3735f1221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "ab_flag_stns = hs_df[hs_df['Flag_Artificial_Boundaries'] == 1]['Official_ID'].values\n",
    "print(f'{len(ab_flag_stns)}/{len(hs_df)} HYSETS boundaries have ab flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36f026-9eb2-4c69-852f-d822a018d37f",
   "metadata": {},
   "source": [
    "Look ahead at the results of this chapter to compare the updated values with the HYSETS attributes.  The remainder of this chapter following the plots below computes the updated catchment attributes that we see compared here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add90d08-ebac-4c68-8b21-9263d2711495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "updated_catchment_attribute_path = os.path.join(os.getcwd(), 'data/BCUB_watershed_attributes_updated.geojson')\n",
    "attributes_fpath = updated_catchment_attribute_path.replace('.geojson', '.csv')\n",
    "\n",
    "if os.path.exists(attributes_fpath):\n",
    "    attributes_df = pd.read_csv(attributes_fpath)\n",
    "    attributes_df.head()    \n",
    "else:\n",
    "    results_df = gpd.read_file(updated_catchment_attribute_path)\n",
    "    attributes_df = results_df[[c for c in results_df.columns if c != 'geometry']].copy()\n",
    "    attributes_df.to_csv(attributes_fpath, index=False)\n",
    "\n",
    "attributes_df.set_index('Official_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c45dc-e4d8-44ad-b255-c9683d7f6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the unaltered hysets attributes for stations in the results dataframe\n",
    "og_df = hs_df[hs_df['Official_ID'].isin(attributes_df.index)].copy()\n",
    "og_df.set_index('Official_ID', inplace=True)\n",
    "# the soil permeability and porosity column names need to be updated\n",
    "og_df.rename({'Permeability_logk_m2': 'logk_ice_x100', 'Porosity_frac': 'porosity_x100'}, axis=1, inplace=True)\n",
    "len(og_df)\n",
    "og_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98a086-2da0-4b80-b801-db0e9273a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the attributes of interest (climate are not included in original as such)\n",
    "attributes = [\n",
    "    'logk_ice_x100', 'porosity_x100',\n",
    "    'Slope_deg', 'Aspect_deg', 'Elevation_m', 'Drainage_Area_km2', \n",
    "    'Land_Use_Forest_frac', 'Land_Use_Shrubs_frac', 'Land_Use_Grass_frac',\n",
    "    'Land_Use_Wetland_frac', 'Land_Use_Crops_frac', 'Land_Use_Urban_frac',\n",
    "    'Land_Use_Water_frac', 'Land_Use_Snow_Ice_frac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae59a3-1e26-4b79-a283-4d32166c817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df, a, ab_flag_stns):\n",
    "    min_val, max_val = df.min().min(), df.max().max()\n",
    "    # Create a new plot with a title and axis labels\n",
    "    p = figure(title=a)\n",
    "    if a.lower() == 'drainage_area_km2':\n",
    "        p = figure(title=a, x_axis_type='log', y_axis_type='log')\n",
    "        \n",
    "    df['stn_id'] = df.index  # Make sure the index column is available for tooltips\n",
    "    df['ab_flag'] = [True if e in ab_flag_stns else False for e in df.index]\n",
    "    flag_df = df[df['ab_flag'] == True].copy()\n",
    "    noflag_df = df[df['ab_flag'] == False].copy()\n",
    "    flag_source = ColumnDataSource(flag_df)\n",
    "    noflag_source = ColumnDataSource(noflag_df)\n",
    "    # Add a scatter renderer with circle markers\n",
    "    p.scatter(\n",
    "        x='original', y='revised', size=3, color=\"dodgerblue\", alpha=0.6, source=noflag_source,\n",
    "        legend_label='no_flag'\n",
    "    )\n",
    "    p.scatter(\n",
    "        x='original', y='revised', size=3, color=\"orange\", alpha=0.6, source=flag_source,\n",
    "        legend_label='ab_flag'\n",
    "    )\n",
    "\n",
    "    # Add a HoverTool to show the index\n",
    "    hover = HoverTool()\n",
    "    hover.tooltips = [\n",
    "        (\"ID\", \"@stn_id\"),\n",
    "    ]\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    \n",
    "    x = np.linspace(min_val, max_val, 1000)\n",
    "    y = x\n",
    "    p.line(x, y, legend_label='1:1', color='red', line_width=3, line_dash='dashed')\n",
    "    \n",
    "    # Set axis labels\n",
    "    p.xaxis.axis_label = 'original'\n",
    "    p.yaxis.axis_label = 'updated'\n",
    "    p.legend.click_policy = 'hide'\n",
    "    p.legend.location = 'top_left'\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53277b89-c829-469a-9205-4dd4f5b8d037",
   "metadata": {},
   "source": [
    "### View scatter plots of HYSETS vs. updated attributes\n",
    "\n",
    "Approximately 25% of the stations we evaluated had an \"artificial bounds\" flag, meaning that catchment geometries were not available from official sources.  These catchment boundaries were approximated by a square centred at the \"centroid\" coordinates which were stated in the HYSET paper to reflect the reported station location.  Below we see the attributes based on updated values are quite different, in particular those which were updated from revised official sources (ab_flag).\n",
    "\n",
    "The soil attributes describe a marked difference between studies.  This may be because this study uses the GLHYMPS 2.0 version {cite}`huscroft2018compiling` [DOI: https://doi.org/10.1002/2017GL075860](https://doi.org/10.1002/2017GL075860).  The source used in HYSETS (https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP2/DLGXYO) is {cite}'gleeson2018' [DOI: https://doi.org/10.5683/SP2/DLGXYO](https://doi.org/10.5683/SP2/DLGXYO)\n",
    "\n",
    "\n",
    "```{note}\n",
    "ab_flag = Artificial boundaries \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62b81d-b4d9-43ec-8e6c-bddfdb2d4b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots = []\n",
    "for a in attributes:\n",
    "    result_a = a    \n",
    "    if a.startswith('Land_Use'):\n",
    "        result_a += '_2010'\n",
    "    og_vals = og_df[[a]].copy().rename({a: 'original'}, axis=1)    \n",
    "    revised_vals = attributes_df[[result_a]].copy().rename({result_a: 'revised'}, axis=1)\n",
    "    comp_df = pd.concat([og_vals, revised_vals], axis=1)\n",
    "    comp_df.dropna(inplace=True, how='any')\n",
    "    if a in ['logk_ice_x100', 'porosity_x100']:\n",
    "        comp_df['revised'] /=  100\n",
    "    plot = scatter_plot(comp_df, a, ab_flag_stns)\n",
    "    plots.append(plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb50734-8878-4403-a1ed-e0b79774360a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layout = gridplot(plots, ncols=3, width=350, height=325)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15319a-251b-4963-8a52-c40154aae3e3",
   "metadata": {},
   "source": [
    "## Load updated catchment polygons\n",
    "\n",
    "The data processing below is optional if you use the pre-processed (revised) attributes `BCUB_watershed_attributes_updated.csv`\n",
    "\n",
    "The file `BCUB_watershed_bounds_updated.geojson` is the end result of the preceding chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f68b1-3290-46c3-9276-36b6f69331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_catchment_geometry_fpath = 'data/BCUB_watershed_bounds_updated.geojson'\n",
    "if os.path.exists(revised_catchment_geometry_fpath):\n",
    "    bcub_gdf = gpd.read_file(revised_catchment_geometry_fpath)\n",
    "    geom_updated_stns = bcub_gdf[bcub_gdf['geometry_updated'] == 1]['Official_ID'].values\n",
    "    print(len(geom_updated_stns))\n",
    "else:\n",
    "    print('Revisit the preceding chapter to generate revised catchment geometries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa56ba6-452e-40d1-8f09-ef64209bd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_pts = bcub_gdf.copy()\n",
    "bcub_pts['geometry'] = bcub_pts.apply(lambda row: Point(row['Centroid_Lon_deg_E'], row['Centroid_Lat_deg_N']), axis=1)\n",
    "# we are overwriting the polygon geometry which is 3005\n",
    "bcub_pts = bcub_pts.set_crs(4326, allow_override=True)\n",
    "bcub_pts[['geometry']].head()\n",
    "if 'index_right' in bcub_pts.columns:\n",
    "    bcub_pts.drop('index_right', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2dd054-5fbf-4267-a090-b4a61351bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_pts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4654f-384f-4909-84fa-b31c6036d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the two added stations 08AG003 (YKR), 10ED002 (LRD)\n",
    "bcub_pts.loc[bcub_pts['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_pts.loc[bcub_pts['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "# assert len(bcub_pts[bcub_pts['region_code'] == None]) == 0\n",
    "\n",
    "region_codes = sorted(list(set(bcub_pts['region_code'])))\n",
    "print(len(bcub_pts), len(bcub_gdf))\n",
    "# make sure all rows have an associated region_code\n",
    "assert len([e for e in region_codes if e is None]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccc13c-6082-4e98-844c-003fbeb664c7",
   "metadata": {},
   "source": [
    "### Extract terrain, climate, land cover, and soil attributes\n",
    "\n",
    "Terrain attributes are extracted from 1-arc-second DEM available at the USGS [National Map Downloader](https://apps.nationalmap.gov/downloader/#/).\n",
    "\n",
    "In the GLHYMPS dataset, the attributes are truncated (.shp truncates at 10 symbols):\n",
    "* porosity: `Porosity_x`,\n",
    "* permeability: `logK_Ice_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643d2f3-46ef-4a17-bff1-69aac4d004c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcub_data_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub'\n",
    "dem_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub/processed_data/processed_dem/'\n",
    "# dem_folder = '/home/danbot/Documents/code/23/bcub/processed_data/processed_dem/'\n",
    "local_data_folder = 'data/geospatial_layers/'\n",
    "glhymps_folder = os.path.join(os.getcwd(), local_data_folder, 'glhymps')\n",
    "nalcms_folder = os.path.join(os.getcwd(), local_data_folder, 'nalcms')\n",
    "daymet_folder = os.path.join(os.getcwd(), local_data_folder, 'daymet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e895232-cbc6-4253-91ae-c860043a7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nalcms_dict = {}\n",
    "for y in [2010, 2015, 2020]:\n",
    "    nalcms_fpath = os.path.join(nalcms_folder, f'NA_NALCMS_landcover_{y}_3005_clipped.tif')\n",
    "    nalcms_dict[y] = rxr.open_rasterio(nalcms_fpath, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1db69-7ff5-4533-9ceb-6536cb3ef9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_dict = {}\n",
    "for c in daymet_attributes:\n",
    "    fpath = os.path.join(daymet_folder, f'{c}_mosaic_3005.tiff')\n",
    "    climate_dict[c] = rxr.open_rasterio(fpath, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9418592-0745-42c5-94bf-ed1b8e7700c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps_data = gpd.read_file(os.path.join(glhymps_folder, 'GLHYMPS_clipped_3005.geojson'))\n",
    "glhymps_data.geometry = glhymps_data.geometry.make_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd06500-7445-425b-8d52-0ff9feac0e4f",
   "metadata": {},
   "source": [
    "### Re-process catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44703394-a7d7-4bc4-bc9e-345840d3a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "def get_merged_dem(basin_geom):\n",
    "    \"\"\"\n",
    "    There is one case (10ED002) where the catchment covers two regions,\n",
    "    so we need to load and merge the two region dem files to process terrain attributes.    \n",
    "    \"\"\"\n",
    "    r1_path = os.path.join(dem_folder, f'10E_USGS_3DEP_3005.tif')\n",
    "    r2_path = os.path.join(dem_folder, f'LRD_USGS_3DEP_3005.tif')\n",
    "    r1_dem, dem_crs, dem_affine = retrieve_raster(r1_path)\n",
    "    r2_dem, dem_crs, dem_affine = retrieve_raster(r2_path)\n",
    "\n",
    "    merged_raster = merge_arrays([r1_dem, r2_dem])\n",
    "    masked_raster = merged_raster.rio.clip(basin_geom.geometry, merged_raster.rio.crs)\n",
    "    \n",
    "    return masked_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767a7fd-d162-4b47-989c-c24f8eef3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_catchment_attributes(rc, row, region_dem, crs):\n",
    "\n",
    "    stn_id = row['Official_ID']\n",
    "    \n",
    "    t0 = time.time()\n",
    "    basin_data = {}\n",
    "    basin_data['region'] = rc\n",
    "    basin_data['Official_ID'] = stn_id\n",
    "    basin_data['geometry'] = row['geometry']\n",
    "    basin_data['Drainage_Area_km2'] = round(row['geometry'].area / 1e6, 1)\n",
    "    basin_data['Centroid_Lon_deg_E'] = row['Centroid_Lon_deg_E']\n",
    "    basin_data['Centroid_Lat_deg_N'] = row['Centroid_Lat_deg_N']\n",
    "        \n",
    "    basin_polygon = gpd.GeoDataFrame(geometry=[row['geometry']], crs=crs)  \n",
    "    basin_polygon.geometry = basin_polygon.geometry.buffer(0)\n",
    "    \n",
    "    if not basin_polygon.is_valid.all():\n",
    "        basin_polygon.geometry = basin_polygon.geometry.make_valid()\n",
    "        if not basin_polygon.is_valid.all():\n",
    "            raise Exception('arg')\n",
    "        else:\n",
    "            print(f'Fixed invalid basin polygon geometry for {stn_id}.')\n",
    "\n",
    "    # process soil attributes\n",
    "    soil_masked = gpd.clip(glhymps_data, mask=basin_polygon)\n",
    "    soil_masked = soil_masked[soil_masked.geometry.area > 1.0]   \n",
    "    soil_masked.geometry = soil_masked.geometry.buffer(0)\n",
    "    soil_masked.geometry = soil_masked.geometry.make_valid()\n",
    "    \n",
    "    assert all(soil_masked.is_valid)    \n",
    "    porosity = get_soil_properties(soil_masked, 'Porosity_x')\n",
    "    permeability = get_soil_properties(soil_masked, 'logK_Ice_x')\n",
    "    basin_data['logk_ice_x100'] = round(permeability, 2)\n",
    "    basin_data['porosity_x100'] = round(porosity, 5)\n",
    "    del soil_masked\n",
    "    \n",
    "    # process NALCMS land cover\n",
    "    for y in [2010, 2015, 2020]:\n",
    "        # nalcms_fpath = os.path.join(nalcms_folder, f'NA_NALCMS_landcover_{y}_3005_clipped.tif')\n",
    "        # clipped_land_cover = rxr.open_rasterio(nalcms_fpath, masked=True).rio.clip(basin_polygon.geometry, all_touched=True)\n",
    "        clip_ok, clipped_nalcms = clip_raster_to_basin(basin_polygon, nalcms_dict[y])\n",
    "        land_cover = process_lulc(i, basin_polygon, clipped_nalcms, y)\n",
    "        land_cover = land_cover.to_dict('records')[0]\n",
    "        basin_data.update(land_cover)\n",
    "\n",
    "    # process terrain\n",
    "    # make a special case for 10ED002 where we need to load and merge\n",
    "    # the rasters for LRD and 10E and merge\n",
    "    del clipped_nalcms\n",
    "    if stn_id == '10ED002':\n",
    "        print(f'processing special case: {stn_id}')\n",
    "        clipped_dem = get_merged_dem(basin_polygon)\n",
    "    else:\n",
    "        dem_fpath = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005.tif')\n",
    "        assert os.path.exists(dem_fpath)\n",
    "        clip_ok, clipped_dem = clip_raster_to_basin(basin_polygon, region_dem)\n",
    "\n",
    "        slope, aspect = calculate_slope_and_aspect(clipped_dem)\n",
    "        # print(f'aspect, slope: {aspect:.1f} {slope:.2f} ')\n",
    "        basin_data['Slope_deg'] = slope\n",
    "        basin_data['Aspect_deg'] = aspect\n",
    "    \n",
    "        mean_el, median_el, min_el, max_el = process_basin_elevation(clipped_dem)\n",
    "        basin_data['median_el'] = median_el\n",
    "        basin_data['mean_el'] = mean_el\n",
    "        basin_data['max_el'] = max_el\n",
    "        basin_data['min_el'] = min_el\n",
    "        basin_data['Elevation_m'] = mean_el\n",
    "\n",
    "    # process climate params\n",
    "    del clipped_dem\n",
    "    for climate_param in daymet_attributes:\n",
    "        clip_ok, clipped_data = clip_raster_to_basin(basin_polygon, climate_dict[climate_param])\n",
    "        # Check if the clipped raster is empty or has no data\n",
    "        if clipped_data is None:\n",
    "            print(f'clip is empty, finding nearest point from polygon centroid')\n",
    "            # If the clipped raster is empty or contains only NaN, find the nearest value\n",
    "            spatial_mean = find_nearest_raster_value(climate_dict[climate_param], basin_polygon)            \n",
    "        else:\n",
    "            spatial_mean = round(clipped_data.mean(dim=['y', 'x']).item(), 1)\n",
    "            \n",
    "        basin_data[climate_param] = spatial_mean\n",
    "            # basin_polygon.to_file(f'{stn_id}_error.geojson')\n",
    "            # raise Exception(f'issue with {climate_param}')\n",
    "        \n",
    "    return basin_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7210fb-25c0-4e00-8a0e-115f8518aa89",
   "metadata": {},
   "source": [
    "### Load existing results, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c43f0-72eb-4c09-8132-059692159ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_basin_data = []\n",
    "t0 = time.time()\n",
    "\n",
    "results_df, processed_ids = pd.DataFrame(), []\n",
    "if os.path.exists(updated_catchment_attribute_path):\n",
    "    print(f'{updated_catchment_attribute_path.split(\"/\")[-1]} exists, loading existing file.')\n",
    "    results_df = gpd.read_file(updated_catchment_attribute_path)\n",
    "    print(f'{len(results_df)} existing results loaded')\n",
    "    processed_ids = results_df['Official_ID'].values.tolist()\n",
    "else:\n",
    "    print('No existing results found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c016a-af27-4b52-a9df-4226c85eb70e",
   "metadata": {},
   "source": [
    "### Process monitored catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823c516-155b-4e48-b7f8-bb43cc23d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "for rc in region_codes:\n",
    "    print(f'Processing {rc} region catchments')\n",
    "    batch_df = bcub_gdf[bcub_gdf['region_code'] == rc].copy()\n",
    "    # batch_df = batch_df[~batch_df['Official_ID'].isin(processed_ids)].copy()\n",
    "    dem_fpath = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005.tif')\n",
    "    assert os.path.exists(dem_fpath), f'{dem_fpath} not found'\n",
    "    region_dem = rxr.open_rasterio(dem_fpath, mask_and_scale=True)\n",
    "    for i, row in batch_df.iterrows():\n",
    "        stn_id = row['Official_ID']\n",
    "\n",
    "        result = process_catchment_attributes(rc, row, region_dem, bcub_gdf.crs)\n",
    "        \n",
    "        batch_results.append(result)\n",
    "        processed_ids.append(stn_id)\n",
    "        if (len(batch_results) % 200 == 0) | (len(processed_ids) >= len(bcub_gdf) - 1):\n",
    "            new_results = gpd.GeoDataFrame(batch_results, crs='EPSG:3005')\n",
    "            results_df = gpd.GeoDataFrame(pd.concat([results_df, new_results]), crs='EPSG:3005')    \n",
    "            batch_results = []\n",
    "            print('     ...saving output file.')\n",
    "            results_df.to_file(updated_catchment_attribute_path, index=False)\n",
    "            n_unique = len(list(set(results_df['Official_ID'])))\n",
    "            print(f'    ...saved {len(results_df)} results file ({n_unique} unique station ids).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c61f5c-5df5-41f5-908b-b81fac43ea1f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a4a3a-3dfe-40f9-9ce0-d91528e5517c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26811713-11c5-4b80-97b7-1e74688f10af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
