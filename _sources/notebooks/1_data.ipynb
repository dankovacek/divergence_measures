{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "```{figure} ../images/study_basins_and_regions.png\n",
    "---\n",
    "alt: Study region polygons and HYSETS monitored catchment polygons.\n",
    "class: bg-primary mb-1\n",
    "name: study-region-fig\n",
    "width: 500px\n",
    "align: center\n",
    "---\n",
    "Study region polygons (purple outline) and HYSETS monitored catchments (yellow).  \n",
    "```\n",
    "\n",
    "```{note}\n",
    "Before proceeding with the computations in the notebook, the streamflow time series and (optionally) catchment boundaries from the HYSETS dataset must be downloaded from the [HYSETS open data repository](https://osf.io/rpc3w/).  Some data are provided in the `data/` folder as part of this repository.  Data pre-processing can be skipped by downloading the input data files from (add dataset repository link)\n",
    "```\n",
    "\n",
    "The data used in this study comes from *The Hydrometeorological Sandbox École de technologie supérieure* (HYSETS) {cite}`arsenault2020comprehensive`.  The HYSETS data, including streamflow time series and attributes for 14,425 catchments can be accessed at [https://osf.io/rpc3w/](https://osf.io/rpc3w/).  We use a subset of approximately 1620 catchments contained in major basins covering and bounding British Columbia, as shown in {numref}`Figure {number} <study-region-fig>`.  Ten climate indices were processed from [Daymet](https://daymet.ornl.gov/) for this subset, for details see *BCUB - A large sample ungauged basin attribute dataset for British Columbia, Canada* {cite}`kovacek2024bcub` ([https://doi.org/10.5194/essd-2023-508](https://doi.org/10.5194/essd-2023-508)).  \n",
    "\n",
    "\n",
    "For this experiment we use the following files:\n",
    "\n",
    "* **Daily average streamflow time series**: filenames follow the convention `<official_id>.csv`.  These should be downloaded from the open data repository linked above and saved under `data/hysets_streamflow_timeseries/`\n",
    "* **Catchment attributes**: filename: `BCUB_watershed_properties_with_climate.csv`.   This file is provided in the `data/` folder, and it was modified from the original file `HYSETS_watershed_properties.txt` in the HYSETS dataset with ten added climate indices as described in {cite}`kovacek2024bcub`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing overview\n",
    "\n",
    "Note that these steps are optional and the results of these pre-processing steps are provided in the open data repository.\n",
    "\n",
    "1) Process climate indices for HYSETS catchments (optional, pre-processed attributes are contained in `BCUB_watershed_properties_with_climate.csv`)\n",
    "2) Define testing parameters:\n",
    "    * **Bitrate:** the number of quantization levels to encode the streamflow time series $N_s = 2^b$.  This is done to test the sensitivity of the predictive model to information loss.   \n",
    "    * **Prior**: when computing the Kullback-Leibler divergence ($D_{KL} (P||Q) = P \\text{log}\\frac{P}{Q}$), the simulated distribution Q can't contain zero probabilities, in other words we must prevent the model from saying observed states are impossible.  This is achieved by applying a prior distribution to q (the simulated series) in order to avoid division by zero.  The KL divergence is then computed on the posterior $Q'$.\n",
    "    * **Minimum record length:** we set the minimum record length to 1 year in order to see the sensitivity of the model to record length,\n",
    "    * **Record \"completeness\":** a (hydrological) year must be at least 90% complete in terms of daily mean observations.\n",
    "    * **Partial counts**: another approach to incorporating measurement uncertainty is to assign a (relative) error interval around each observation. The quantization step then computes counts based on the proportion of the error interval covered by each bin.  This has a smoothing effect on the discrete PMF, with an increasing effect as the bitrate (number of bins / quantization levels) increases and bin intervals shrink.\n",
    "3) Compute the (Shannon) entropy of the streamflow time series for each bitrate and prior.\n",
    "4) Compute the KL divergence (KLD), Wasserstein distance (WD), and total variation distance (TVD) for each pair of stations meeting the minimum record / minimum concurrency criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and inspect catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the catchments used in the study\n",
    "attributes_filename = 'BCUB_HYSETS_properties_with_climate.csv'\n",
    "df = pd.read_csv(os.path.join('data', 'BCUB_HYSETS_properties_with_climate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of unique stations in the dataset\n",
    "unique_stations = np.unique(df['official_id'])\n",
    "print(f'{len(unique_stations)} unique monitored catchments in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the locations (centroids) of the catchments\n",
    "centroids = df.apply(lambda row: Point(row['centroid_lon_deg_e'], row['centroid_lat_deg_n']), axis=1)\n",
    "# convert to geodataframe\n",
    "gdf = gpd.GeoDataFrame(df, geometry=centroids, crs='EPSG:4269')\n",
    "# convert coordinate reference system to 3857 for plotting\n",
    "gdf = gdf.to_crs(3857)\n",
    "bbox = gdf.geometry.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the catchment centroid locations\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import Colorblind\n",
    "output_notebook()\n",
    "\n",
    "# range bounds supplied in web mercator coordinates\n",
    "p = figure(x_axis_type=\"mercator\", y_axis_type=\"mercator\", width=700, height=400,\n",
    "          x_range=(bbox[0], bbox[2]), y_range=(bbox[1], bbox[3]))\n",
    "p.add_tile(\"CartoDB Positron\", retina=True)\n",
    "p.scatter(x=gdf.geometry.x, y=gdf.geometry.y, color='orange', size=4)\n",
    "\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import streamflow timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing_functions as dpf\n",
    "\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "BASE_DIR = os.getcwd()\n",
    "STREAMFLOW_DIR = os.path.join(BASE_DIR, 'data', 'hysets_streamflow_timeseries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading streamflow time series for a pair of monitoring stations\n",
    "s1, s2 = unique_stations[0], unique_stations[1]\n",
    "print(s1, s2)\n",
    "test_df = dpf.retrieve_nonconcurrent_data(s1, s2)\n",
    "flow_fig = figure(width=700, height=350, x_axis_type='datetime')\n",
    "flow_fig.line(test_df.index, test_df[unique_stations[0]], color='navy', legend_label=unique_stations[0])\n",
    "flow_fig.line(test_df.index, test_df[unique_stations[1]], color='dodgerblue', legend_label=unique_stations[1])\n",
    "flow_fig.yaxis.axis_label = r'$$\\text{Flow } \\frac{m^3}{s}$$'\n",
    "flow_fig.xaxis.axis_label = r'$$\\text{Date}$$'\n",
    "show(flow_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon entropy processing\n",
    "\n",
    "Compute the Shannon entropy of individual streamflow time series.  The Shannon entropy is given by: $$H(X) = \\sum P \\log P$$\n",
    "\n",
    "The entropy is computed for various quantization bit depths (`bitrate` parameter).  No prior is applied here.\n",
    "\n",
    "From the scipy.stats [docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html):\n",
    ">*\"If messages consisting of sequences of symbols from a set are to be encoded and transmitted over a noiseless channel, then the Shannon entropy H(pk) gives a tight lower bound for the average number of units of information [bits] needed per symbol if the symbols occur with frequencies governed by the discrete distribution pk.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new output filename \n",
    "output_filename = attributes_filename.replace('.csv', f'_with_entropy.csv')\n",
    "output_fpath = os.path.join('data', output_filename)\n",
    "\n",
    "# the bitrate dictates the number of quantization levels = 2**b, i.e. 4 bits = 16 levels\n",
    "quant_labels = []\n",
    "if not os.path.exists(output_fpath):\n",
    "    bitrates = [4, 6, 8]\n",
    "    for bitrate in bitrates:\n",
    "        label = f'H_{bitrate}_bits'\n",
    "        print(f'Processing {bitrate} bit entropy')\n",
    "        df[label] = df.apply(lambda row: dpf.compute_observed_series_entropy(row, bitrate), axis=1)\n",
    "        quant_labels.append(label)\n",
    "    # save the results\n",
    "    df.to_csv(output_fpath, index=False)\n",
    "else:\n",
    "    df = pd.read_csv(output_fpath)\n",
    "    quant_labels = [e for e in df.columns if e.startswith('H')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the CDFs of entropy by quantization\n",
    "fig = figure(width=600, height=350, x_axis_label=r'$$H(X)$$', y_axis_label=r'$$P(H)$$')\n",
    "n = 0\n",
    "for l in quant_labels:\n",
    "    x, y = dpf.compute_cdf(df[l])\n",
    "    fig.line(x, y, legend_label=' '.join(l.split('_')[1:]), line_width=2, color=Colorblind[len(quant_labels)][n])\n",
    "    n += 1\n",
    "fig.legend.location = 'top_left'\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise f-divergence processing\n",
    "\n",
    "There are about 1.3 million pairings.  To speed up the processing and avoid losing progress, we process these in parallel in batches and save the results intermittently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# generate all combinations of pairs of station ids\n",
    "id_pairs = list(itertools.combinations(unique_stations, 2))\n",
    "print(f' There are {len(id_pairs)} unique pairings in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If true, the observation counts will incorporate observation error\n",
    "# and divide observation counts based on proportion of bin covered \n",
    "# by the error range\n",
    "partial_counts = [False, True]\n",
    "# set a revision date for the results output file\n",
    "revision_date = '20240717'\n",
    "\n",
    "# how many pairs to compute in each batch\n",
    "batch_size = 5000\n",
    "\n",
    "# what percentage of 365 observations in a year counts as a \"complete\" year\n",
    "completeness_threshold = 0.9\n",
    "min_observations = 365 * 0.9\n",
    "\n",
    "# station pairs with less than min_years concurrent years of data are excluded (for concurrent analysis),\n",
    "# stations with less than min_years are excluded (for non-concurrent analysis),\n",
    "min_years = 1 #[2, 3, 4, 5, 10]\n",
    "\n",
    "# a prior is applied to q in the form of a uniform array of 10**c pseudo-counts \"c\"\n",
    "# this prior is used to test the effect of the choice of prior on the model\n",
    "pseudo_counts = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# set the number of quantization levels to test, equal to 2^bitrate\n",
    "bitrates = [4, 6, 8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review, organize, and separate the attribute and metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_cols = [\n",
    "    'drainage_area_km2', 'elevation_m', 'slope_deg', 'gravelius', 'perimeter', 'aspect_deg', \n",
    "    'land_use_forest_frac_2010','land_use_grass_frac_2010', 'land_use_wetland_frac_2010',\n",
    "    'land_use_water_frac_2010', 'land_use_urban_frac_2010', 'land_use_shrubs_frac_2010', \n",
    "    'land_use_crops_frac_2010', 'land_use_snow_ice_frac_2010', 'logk_ice_x100', 'porosity_x100']\n",
    "\n",
    "climate_cols = [\n",
    "    'tmax', 'tmin', 'prcp', 'srad', 'swe', 'vp', \n",
    "    'high_prcp_freq', 'low_prcp_freq', 'high_prcp_duration', 'low_prcp_duration',\n",
    "]\n",
    "\n",
    "flag_cols = ['flag_shape_extraction', 'flag_terrain_extraction', 'flag_subsoil_extraction', 'flag_gsim_boundaries', 'flag_artificial_boundaries', 'flag_land_use_extraction']\n",
    "metadata_cols = [e for e in df.columns if e not in climate_cols + attr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 'process' variable is here so jupyter doesn't go computing \n",
    "# three million rows when the book is rebuilt.  This step is very time consuming.\n",
    "process = False\n",
    "if process:\n",
    "    for use_partial_counts in partial_counts:\n",
    "        for b in bitrates:\n",
    "            print(f'Processing pairs at {b} bits (concurrent data={use_partial_counts})')\n",
    "            results_fname = f'DKL_results_{b}bits_{revision_date}.csv'\n",
    "            if use_partial_counts == True:\n",
    "                results_fname = results_fname.replace('.csv', '_partial_counts.csv')\n",
    "\n",
    "            out_fpath = os.path.join('data/', results_fname)\n",
    "            existing_results = dpf.check_processed_results(out_fpath)\n",
    "            print(f'    {len(existing_results)} existing results loaded.')\n",
    "\n",
    "            if existing_results.empty:\n",
    "                id_pairs_filtered = id_pairs\n",
    "            else:\n",
    "                id_pairs_filtered = dpf.filter_processed_pairs(existing_results, id_pairs)\n",
    "\n",
    "            inputs = [(proxy_stn, target_stn, b, completeness_threshold, min_years, use_partial_counts, attr_cols, climate_cols, pseudo_counts) for proxy_stn, target_stn in id_pairs_filtered]\n",
    "\n",
    "            batch_files = dpf.process_pairwise_comparisons(inputs, b, results_fname, batch_size)\n",
    "\n",
    "            print(f'    Processed {len(sample_pairs)} pairs at ({b} bits) in {time() - t0:.1f} seconds')\n",
    "            print(f'    Concatenating {len(batch_files)} batch files.')\n",
    "\n",
    "            if len(batch_files) > 0:\n",
    "                all_results = pd.concat([pd.read_csv(f, engine='pyarrow') for f in batch_files], axis=0)\n",
    "                all_results.to_csv(out_fpath, index=False)\n",
    "                if os.path.exists(out_fpath):\n",
    "                    for f in batch_files:\n",
    "                        os.remove(f)\n",
    "                print(f'    Wrote {len(all_results)} results to {out_fpath}')\n",
    "            else:\n",
    "                print('    No new results to write to file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
