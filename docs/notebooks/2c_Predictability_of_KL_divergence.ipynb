{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bd3e2d-4130-4fd8-8d0e-aebdadd4d447",
   "metadata": {},
   "source": [
    "# Predict Kullback-Leibler (KL) Divergence\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Streamflow prediction in ungauged basins has classically focused on minimizing one or more loss functions, typically square error, NSE, and KGE, between observed (daily streamflow) values and predictions generated by some type of model, whether it be an empirical regionalization, statistical machine learning, or process-based rainfall runoff model. \n",
    "\n",
    "Regionalization and machine learning models for PUB rely on existing streamflow monitoring network, and the performance of these models is tied to how well the network represents the ungauged space.  This connection leads to the question of how the arrangement of streamflow monitoring stations within the network impacts the overall performance of PUB models, particularly in terms of expected prediction error across all ungauged locations. Furthermore, are there environmental signals, orthogonal to streamflow, that contain enough information to differentiate between network arrangements such that the prediction error over ungauged areas can be minimized?  \n",
    "\n",
    "A simple interpretation of the loss functions commonly used in the PUB literature might be \"how close are mean daily streamflow predictions to observed values?\"  A much simpler question to ask of observational data is: \"will a given model outperform random guessing in the long run?\".  This binary question represents a starting point to approach the optimal streamflow monitoring network problem.  The justification for asking such a basic question is that an expectation of the uncertainty reduction over the unmonitored space provided by a given monitoring arrangement supports a discriminant function to compare unique arrangements.  A simple question can be formulated to test on real data, in this case an ungauged space of over 1 million ungauged catchments and a set of over 1600 monitored catchments with which to train a model.\n",
    "\n",
    "The binary prediction problem is followed by a regresson problem where the goal is to minimize the expectation of prediction error based on the **Kullback-Leibler divergence** $D_{KL}$, (a surrogate loss function from the class of *information discriminant* measures which is consistent with the exponential loss {cite}`nguyen2009surrogate`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95290111-10c4-461b-9194-df35f8f3ad3c",
   "metadata": {},
   "source": [
    "## Data Preview\n",
    "\n",
    "As a first step, it's important to understand how the sample is distributed over the target variable that we're trying to predict.  The distrbution reflects what we're asking the model to tell us, and it helps set our expectations as far as interpreting the model loss.  For example, if we know that the range of our target variable is bounded in (0, 10], if our predictive model produces mean absolute loss or RMSE of something like 1, is this a good result?  It depends 1) on the application if we can accept such an error, but it also depends upon the distribution of the target.  If the target variable is heavily skewed, and the median value is 1, then a very large number of our errors are in the range of 100% which doesn't sound very good.  \n",
    "\n",
    "For the target variable tested in this notebook, we also test two key assumptions: i) the quantization (referred here as bitrate) which reflects the precision that we use in representing a \"continuous\" signal as a discrete set of states represented by $2^b$ symbols, and ii) the prior we must apply to the simulated distribution $Q(X)$ where models are underspecified.  A prior must be assumed to handle the (common) case where any state observed *a posteriori* with  $p_i > 0$, if the corresponding model predicts $q_i = 0$, since KL divergence is a function of $\\log \\frac{P}{Q}$. i.e. we can't have $q_i=0$ in the denominator. \n",
    "\n",
    "First we preload the results to view the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a42fc6-9ff0-4f8e-b904-d73810419b8a",
   "metadata": {},
   "source": [
    "## Data Import and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9864ab-af00-457b-bfe0-2e69ac18b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xyzservices.providers as xyz\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import gridplot, row, column\n",
    "from bokeh.transform import factor_cmap, linear_cmap\n",
    "from bokeh.models import ColumnDataSource, LinearAxis, Range1d\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import Sunset10, Vibrant7, Category20, Bokeh6, Bokeh7, Bokeh8, Greys256\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.config_context(verbosity=2)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    roc_auc_score,\n",
    "    roc_curve, auc,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import data_processing_functions as dpf\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import lognorm\n",
    "from scipy.special import kl_div\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3f79c-03c7-4d25-aac0-3365836b1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "tiles = xyz['USGS']['USTopo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36488ad5-32a9-4d0a-b8ae-64967236e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the catchment characteristics\n",
    "fname = 'BCUB_watershed_attributes_updated.csv'\n",
    "attr_df = pd.read_csv(os.path.join('data', fname))\n",
    "attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "station_ids = attr_df['official_id'].values\n",
    "print(f'There are {len(station_ids)} monitored basins in the attribute set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee077b8-18d1-4d0d-979b-c59cb5518c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an example pairwise results file\n",
    "input_folder = os.path.join(\n",
    "    BASE_DIR, \"data\", \"processed_divergence_inputs\",\n",
    ")\n",
    "pairs_files = os.listdir(input_folder)\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(input_folder, pairs_files[0]), nrows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccb359-0265-4121-b835-5ba91a9de406",
   "metadata": {},
   "source": [
    "### Pre-load the results to avoid repeat loads in the model training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbc003-55a8-4953-ac51-bfb2714edb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the input data file has an associated revision date\n",
    "revision_date = '20240812'\n",
    "revision_date = '20241016'\n",
    "\n",
    "result_dict = {}\n",
    "nrows = None\n",
    "\n",
    "bitrates = [4, 6, 8, 10]\n",
    "for bitrate in bitrates:\n",
    "    if bitrate in [5, 7]:\n",
    "        continue\n",
    "    print(f'bitrate = {bitrate}')\n",
    "    fname = f\"KL_results_{bitrate}bits_{revision_date}.csv\"\n",
    "    # if partial_counts is true, we load a separate result file\n",
    "    # where observation counts incorporated a 10% uniform uncertainty\n",
    "    whole_counts_fname = f\"KL_results_{bitrate}bits_{revision_date}.csv\"\n",
    "    partial_counts_fname = f\"KL_results_{bitrate}bits_{revision_date}_partial_counts.csv\"\n",
    "\n",
    "    wc_input_data_fpath = os.path.join(input_folder, whole_counts_fname)\n",
    "    pc_input_data_fpath = os.path.join(input_folder, partial_counts_fname)\n",
    "        \n",
    "    df_partial = pd.read_csv(pc_input_data_fpath, nrows=nrows, low_memory=False)\n",
    "    df_whole = pd.read_csv(wc_input_data_fpath, nrows=nrows, low_memory=False)\n",
    "    \n",
    "    result_dict[bitrate] = {'partial': df_partial, 'whole': df_whole}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08347bc1-2910-45bf-a326-55dd1e971be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = {}\n",
    "percentiles = np.linspace(0, 100, 1000)\n",
    "distribution_plots = []\n",
    "for b, set_dict in result_dict.items():\n",
    "    if b in [7, 9, 11]:\n",
    "        continue\n",
    "    dfig = figure(title=f\"{b} bits quantization\", width=600, height=450)#, x_axis_type='log')#,\n",
    "                 # x_axis_type='log')\n",
    "    if len(distribution_plots) > 0:\n",
    "        dfig = figure(title=f\"{b} bits quantization\", width=600, height=450, \n",
    "                     x_range=distribution_plots[-1].x_range, # x_axis_type='log',\n",
    "                      y_range=distribution_plots[-1].y_range)#, \n",
    "                     # x_axis_type='log')\n",
    "    \n",
    "    partial_counts = set_dict['partial']\n",
    "    whole_counts = set_dict['whole']\n",
    "\n",
    "    concurrent_prior_cols = [c for c in whole_counts if c.startswith('dkl_concurrent_post')]\n",
    "    nonconcurrent_prior_cols = [c for c in whole_counts if c.startswith('dkl_nonconcurrent_post')]\n",
    "    uniform_col = 'dkl_concurrent_uniform'\n",
    "    assert uniform_col in partial_counts.columns\n",
    "    \n",
    "    n = 0\n",
    "    for c in concurrent_prior_cols:\n",
    "        prior = float(c.split('_')[-1].split('R')[0])\n",
    "        # the distributions converge when the prior is 10^5\n",
    "        if prior > 5:\n",
    "            continue\n",
    "        # compute empirical cdf of \"Actual\" Target\n",
    "        # values = partial_counts[c].copy().dropna()\n",
    "        values = whole_counts[c].copy().dropna()\n",
    "        minv, maxv = min(values), max(values)\n",
    "        # print(f'{b}bits, 10^{prior} prior min dkl: {minv:.1e}, max dkl: {maxv:.1F}')\n",
    "        sample_vals = np.percentile(values, percentiles)\n",
    "        # Calculate the CDF values\n",
    "        cdf_values = np.arange(1, len(sample_vals) + 1) / len(sample_vals)\n",
    "        dfig.line(sample_vals, cdf_values, color=Category20[17][n], \n",
    "                  line_width=2.5, legend_label=f'10^{prior}')\n",
    "        n += 1\n",
    "\n",
    "    values = whole_counts[uniform_col].copy().dropna()\n",
    "    sample_percentiles = np.percentile(values, percentiles)\n",
    "    cdf_values = np.arange(1, len(sample_percentiles) + 1) / len(sample_percentiles)\n",
    "    dfig.line(sample_percentiles, cdf_values, color='red', \n",
    "              line_width=3, legend_label='Q(X)=U', line_dash='dashed')\n",
    "        \n",
    "    dfig.legend.location ='bottom_right'\n",
    "    dfig.xaxis.axis_label = r'$$D_{\\text{KL}} [\\text{bits}/\\text{sample}]$$'\n",
    "    dfig.yaxis.axis_label = r'$$\\text{Pr}(x \\leq X)$$'\n",
    "    dfig.legend.ncols = 1\n",
    "    dfig.legend.click_policy = 'hide'\n",
    "    dfig.add_layout(dfig.legend[0], 'right')\n",
    "    distribution_plots.append(dfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36046c1e-3d5b-4179-826d-7927d044b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = gridplot(distribution_plots, ncols=3, width=400, height=500)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d505267-1e3a-4a6c-9f20-9c9312befd7a",
   "metadata": {},
   "source": [
    "### Information Loss due to the Prior\n",
    "\n",
    "Adding pseudo-counts to the simulated distribution $Q(X)$ shifts it to a posterior distribution $R(X)$.\n",
    "\n",
    "The information loss from adding pseudo-counts is then the KL divergence between the empirical distribution $Q(X)$ and the posterior $R(X)$:\n",
    "$$D_\\text{KL}(Q||R) = \\sum_{i=1}^k Q(x_i) \\log_2 \\left( \\frac{Q(x_i)}{R(x_i)} \\right)$$\n",
    "\n",
    "Where: \n",
    "* $q_i = Q(x_i)$ is the empirical frequency of state $i$,\n",
    "\n",
    "We want to  **normalize** the posterior distribution $R(x_i) = r_i = q_i + c_i$ after adding a prior distribution $\\alpha(x_i)=\\{ c_1, c_2, \\dots, c_k\\}$. \n",
    "\n",
    "$$R(x_i) = \\frac{q_i + c_i}{\\sum_{j=1}^k(q_j+c_j)}$$\n",
    "$$\\quad = \\frac{q_i + c_i}{\\sum_{j=1}^kq_j+ \\sum_{j=1}^k c_j}$$\n",
    "\n",
    "We know $\\sum_{i=1}^k Q(x_i) = 1$, and we let $\\sum_{j=1}^k c_j = C$, so $$R(x_i) = \\frac{q_i + c_i}{1+ C}$$\n",
    "\n",
    "* $k$ is the dictionary size ($k=2^b$), and\n",
    "* $c_i$ is the pseudo-counts added for state $i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d9ff9-d923-429e-a925-48981560e3b5",
   "metadata": {},
   "source": [
    "The ratio $\\frac{q_i}{r_i}$ becomes:\n",
    "\n",
    "$$\\frac{q_i}{r_i} = \\frac{q_i}{\\frac{q_i + c_i}{1 +C}} =  \\frac{(1 + C) \\cdot q_i}{q_i + c_i}$$\n",
    "\n",
    "Substituting $\\frac{q_i}{r_i}$ into $D_\\text{KL}(Q||R)$ gives:\n",
    "\n",
    "$$D_\\text{KL}(Q||R) = \\sum_{i=1}^k q_i \\log_2 \\left( (1+C) \\cdot \\frac{q_i}{q_i + c_i} \\right)$$\n",
    "\n",
    "$$=\\sum_{i=1}^k q_i \\left[ \\log_2(1+C) + \\log_2 \\left( \\frac{q_i}{q_i + c_i} \\right) \\right]$$\n",
    "$$= \\log_2(1+C) \\sum_{i=1}^k q_i + \\sum_{i=1}^k q_i \\log_2 \\left( \\frac{q_i}{q_i + c_i} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276e35f-c330-435d-98b8-c60bed06e1bd",
   "metadata": {},
   "source": [
    "Since again $\\sum_{i=1}^k q_i = 1$, the expression for the information loss due to the prior simplifies to:\n",
    "\n",
    "$$D_\\text{KL}(Q||R) = \\log_2(1+C) + \\sum_{i=1}^k q_i \\log_2 \\left( \\frac{q_i}{q_i + c_i} \\right)$$\n",
    "\n",
    "But $C$ should also be normalized, that is $\\sum_{j=1}^k c_i = 1$, so then R(x) becomes:\n",
    "\n",
    "$$R(x) = \\frac{q_i + c_i}{\\sum_{j=1}^k q_j + \\sum_{j=1}^k c_j} = \\frac{q_i + c_i}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51990af7-336d-4a96-b13a-55ace6f7126b",
   "metadata": {},
   "source": [
    "And the KL divergence between the empirical $Q(x)$ and posterior $R(x)$ distributions is then:\n",
    "\n",
    "$$D_\\text{KL}(Q||R) = \\sum_{i=1}^k q_i \\log_2 \\left( \\frac{q_i}{R(x_i)} \\right) = \\sum_{i=1}^k q_i \\log_2 \\left( \\frac{2\\cdot q_i}{q_i + c_i} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42a8dc-ff88-43f7-8ed5-31a1362d1359",
   "metadata": {},
   "source": [
    "The first term is constant, regardless of what the distribution $Q(X)$ looks like we should just minimize the total number of counts, which we can make equal by using fractional counts, or simply assuming some total pseudo-counts that we divide by the dictionary size.\n",
    "\n",
    "If we assume the maximum uncertainty prior (uniform distribution), the posterior becomes $R(x_i) = \\frac{q_i + \\frac{1}{k}}{2}$.\n",
    "\n",
    "As the (uniform) prior $c_i = \\frac{1}{k}$ approaches $q_i$, $R(x_i) = \\frac{q_i + \\frac{1}{k}}{2} \\rightarrow \\frac{\\frac{2}{k}}{2} = \\frac{1}{k}$ \n",
    "\n",
    "At the opposite extreme, as $q_i \\rightarrow 0$, $R(x_i) \\rightarrow \\frac{0 + \\frac{1}{k}}{2} \\rightarrow \\frac{1}{2} \\cdot \\frac{1}{k}$\n",
    "\n",
    "So then the distortion is a minimum when $q_i = c_i$ and a maximum when $q_i = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fefbb5-53ce-4d58-bffe-e1a33ff951a9",
   "metadata": {},
   "source": [
    "### Check CDFs based on support coverage flags\n",
    "\n",
    "Find the following:\n",
    "\n",
    "1. Separate the dataset into two groups base on the `underspecified_model_flag` which represents models where the support of $Q$ does not cover the support of $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45bbd9-5382-401b-9ac7-0f0d63778c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_cols = concurrent_prior_cols = [c for c in result_dict[4]['whole'].columns if c.startswith('dkl_concurrent_post')]\n",
    "priors = list(sorted([c.split('_')[-1].split('R')[0] for c in prior_cols]))\n",
    "priors = ['-4', '-3', '-2', '-1', '-0.5', '-0.2', '-0.1', '0', '0.1', '0.2', '0.5', '1', '2', '3', '4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235ac2b-15cd-41bd-8709-e84f79658a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "pcts = np.linspace(0, 100, 200)\n",
    "for pr in priors:\n",
    "    cfig = figure(width=500, height=400, title=f'10^{pr} pseudo-counts')\n",
    "    if len(plots) > 0:\n",
    "        cfig = figure(width=500, height=400, title=f'10^{pr} pseudo-counts', x_range=plots[-1].x_range)\n",
    "    c = 0\n",
    "    print(f'   Processing 10^{pr} prior')\n",
    "    for b, set_dict in result_dict.items():\n",
    "        if b in [7, 9, 11]:\n",
    "            continue\n",
    "        dkl_col = f'dkl_concurrent_post_{pr}R'\n",
    "        # part = set_dict['partial'].copy()\n",
    "        # part.dropna(subset=[dkl_col], inplace=True)\n",
    "        whole = set_dict['whole'].copy()\n",
    "        whole.dropna(subset=[dkl_col], inplace=True)    \n",
    "\n",
    "        # p_underspec = part.loc[part['underspecified_model_flag'] == 1, dkl_col]\n",
    "        w_underspec = whole.loc[whole['underspecified_model_flag'] == 1, dkl_col]\n",
    "        # p_covered = part.loc[part['underspecified_model_flag'] == 0, dkl_col]\n",
    "        w_covered = whole.loc[whole['underspecified_model_flag'] == 0, dkl_col]\n",
    "                \n",
    "        # p_under_cdf = np.percentile(p_underspec, pcts) \n",
    "        w_under_cdf = np.percentile(w_underspec, pcts) \n",
    "        # p_cover_cdf = np.percentile(p_covered, pcts) \n",
    "        w_cover_cdf = np.percentile(w_covered, pcts) \n",
    "        # cfig.line(p_under_cdf, pcts, color=Bokeh6[c], line_width=2, legend_label=f'{b}b underspec')\n",
    "        # cfig.line(p_cover_cdf, pcts, color=Bokeh6[c], line_width=2, legend_label=f'{b}b covered', line_dash='dashed')\n",
    "        cfig.line(w_under_cdf, pcts, color=Bokeh6[c], line_width=2, legend_label=f'{b}b underspec')\n",
    "        cfig.line(w_cover_cdf, pcts, color=Bokeh6[c], line_width=2, legend_label=f'{b}b covered', line_dash='dashed')\n",
    "\n",
    "        c += 1\n",
    "        cfig.xaxis.axis_label = dkl_col\n",
    "        cfig.yaxis.axis_label = r'$$\\text{Pr}(x \\leq X)$$'\n",
    "        cfig.axis.background_fill_alpha = 0.6\n",
    "        cfig.legend.location = 'bottom_right'\n",
    "        cfig.legend.click_policy = 'hide'\n",
    "        cfig = dpf.format_fig_fonts(cfig, font_size=16)\n",
    "    plots.append(cfig)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625bbee-8fd6-4880-82d4-99d3dba22737",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = gridplot(plots, ncols=3, width=400, height=350)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665fd63-5469-4c20-bcb2-f820cafbd376",
   "metadata": {},
   "source": [
    "### Information Loss Due to the Quantization\n",
    "\n",
    "The quantization itself sheds some of the information in both $P$ and $Q$.  We can quantify this on individual signals by mapping one quantization $Q(x_i | \\rho_1)$ to another $Q(x_i | \\rho_2)$, where $\\rho_b$ are the set of quantization schemes corresponding to dictionary sizes of $2^b$ symbols.\n",
    "\n",
    "1. Choose a baseline bitrate. I think it makes sense to set the highest bitrate (largest dictionary size is 12 bits) as the baseline since it reflects preserving the most information from the signal.\n",
    "2. Compute the ratio of KL divergence between bitrates $D_{KL}(P||R, \\rho_b) / D_{KL}(P||R, \\rho_{12})$ for $b \\in \\{10, 8, 6, 4\\}$\n",
    "\n",
    "**NEED TO RETHINK THIS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc52cd-1df1-449f-b293-37105fd90b5d",
   "metadata": {},
   "source": [
    "### Distortion Due to the Assumed Error\n",
    "\n",
    "Measurement (rating curve) uncertainty carries potentially the biggest distortion of both P and Q.  If instead of discrete points we treated observations as distributions relative to an assumed error distribution, then observations could count in multiple states in proportion to how much of the error interval overlaps with each bin.  The effect of this is greatest on large dictionaries with many empty bins, and the effect is the same as adding a prior -- information is lost.  \n",
    "\n",
    "Compare $D_{KL}(P_p||R_p) / D_{KL}(P_w||R_w)$ where $p$ represents \"partial counts\", or the set where the observations are treated as probability distributions over some assumption of error (in this case, 10% uniform), and $w$ represents \"whole counts\", i.e. the state frequencies are computed strictly by the bin the observation falls in.\n",
    "\n",
    "**HERE AGAIN NEED TO RETHINK IF THIS MAKES SENSE ON INDIVIDUAL VS. PAIRED BASIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c782d-2ed4-4e6f-ae62-fd460bbc6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the error model distortion values for individual stations\n",
    "error_model_distortion_fname = 'data/error_model_distortion/error_model_distortion_test.csv'\n",
    "err_df = pd.read_csv(error_model_distortion_fname)\n",
    "err_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2098677-14bb-4e7b-866d-a43a7009565e",
   "metadata": {},
   "source": [
    "### Basis for a \"Distinguishability Criteria\"\n",
    "\n",
    "To compute $D_{KL}(P||Q)$ on discrete distributions $P(x)$ and $Q(x)$, where $Q$ is a simulation or model of $P$, we must add a prior in order to prevent issues dividing by zero in $D_{KL}(P||Q) = \\sum_{i=1}^k P(x)\\log_2\\frac{P(x)}{Q(x)}$ if $q_i = 0$ for any state $i$ where $p_i > 0$.\n",
    "\n",
    "The assumption of a uniform prior in the form of pseudo-counts adds noise to $Q$.  If we want to compare two models $Q_a$ and $Q_b$ to determine which is more representative of $P$ based on the KL divergence, we should check that $|D_{KL}(P||Q_a) - D_{KL}(P||Q_b)| > D_{KL}(Q_a||R_a) + D_{KL}(Q_b||R_b)$ where $R(x_i) = (q_i + c_i) / (1 + C)$ and $C = \\sum_{i=1}^k c_i$ for a general prior distribution $\\alpha(x_i) = \\{c_1, c_2, \\dots, c_k\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630f06e-b9e7-4f36-853d-719d2e93ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_lognormal(mu1, sigma1, mu2, sigma2, b, prior_pseudo_count, y_range=(0, 0.035)):\n",
    "    \"\"\"\n",
    "    Generates two log-normal distributions, quantizes them with 2^b symbols,\n",
    "    applies a uniform pseudo-count prior to one, and plots the distributions\n",
    "    with Bokeh (including the posterior as a dashed line).\n",
    "\n",
    "    Parameters:\n",
    "    - mu1, sigma1: Parameters for the first log-normal distribution.\n",
    "    - mu2, sigma2: Parameters for the second log-normal distribution.\n",
    "    - b: Number of symbols = 2^b for quantization.\n",
    "    - prior_pseudo_count: Uniform pseudo-count to apply to one distribution.\n",
    "    \"\"\"\n",
    "    # Create the two log-normal distributions\n",
    "    minx, maxx = 0.0, 5\n",
    "    x = np.linspace(minx, maxx, 200)  # Values over which distributions are evaluated\n",
    "    dist1 = lognorm.pdf(x, sigma1, loc=mu1, scale=sigma1)\n",
    "    dist2 = lognorm.pdf(x, sigma2, loc=mu2, scale=sigma2)\n",
    "\n",
    "    bins = np.linspace(minx, maxx, 2 ** b + 1)\n",
    "    bins_midpoints = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    # Create the continuous log-normal distributions evaluated at the midpoints\n",
    "    dist1 = lognorm.pdf(bins_midpoints, sigma1, scale=np.exp(mu1))\n",
    "    dist2 = lognorm.pdf(bins_midpoints, sigma2, scale=np.exp(mu2))\n",
    "\n",
    "    dist3 = lognorm.pdf(bins_midpoints, sigma1, scale=np.exp(mu1))\n",
    "    dist4 = lognorm.pdf(bins_midpoints, sigma2, scale=np.exp(mu2))\n",
    "\n",
    "    # Quantize the distributions\n",
    "    # Normalize the distributions to form proper PDFs over the quantized bins\n",
    "    P = dist1 / np.sum(dist1)\n",
    "    Q = dist2 / np.sum(dist2)\n",
    "\n",
    "    # Apply the uniform prior as pseudo-counts to the second distribution\n",
    "    prior_counts = np.full_like(Q, prior_pseudo_count)\n",
    "    posterior_counts = Q * np.sum(dist2) + prior_counts  # Adding pseudo-counts\n",
    "    R = posterior_counts / np.sum(posterior_counts)  # Renormalize\n",
    "\n",
    "    assert np.abs(sum(P) - 1) < 0.001, 'P does not sum to 1'\n",
    "    assert np.abs(sum(Q) - 1) < 0.001, 'Q does not sum to 1'\n",
    "    assert np.abs(sum(R) - 1) < 0.001, 'R does not sum to 1'\n",
    "\n",
    "    kl_pq = kl_div(P, Q)\n",
    "    kl_pr = kl_div(P, R)\n",
    "\n",
    "    # Prepare data for Bokeh plotting\n",
    "    source1 = ColumnDataSource(data=dict(x=bins_midpoints, y=P))\n",
    "    source2 = ColumnDataSource(data=dict(x=bins_midpoints, y=Q))\n",
    "    source2_posterior = ColumnDataSource(data=dict(x=bins_midpoints, y=R))\n",
    "\n",
    "    # Create the Bokeh plot\n",
    "    p = figure(title=\"\", y_range=y_range,\n",
    "               x_axis_label='x', y_axis_label=r'$$\\text{Pr}(X)$$', width=500, height=350)\n",
    "\n",
    "    p.line('x', 'y', source=source1, line_width=2, color='black', legend_label=f\"P(x)=LN(x|{mu1:.1f},{sigma1:.2f})\")\n",
    "    p.line('x', 'y', source=source2, line_width=2, color='red', legend_label=f\"Q(x)=LN(x|{mu2:.1f},{sigma2:.2f})\")\n",
    "    p.line('x', 'y', source=source2_posterior, line_width=2, line_dash='dashed',\n",
    "           color='red', legend_label=f\"R(X) (Prior={prior_pseudo_count})\")\n",
    "\n",
    "    # Configure the legend and show the plot\n",
    "    p.legend.location = \"top_right\"\n",
    "    p.legend.click_policy = \"hide\"\n",
    "\n",
    "    return p, sum(kl_pq), sum(kl_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902a625-93a9-4060-a21a-564277329168",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0.25, 0.35\n",
    "p1, klpq1, klpr1 = generate_and_plot_lognormal(mu, sigma, mu+0.01, sigma-0.1, 8, 0.03, y_range=(0, 0.035))\n",
    "p2, klpq2, klpr2 = generate_and_plot_lognormal(mu, sigma, mu+0.01, sigma+0.1, 8, 0.03)\n",
    "p1, p2 = dpf.format_fig_fonts(p1), dpf.format_fig_fonts(p2)\n",
    "\n",
    "kl1_text = \"    -->Q1 is closer to P than R1\"\n",
    "if klpr1 < klpq1:\n",
    "    kl1_text = \"    -->R1 is closer to P than Q\"\n",
    "kl2_text = \"    -->Q2 is closer to P than R2\"\n",
    "if klpr2 < klpq2:\n",
    "    kl2_text = \"    -->R2 is closer to P than Q2\"\n",
    "\n",
    "print(f'DKL_1(P||Q) = {klpq1:.2f}, DKL(P||R) = {klpr1:.2f}')\n",
    "print(kl1_text)\n",
    "print(f'DKL_2(P||Q) = {klpq2:.2f}, DKL(P||R) = {klpr2:.2f}')\n",
    "print(kl2_text)\n",
    "layout = gridplot([p1, p2], ncols=2, width=500, height=350)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50299ae1-c476-4d89-a15b-2db47bfb87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(P, Q):\n",
    "    \"\"\"Compute the KL divergence DKL(P || Q).\"\"\"\n",
    "    return np.sum(P * np.log(P / Q), where=(P != 0))\n",
    "\n",
    "def generate_and_plot_kl_vs_prior(mu1, sigma1, mu2, sigma2, b, priors, y_range=(0, 0.02)):\n",
    "    \"\"\"\n",
    "    Generates two log-normal distributions, quantizes them with 2^b symbols,\n",
    "    computes the posterior with varying priors, and plots KL divergences DKL(P||R) and DKL(Q||R).\n",
    "\n",
    "    Parameters:\n",
    "    - mu1, sigma1: Parameters for the first log-normal distribution.\n",
    "    - mu2, sigma2: Parameters for the second log-normal distribution.\n",
    "    - b: Number of symbols = 2^b for quantization.\n",
    "    - priors: Array of prior pseudo-counts to apply to Q.\n",
    "    - y_range: Range for the y-axis in the plot.\n",
    "    \"\"\"\n",
    "    # Create the two log-normal distributions\n",
    "    minx, maxx = 0.0, 5.0\n",
    "    bins = np.linspace(minx, maxx, 2 ** b + 1)\n",
    "    bins_midpoints = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    # Evaluate the distributions at the bin midpoints\n",
    "    dist1 = lognorm.pdf(bins_midpoints, sigma1, scale=np.exp(mu1))\n",
    "    dist2 = lognorm.pdf(bins_midpoints, sigma2, scale=np.exp(mu2))\n",
    "\n",
    "    # Normalize to create PDFs\n",
    "    P = dist1 / np.sum(dist1)\n",
    "    Q = dist2 / np.sum(dist2)\n",
    "\n",
    "    kl_p_r_list = []\n",
    "    kl_q_r_list, ratio_list = [], []\n",
    "\n",
    "    # Compute KL divergences for each prior value\n",
    "    for prior_pseudo_count in priors:\n",
    "        prior_counts = np.full_like(Q, prior_pseudo_count)\n",
    "        posterior_counts = Q * np.sum(dist2) + prior_counts  # Adding pseudo-counts\n",
    "        R = posterior_counts / np.sum(posterior_counts)  # Renormalize\n",
    "\n",
    "        # Ensure valid PDFs\n",
    "        assert np.abs(sum(R) - 1) < 0.001, 'R does not sum to 1'\n",
    "\n",
    "        # Compute KL divergences\n",
    "        kl_p_r = compute_kl_divergence(P, R)\n",
    "        kl_q_r = compute_kl_divergence(Q, R)\n",
    "\n",
    "        kl_p_r_list.append(kl_p_r)\n",
    "        kl_q_r_list.append(kl_q_r)\n",
    "        \n",
    "        # Compute ratio as percentage\n",
    "        ratio = (kl_q_r / kl_p_r) * 100 if kl_p_r != 0 else np.nan\n",
    "        ratio_list.append(ratio)\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    source = ColumnDataSource(data=dict(\n",
    "        prior=priors,\n",
    "        kl_p_r=kl_p_r_list,\n",
    "        kl_q_r=kl_q_r_list,\n",
    "        ratio=ratio_list,\n",
    "    ))\n",
    "\n",
    "    ratio_range = (min(ratio_list) * 0.98, max(ratio_list) * 1.02)\n",
    "    ratio_range = (min(ratio_list) * 0.98, 10)\n",
    "\n",
    "    # Create the Bokeh plot\n",
    "    p = figure(title=\"\",\n",
    "               x_axis_label='Prior Pseudo-count',\n",
    "               y_axis_label='KL Divergence',\n",
    "               x_axis_type='log',\n",
    "               y_range=y_range,\n",
    "               width=600, height=400)\n",
    "\n",
    "    # Add secondary y-axis for the ratio\n",
    "    p.extra_y_ranges = {\"ratio\": Range1d(*ratio_range)}\n",
    "    p.add_layout(LinearAxis(y_range_name=\"ratio\", axis_label='Distortion (%)'), 'right')\n",
    "\n",
    "    p.line('prior', 'kl_p_r', source=source, line_width=2, color='black', legend_label='DKL(P || R)')\n",
    "    p.line('prior', 'kl_q_r', source=source, line_width=2, color='red', legend_label='DKL(Q || R)')\n",
    "\n",
    "    # Plot the ratio on the secondary y-axis\n",
    "    p.line('prior', 'ratio', source=source, line_width=2, color='red', \n",
    "           line_dash='dashed', y_range_name=\"ratio\",\n",
    "           legend_label='Prior Distortion (%)')\n",
    "\n",
    "    p.line(priors, [5 for _ in priors], line_width=2, color='red',\n",
    "           line_dash='dotted', y_range_name='ratio',\n",
    "           legend_label='5% distortion limit')\n",
    "\n",
    "    # Configure the legend and show the plot\n",
    "    p.legend.location = \"top_left\"\n",
    "    p.legend.click_policy = \"hide\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110ea91-b8d3-4658-82d3-a4e6252e860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0.25, 0.35\n",
    "priors = np.logspace(-6, -2, 100)\n",
    "prior_vs_kld = generate_and_plot_kl_vs_prior(mu, sigma, mu+0.025, sigma - 0.02, b, priors)\n",
    "prior_vs_kld = dpf.format_fig_fonts(prior_vs_kld)\n",
    "show(prior_vs_kld)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59698c-ec49-43ba-8dea-9115b8616474",
   "metadata": {},
   "source": [
    "Compute th $D_{KL}$ curve on all samples.\n",
    "\n",
    "1. set the prior array according to what has been computed for the dataset.\n",
    "2. compute the ratio $D_{KL}(Q||R) / D_{KL}(P||R)$ for all priors. This is pre-computed in `KL_results_<b>bits_<revision_date>.csv`.\n",
    "3. for each prior, compute the 95% CI of distortion ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0db6d-ef26-4590-bc67-1277853fcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the priors used in processing DKL \n",
    "pseudo_counts = [-4, -3, -2, -1, -0.5, -0.2, -0.1, 0, 0.1, 0.2, 0.5, 1, 2, 3, 4]\n",
    "pcts = {}\n",
    "for bitrate in bitrates:\n",
    "    partial_label = 'partial' # or 'whole'\n",
    "    tdf =  result_dict[bitrate][partial_label]\n",
    "    pcts[bitrate] = []\n",
    "    for pc in pseudo_counts:\n",
    "        # the distortion label\n",
    "        dkl_qr_label = f'dkl_prior_distortion_post_{pc}R'\n",
    "        # the posterior label\n",
    "        dkl_pr_label = f'dkl_concurrent_post_{pc}R'\n",
    "        \n",
    "        pc_data = tdf.copy().dropna(subset=[dkl_qr_label, dkl_pr_label])\n",
    "        ratios = pc_data[dkl_qr_label] / pc_data[dkl_pr_label]\n",
    "        vals = 100 * np.percentile(ratios, [5, 25, 50, 75, 95])\n",
    "        pcts[bitrate].append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc5c63-6005-4324-a703-eb4e272461df",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortion_figs = []\n",
    "for bitrate in bitrates:\n",
    "    if len(distortion_figs) > 0:\n",
    "        basis_y_range = distortion_figs[-1].y_range\n",
    "        basis_x_range = distortion_figs[-1].x_range\n",
    "        dfig = figure(title=f'{bitrate} bits', width=600, height=400, y_range=basis_y_range, x_range=basis_x_range)\n",
    "    else:\n",
    "        dfig = figure(title=f'{bitrate} bits', width=600, height=400, y_range=(-5, 20))       \n",
    "    \n",
    "    dist_df = pd.DataFrame(pcts[bitrate], columns=['lb', 'lq', 'median', 'uq', 'ub'])\n",
    "    dist_df['prior'] = pseudo_counts\n",
    "    source = ColumnDataSource(dist_df)\n",
    "    \n",
    "    dfig.varea(x='prior', y1='lb', y2='ub', source=source, \n",
    "               legend_label='90% CI', color='grey', fill_alpha=0.4)\n",
    "    dfig.varea(x='prior', y1='lq', y2='uq', source=source, \n",
    "               legend_label='IQR', color='black', fill_alpha=0.4)\n",
    "    dfig.line('prior', 'median', source=source, legend_label='median', \n",
    "              color='red', line_width=2, line_dash='dashed')\n",
    "    dfig.line([-5, 5], [5, 5], color='red', line_dash='dotted', \n",
    "              legend_label='distortion limit')\n",
    "    dfig.xaxis.axis_label = r'$$\\text{Prior } [10^x \\text{pseudo-counts}]$$'\n",
    "    if len(distortion_figs) == 0:\n",
    "        dfig.yaxis.axis_label = r'$$\\text{distortion} [\\%]$$'\n",
    "    dfig.legend.background_fill_alpha = 0.45\n",
    "    dfig.legend.location = 'top_left'\n",
    "    dfig = dpf.format_fig_fonts(dfig)\n",
    "    distortion_figs.append(dfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256826e-5b9f-4fdc-91c3-06d5723f609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = gridplot(distortion_figs, ncols=4, width=400, height=350)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df599a40-f7df-4c3e-a91e-bd7a959b0a30",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "Now that we have determined a basis for interpreting the magnitude of $D_\\text{KL}$ in terms of the sources of \"distortion\" to the signal (the prior, the bitrate, and some estimate of the rating curve uncertainty), we can use these \"uncertainties\" when comparing models.  That is, we can filter out model comparisons where the KL divergence is small relative to the sources of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309bec8-78c6-4db4-9b52-b23d0fa61018",
   "metadata": {},
   "source": [
    "It might help determine a suitable approach to first understand what the distribution of our target variables look like.\n",
    "\n",
    "Given what we know about the distortion from the prior, and also understanding that underspecified models can be particularly sensitive to the prior if they mis-specify any $p_i$ with large probability, it would be helpful to know what proportion of the dataset does the model Q underspecify the target P, stated otherwise how many sample pairs does the support of Q not cover P?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c232ab-ff79-4821-be71-5d786efd8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcounts, wcounts, brs = [], [], []\n",
    "for b, set_dict in result_dict.items():\n",
    "    if b in [7, 9, 11]:\n",
    "        continue\n",
    "    partial_counts = set_dict['partial']\n",
    "    whole_counts = set_dict['whole']\n",
    "    partial_counts_underspecified = partial_counts['underspecified_model_flag'].sum()\n",
    "    whole_counts_underspecified = whole_counts['underspecified_model_flag'].sum()\n",
    "    partial_pct = 100-100*partial_counts_underspecified / len(partial_counts)\n",
    "    whole_pct = 100-100*whole_counts_underspecified / len(whole_counts)\n",
    "    print(f'{b} bits, {whole_pct:.1f}% whole counts {partial_pct:.1f}% (partial counts)')\n",
    "    brs.append(b)\n",
    "    pcounts.append(partial_pct)\n",
    "    wcounts.append(whole_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad36624-c579-4ed9-a599-22536185ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_fig = figure(width=500, height=400)\n",
    "pc_fig.line(brs, wcounts, legend_label='Deterministic', line_width=2,\n",
    "           color='black')\n",
    "pc_fig.line(brs, pcounts, legend_label='Probabilistic', line_width=2, \n",
    "           line_dash='dashed', color='black')\n",
    "pc_fig.legend.location = 'bottom_left'\n",
    "pc_fig.xaxis.axis_label = r'$$\\text{Dictionary Size } [2^x \\text{bits}]$$'\n",
    "pc_fig.yaxis.axis_label = r'$$\\text{Support coverage rate [\\%] }$$'\n",
    "pc_fig = dpf.format_fig_fonts(pc_fig)\n",
    "show(pc_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac6fcf-f847-4144-b4e5-1bb8228a5e61",
   "metadata": {},
   "source": [
    "### Combine Error Model Distortion and $D_{KL}$ CDF\n",
    "\n",
    "Compare the distortion due to an assumed error model versus the distribution of DKL values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b582da-3493-4919-b1d3-6523c456f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "pcts = np.linspace(0, 100, 200)\n",
    "for b, set_dict in result_dict.items():\n",
    "    if b in [7, 9, 11]:\n",
    "        continue\n",
    "    cfig = figure(width=500, height=400, title=f'{b} bits')\n",
    "    if len(plots) > 0:\n",
    "        cfig = figure(width=500, height=400, title=f'{b} bits', \n",
    "                      x_range=plots[-1].x_range, y_range=plots[-1].y_range)\n",
    "    c = 0\n",
    "    print(f'   Processing {b} bits')\n",
    "    # plot the error model distortion \"bounds\"\n",
    "    err_model = err_bound_dict[b].copy()\n",
    "    err_pct = 0.1\n",
    "    err_bounds = err_model.loc[err_model['err'] == err_pct, :].to_dict('records')[0]\n",
    "    cfig.harea(x1=[err_bounds[2.5], err_bounds[2.5]], x2=[err_bounds[98.5], err_bounds[98.5]],\n",
    "               y=[0, 100], color='grey', fill_alpha=0.4, legend_label='95% CI')\n",
    "    cfig.harea(x1=[err_bounds[25], err_bounds[25]], x2=[err_bounds[75], err_bounds[75]],\n",
    "               y=[0, 100], color='black', fill_alpha=0.4, legend_label='IQR')\n",
    "    cfig.line([err_bounds[50], err_bounds[50]], [0, 100], color='red', line_width=3, \n",
    "              line_dash='dotted', legend_label=f'{int(100*err_pct)}% error')\n",
    "\n",
    "    for pr in priors:\n",
    "        dkl_col = f'dkl_concurrent_post_{pr}R'\n",
    "        part = set_dict['partial'].copy()\n",
    "        part.dropna(subset=[dkl_col], inplace=True)\n",
    "        whole = set_dict['whole'].copy()\n",
    "        whole.dropna(subset=[dkl_col], inplace=True)    \n",
    "\n",
    "        p_underspec = part.loc[part['underspecified_model_flag'] == 1, dkl_col]\n",
    "        w_underspec = whole.loc[whole['underspecified_model_flag'] == 1, dkl_col]\n",
    "        p_covered = part.loc[part['underspecified_model_flag'] == 0, dkl_col]\n",
    "        w_covered = whole.loc[whole['underspecified_model_flag'] == 0, dkl_col]\n",
    "\n",
    "        p_under_cdf = np.percentile(p_underspec, pcts) \n",
    "        w_under_cdf = np.percentile(w_underspec, pcts) \n",
    "        p_cover_cdf = np.percentile(p_covered, pcts) \n",
    "        w_cover_cdf = np.percentile(w_covered, pcts) \n",
    "        cfig.line(p_under_cdf, pcts, color=Category20[17][c], line_width=2, legend_label=f'{pr} underspec')\n",
    "        cfig.line(p_cover_cdf, pcts, color=Category20[17][c], line_width=2, legend_label=f'{pr} covered', line_dash='dashed')\n",
    "\n",
    "        c += 1\n",
    "        cfig.xaxis.axis_label = dkl_col\n",
    "        cfig.yaxis.axis_label = r'$$\\text{Pr}(x \\leq X)$$'\n",
    "        cfig.axis.background_fill_alpha = 0.6\n",
    "        cfig.legend.location = 'bottom_right'\n",
    "        cfig.legend.click_policy = 'hide'\n",
    "        cfig = dpf.format_fig_fonts(cfig, font_size=16)\n",
    "    plots.append(cfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563ccdc-634d-45b2-9d75-c913c50cae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = gridplot(plots, ncols=3, width=450, height=700)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829d31e-3c23-4634-9f15-51c4719de7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_model_pct = 0.1\n",
    "for b, set_dict in result_dict.items():\n",
    "    if b in [7, 9, 11]:\n",
    "        continue\n",
    "    \n",
    "    print(f'Processing {b} bits')\n",
    "    # get the error model distortion by bitrate\n",
    "    filtered_err = err_df[(err_df['bitrate'] == b) & (err_df['err'] == error_model_pct)].copy()\n",
    "    filtered_err.set_index('official_id', inplace=True)\n",
    "    err_by_station = filtered_err[['value']].to_dict('index')\n",
    "    \n",
    "    for pr in priors:\n",
    "        dkl_col = f'dkl_concurrent_post_{pr}R'\n",
    "        part = set_dict['partial'].copy()\n",
    "        part.dropna(subset=[dkl_col], inplace=True)\n",
    "        whole = set_dict['whole'].copy()\n",
    "        whole.dropna(subset=[dkl_col], inplace=True)\n",
    "\n",
    "        # create a boolean flag to indicate that the distortion \n",
    "        # due to the assumed error is greater than the DKL\n",
    "        # here i use the distortion of P only.\n",
    "        part[f'P_distortion_{pr}_prior'] = part['proxy'].map(lambda x: err_by_station.get(x, {}).get('value'))\n",
    "        whole[f'P_distortion_{pr}_prior'] = part['proxy'].map(lambda x: err_by_station.get(x, {}).get('value'))\n",
    "        part[f'P_distortion_flag_{pr}'] = part[f'P_distortion_{pr}_prior'] >= part[dkl_col]\n",
    "        whole[f'P_distortion_flag_{pr}'] = whole[f'P_distortion_{pr}_prior'] >= whole[dkl_col]\n",
    "        pct_flags_partial_counts = 100*part[f'P_distortion_flag_{pr}'].sum() / len(part)\n",
    "        pct_flags_whole_counts = 100*whole[f'P_distortion_flag_{pr}'].sum() / len(whole)\n",
    "        print(f'    {pct_flags_partial_counts:.1f}%/{pct_flags_whole_counts:.1f}% partial/whole counts are flagged as < {int(100*error_model_pct)}% RC error distortion given 10^{pr} prior')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11eae4-7979-47a3-98c0-42a0a0a6242d",
   "metadata": {},
   "source": [
    "### Binary Classification Problem Part 1: Prediction and Feature Group Importance\n",
    "\n",
    "The simulated runoff distribution is said to be \"optimized\" on the proxy.  That is, the bin edges are defined to suit the observed range of the streamflow from the catchment that is used as a donor/proxy (the model $Q(x_i)$) to simulate the general hydrological response of some target.  The $D_{KL}(P||Q)$ then represents the information cost of poorly estimated frequencies compared to the target $P(x_i)$ observed *a posteriori*.\n",
    "\n",
    "The support of a probability distribution $P(x_i)$ is the set of states $x_i \\in X$ for which $P(x_i) > 0$, and likewise for $Q(x_i)$.  If there is a mismatch in the support of P and Q, the KL divergence behaves in different ways. By definition (L'Hopitale's rule) $\\log_2(0/0) = 0$ and $\\log_2(0/q_i) = 0$ for $q_i > 0$. \n",
    "\n",
    "If the support of P does not cover Q, then we are bound to have some $p_i/q_i < 0$ which will yield $p_i\\log_2(p_i/q_i)$ greater than zero, weighted by the frequency $p_i$.  Conversely, if the support of Q does not cover P, then we have a problem since $q_i = 0$ where $p_i > 0$ leading to $\\log_2(p_i / 0)$.  This issue was addressed above where we assumed a prior: $$R(x_i) = \\frac{q_i + c_i}{\\sum_{i=1}^k (q_i + c_i)} $$  \n",
    "\n",
    "This prior adds some noise to the signal, but it's a function of the bitrate, the prior, and the distribution $Q(x_i)$. If Q covers P, then the smallest prior will add the least noise to Q.  But we just said we don't know ahead of time if the support of Q covers P.  Since we don't know the range of $P(x_i)$ beforehand (though it could be predicted from catchment attributes), it might be helpful to know whether or not the support of a model $Q(x_i)$ maps to some target $P(x_i)$ or not.  This question can be formulated as a binary classification prediction problem.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe9a24-5b8c-4f01-ba09-3e3514f7edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cluster information from the Methods section \n",
    "# where we partitioned the graph to evaluate the distribution\n",
    "# of the target variable across folds\n",
    "# n_clusters = 15\n",
    "# for spatial clustering\n",
    "# cluster_fname = f'stn_attributes_with_assigned_cluster_{n_clusters}.geojson'\n",
    "# for distributed classification (alternating labels spatially)\n",
    "n_classes = 5\n",
    "cluster_fname = f'stn_attributes_with_{n_classes}_spatial_partitions.geojson'\n",
    "cluster_ids = gpd.read_file(os.path.join('data', cluster_fname))\n",
    "cluster_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2f208-83a3-4b09-971d-6f29ca6813e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_df['cluster_id'] = attr_df['official_id'].apply(lambda x: cluster_ids.loc[cluster_ids['official_id'] == x, 'cluster'].values[0])\n",
    "attr_df['cluster_id'] = attr_df['official_id'].apply(lambda x: cluster_ids.loc[cluster_ids['official_id'] == x, f'{n_classes}_spatial'].values[0])\n",
    "# check the number of station per fold\n",
    "f_unique, f_counts = np.unique(attr_df['cluster_id'], return_counts=True)\n",
    "print(f_unique)\n",
    "f_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc3d94-dc6d-43e8-a308-fe5e4c6b7c84",
   "metadata": {},
   "source": [
    "### Load pairwise attribute comparisons\n",
    "\n",
    "Load a few rows from one of the pairwise data files.  These contain attributes about divergence measures that are computed on concurrent and non-concurrent time series at two monitored locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2831309-c510-40b3-b61f-404e30f0df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "kld_columns = [c for c in test_df.columns if 'dkl' in c]\n",
    "\n",
    "binary_results_folder = os.path.join(BASE_DIR, 'data', 'kld_prediction_results_binary')\n",
    "if not os.path.exists(binary_results_folder):\n",
    "    os.makedirs(binary_results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dfec8-2810-4d35-b33e-00c58c3eb869",
   "metadata": {},
   "source": [
    "### Define attribute groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc872c-e2cb-475b-9366-f57b1e1409d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain = ['drainage_area_km2', 'elevation_m', 'slope_deg', 'aspect_deg'] #'gravelius', 'perimeter',\n",
    "land_cover = [\n",
    "    'land_use_forest_frac_2010', 'land_use_grass_frac_2010', 'land_use_wetland_frac_2010', 'land_use_water_frac_2010', \n",
    "    'land_use_urban_frac_2010', 'land_use_shrubs_frac_2010', 'land_use_crops_frac_2010', 'land_use_snow_ice_frac_2010']\n",
    "soil = ['logk_ice_x100', 'porosity_x100']\n",
    "climate = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'high_prcp_freq', 'high_prcp_duration', 'low_prcp_freq', 'low_prcp_duration']\n",
    "all_attributes = terrain + land_cover + soil + climate\n",
    "len(all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35d9cb-cb19-4577-94d0-a9a866e9ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the amount of data to set aside for final testing\n",
    "# n_cv_folds = 5\n",
    "n_boost_rounds = 2500\n",
    "random_seed = 42\n",
    "loss_function = 'reg:absoluteerror'  # binary classification\n",
    "\n",
    "#define if testing concurrent or nonconcurrent data\n",
    "concurrent = 'concurrent'\n",
    "\n",
    "# partial counts refer to the test where observations were assigned\n",
    "# a uniform distribution to approximate error and allow fractional \n",
    "# observations in state space\n",
    "partial_counts = False\n",
    "\n",
    "# cross validation parameters\n",
    "optimize_cv_folds = False\n",
    "cv_fold_seed = 83561\n",
    "n_cv_fold_optimization_trials = 10\n",
    "# limit the maximum distance to make the network \n",
    "# graph of station pairs more separable\n",
    "max_centroid_distance = 1000\n",
    "\n",
    "attribute_set_names = ['proximity', '+climate', '+terrain', '+land_cover', '+soil']\n",
    "attribute_group_sets = [['centroid_distance'], climate, terrain, land_cover, soil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e97c8-2c31-429c-abd2-df7978647470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the fold dictionary to initialize the train/test split for each fold\n",
    "def create_fold_dict(df):\n",
    "    cluster_ids = sorted(list(set(attr_df['cluster_id'].values)))\n",
    "    print(f'The fold ids are: {cluster_ids}')\n",
    "    fold_dict = {}\n",
    "    for c in cluster_ids:\n",
    "        cluster_stns = attr_df.loc[attr_df['cluster_id'] == c, 'official_id'].values\n",
    "        # in-group edges\n",
    "        dkl_sample_AND = df[(df['proxy'].isin(cluster_stns)) & (df['target'].isin(cluster_stns))].copy()\n",
    "        # out-of-group edges\n",
    "        dkl_sample_NOR = df[(~df['proxy'].isin(cluster_stns)) & (~df['target'].isin(cluster_stns))].copy()\n",
    "        # assert that these are mutually exclusive groups\n",
    "        and_official_ids = set(dkl_sample_AND['proxy'].values + dkl_sample_AND['target'].values)\n",
    "        nor_official_ids = set(dkl_sample_NOR['proxy'].values + dkl_sample_NOR['target'].values)\n",
    "        assert len(list(set(np.intersect1d(and_official_ids, nor_official_ids)))) == 0, 'stations in list are not unique'\n",
    "        fold_dict[c] = {\n",
    "            'test': dkl_sample_AND.index.values,\n",
    "            'train': dkl_sample_NOR.index.values,\n",
    "        }\n",
    "    return fold_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad14d45-f773-431f-9f67-82bd77456963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_features(input_attributes):\n",
    "    features = []\n",
    "    for a in input_attributes:\n",
    "        features.append(f\"proxy_{a}\".lower())\n",
    "        features.append(f\"target_{a}\".lower())\n",
    "    return features\n",
    "\n",
    "def add_attributes(attr_df, df, attribute_cols):\n",
    "    \"\"\"\n",
    "    Adds attributes from the df_attributes to the df_relations based on the 'proxy' and 'target' columns\n",
    "    using map for efficient lookups.\n",
    "\n",
    "    Parameters:\n",
    "    df_attributes (pd.DataFrame): DataFrame with 'id' and attribute columns.\n",
    "    df_relations (pd.DataFrame): DataFrame with 'proxy' and 'target' columns.\n",
    "    attribute_cols (list of str): List of attribute columns to add to df_relations.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated df_relations with added attribute columns.\n",
    "    \"\"\"\n",
    "    # Create dictionaries for each attribute for quick lookup\n",
    "    attr_dicts = {col: attr_df.set_index('official_id')[col].to_dict() for col in attribute_cols}\n",
    "\n",
    "    # Add target attributes\n",
    "    for col in attribute_cols:\n",
    "        df[f'target_{col}'] = df['target'].map(attr_dicts[col])\n",
    "\n",
    "    # Add proxy attributes\n",
    "    for col in attribute_cols:\n",
    "        df[f'proxy_{col}'] = df['proxy'].map(attr_dicts[col])\n",
    "\n",
    "    for col in attribute_cols:\n",
    "        df[f'{col}_diff'] = df[f'target_{col}'] - df[f'proxy_{col}'] \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba08d6-b136-4af8-aa08-0255fd5e01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_model(\n",
    "    input_data, train_idxs, test_idxs, attributes, target, params, num_boost_rounds,\n",
    "):\n",
    "    train_data = input_data.iloc[train_idxs].copy()\n",
    "    test_data = input_data.iloc[test_idxs].copy()\n",
    "    \n",
    "    X_train = train_data[attributes].values\n",
    "    Y_train = np.log10(train_data[target].values)\n",
    "    X_test = test_data[attributes].values\n",
    "    Y_test = np.log10(test_data[target].values)\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=Y_test)\n",
    "\n",
    "    eval_list = [(dtrain, \"train\"), (dtest, \"eval\")]\n",
    "\n",
    "    params['eval_metric'] = 'auc'\n",
    "    evals_result = {}\n",
    "\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_rounds,\n",
    "        evals=eval_list,\n",
    "        verbose_eval=0,\n",
    "        early_stopping_rounds=None,\n",
    "        evals_result=evals_result,\n",
    "    )\n",
    "\n",
    "    raw_preds = bst.predict(dtest)\n",
    "    predicted_y =  1 / (1 + np.exp(-raw_preds))\n",
    "    test_results = pd.DataFrame(\n",
    "        {\n",
    "            \"predicted\": predicted_y,\n",
    "            \"actual\": Y_test,\n",
    "        }\n",
    "    )\n",
    "    test_results['predicted'] = test_results['predicted']\n",
    "    test_results['actual'] = test_results['actual']\n",
    "\n",
    "    return bst, test_results, evals_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1d03e-3619-42bb-9407-5edd052f3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary_trials_custom_CV(\n",
    "    set_name,\n",
    "    attributes,\n",
    "    target,\n",
    "    input_data,\n",
    "    fold_dict, \n",
    "    n_optimization_rounds,\n",
    "    num_boost_rounds,\n",
    "    results_folder,\n",
    "    loss='reg:squarederror',\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom CV refers to cross validation.  Custom cross validation means the \n",
    "    held-out set must be determined in a more robust way to avoid \"data leakage\".\n",
    "    That is, the pairs making up the training, validation, and test sets must \n",
    "    be made up of pairings from unique sets of stations.\n",
    "    \"\"\"\n",
    "    # select random hyperparameters for n_optimization_rounds\n",
    "    sample_choices = np.arange(0.5, 0.9, 0.02)  # subsample and colsample percentages\n",
    "    lr_choices = np.arange(0.001, 0.1, 0.0005)  # learning rates\n",
    "    learning_rates = np.random.choice(lr_choices, n_optimization_rounds)\n",
    "    subsamples = np.random.choice(sample_choices, n_optimization_rounds)\n",
    "    colsamples = np.random.choice(sample_choices, n_optimization_rounds)\n",
    "    num_boost_rounds = num_boost_rounds\n",
    "\n",
    "    all_results = []\n",
    "    best_result = (None, np.inf, None)\n",
    "    best_params = None\n",
    "    best_mean_test_perf = 0\n",
    "    best_convergence_df = pd.DataFrame()\n",
    "    best_trial_test_predictions = None\n",
    "    output_target_cdfs = None\n",
    "\n",
    "    for trial in range(n_optimization_rounds):\n",
    "        lr, ss, cs = learning_rates[trial], subsamples[trial], colsamples[trial]\n",
    "        params = {\n",
    "            \"objective\": loss,\n",
    "            \"eta\": lr,\n",
    "            # \"max_depth\": 6,  # use default max_depth\n",
    "            # \"min_child_weight\": 1, # use colsample and subsample instead of min_child_weight\n",
    "            \"subsample\": ss,\n",
    "            \"colsample_bytree\": cs,\n",
    "            \"seed\": random_seed,\n",
    "            \"device\": \"cuda\",  # note, change this to 'cpu' if your system doesn't have a CUDA GPU\n",
    "            \"sampling_method\": \"gradient_based\",\n",
    "            \"tree_method\": \"hist\",\n",
    "        }\n",
    "\n",
    "        results_fname = (\n",
    "            f\"{set_name}_{bitrate}_bits_{lr:.3f}_lr_{ss:.3f}_sub_{cs:.3f}_col.csv\"\n",
    "        )\n",
    "        results_fpath = os.path.join(results_folder, results_fname)\n",
    "\n",
    "        # k-fold cross validation\n",
    "        n_samples = len(input_data)\n",
    "        fold_no = 0\n",
    "        fold_scores = []\n",
    "        fold_balances = []\n",
    "        learning_curves = []\n",
    "        all_test_set_predictions = []\n",
    "        for fold_no, cv_data in fold_dict.items():\n",
    "\n",
    "            cv_model, cv_test, evals_result = train_binary_model(\n",
    "                input_data,\n",
    "                cv_data['train'],\n",
    "                cv_data['test'],\n",
    "                attributes,\n",
    "                target,\n",
    "                params,\n",
    "                num_boost_rounds,\n",
    "            )\n",
    "            obs, pred = cv_test['actual'].values, cv_test['predicted'].values\n",
    "            test_balance = sum(cv_test['actual'].values) / len(cv_test)\n",
    "            \n",
    "            obs_set, obs_counts = np.unique(obs, return_counts=True)\n",
    "            if (obs == pred).all() & (len(obs_set) == 1):\n",
    "                # print('    All observations have the same class.')\n",
    "                raise Exception('All observations have the same class')\n",
    "                accuracy = 1.0\n",
    "            else:\n",
    "                # tn, fp, fn, tp = confusion_matrix(obs, pred).ravel()\n",
    "                # accuracy = (tp + tn) / (tp + fp + fn + tn)                 \n",
    "                fpr, tpr, thresholds = roc_curve(obs, pred)\n",
    "                auc_score = auc(fpr, tpr)\n",
    "            \n",
    "            # error = fbeta_score(obs, pred, 1)\n",
    "            # print(f'Accuracy: {accuracy:.2f}')#, F1 beta: {error:.2f}')\n",
    "            fold_scores.append(auc_score)\n",
    "            learning_curves.append(evals_result)\n",
    "            all_test_set_predictions.append(cv_test)\n",
    "            fold_balances.append(test_balance)\n",
    "\n",
    "        cv_mean, cv_std = np.mean(fold_scores), np.std(fold_scores)\n",
    "        fold_balance_mean, fold_balance_std = np.mean(fold_balances), np.std(fold_balances)\n",
    "        results_dict = {\n",
    "            \"trial\": trial,\n",
    "            \"test_auc_mean\": cv_mean,\n",
    "            \"test_auc_stdev\": cv_std,\n",
    "            \"test_balance_mean\": fold_balance_mean,\n",
    "            \"test_balance_std\": fold_balance_std,\n",
    "        }\n",
    "        results_cols = list(results_dict.keys())\n",
    "        results_dict.update(params)\n",
    "\n",
    "        all_results.append(results_dict)\n",
    "        if (trial > 0) & (trial % 20 == 0):\n",
    "            print(f\"   completed {trial}/{n_optimization_rounds}\")            \n",
    "\n",
    "        all_results.append(results_dict)\n",
    "        if (trial > 0) & (trial % 10 == 0):\n",
    "            print(f\"   completed {trial}/{n_optimization_rounds}\")\n",
    "\n",
    "        if round(cv_mean, 2) > round(best_mean_test_perf, 2):\n",
    "            best_params = params\n",
    "            best_mean_test_perf = cv_mean\n",
    "            best_learning_curve = learning_curves\n",
    "            best_trial_predictions = all_test_set_predictions\n",
    "            print(f'    New best result: AUC={cv_mean:.2f} (trial {trial})')\n",
    "    \n",
    "    # save the best trial results\n",
    "    # best_trial_test_predictions.to_csv(results_fpath)\n",
    "    results_all_trials = pd.DataFrame(all_results)\n",
    "    # get the mean and standard deviation of the error metrics over all trials\n",
    "    all_trials_mean = results_all_trials[f\"test_auc_mean\"].mean()\n",
    "    all_trials_stdev = results_all_trials[f\"test_auc_mean\"].std()\n",
    "    all_fold_balances_mean = results_all_trials[f\"test_balance_mean\"].mean()\n",
    "    # mean of the standard devations of all folds, maybe should rethink this\n",
    "    all_fold_balances_std = results_all_trials[f\"test_balance_mean\"].std()\n",
    "    print(\n",
    "        f\"    {all_trials_mean:.2f} ± {2*all_trials_stdev:.3f} mean (95% CI) AUC ({100*all_fold_balances_mean:.0f}% mean fold +/-{all_fold_balances_std:2f} ) (of {n_optimization_rounds} hyperparameter optimization rounds.)\"\n",
    "    )\n",
    "    \n",
    "    return best_params, best_mean_test_perf, best_learning_curve, results_all_trials, best_trial_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b87f-d1cd-4218-b0c1-3d42e69f29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_underspecification_from_attributes(attr_df, target_variable, max_centroid_distance, results_folder, prior,\n",
    "                                loss_function=None, partial_counts=False, n_boost_rounds=100, random_seed=42, \n",
    "                              optimize_cv_folds=True, n_cv_fold_optimization_trials=20, cv_fold_seed=42):\n",
    "    counts_key = 'partial'\n",
    "    if partial_counts == \"False\":\n",
    "        counts_key = 'whole'\n",
    "    \n",
    "    all_results = {}\n",
    "    for bitrate in [4, 6, 8, 10]:\n",
    "        all_results[bitrate] = {}\n",
    "        t0 = time()\n",
    "        print(f'bitrate = {bitrate}')\n",
    "        input_data_fpath = os.path.join(input_folder, fname)\n",
    "        nrows = None\n",
    "        # df = pd.read_csv(input_data_fpath, nrows=nrows, low_memory=False)\n",
    "        df = result_dict[bitrate][counts_key]\n",
    "        df.dropna(subset=[target_variable], inplace=True)\n",
    "\n",
    "        # check the target variable balance\n",
    "        true_vals = len(df[df[target_variable] == True])\n",
    "        pct_true = int(100*true_vals / len(df))\n",
    "        print(f'{true_vals}/{len(df)} ({pct_true}%) of the dataset are underspecified')\n",
    "                        \n",
    "        # reduce the maximum distance separating pairs such that the \n",
    "        # graph can be more evenly separated.  Too permissible a distance\n",
    "        # filter increases the graph connectivity, making it difficult to\n",
    "        # create cross validation folds with no data leakage.\n",
    "        df = df[df['centroid_distance'] < max_centroid_distance]\n",
    "        print(f'  {len(df)} pairs remaining after filtering by max distance of {max_centroid_distance} km')        \n",
    "        # add the attributes into the input dataset\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        df = add_attributes(attr_df, df.copy(), all_attributes)\n",
    "        fold_dict = create_fold_dict(df)\n",
    "\n",
    "        # add attribute groups successively\n",
    "        predictor_attributes = []\n",
    "        # make sure the order of attributes matches the attribute_set_names list\n",
    "        for attribute_set, set_name in zip(attribute_group_sets, attribute_set_names):\n",
    "            print(f'  Processing {set_name} attribute set: {target_variable}')            \n",
    "            # initialize the predictor variables (features)\n",
    "            predictor_attributes += attribute_set\n",
    "            if set_name == 'proximity':\n",
    "                features = ['centroid_distance']\n",
    "            else:\n",
    "                non_dist_features = [c for c in predictor_attributes if c != 'centroid_distance']\n",
    "                pair_features = format_features(non_dist_features)\n",
    "                diff_features = [f'{c}_diff' for c in non_dist_features]\n",
    "                features = ['centroid_distance'] + pair_features + diff_features                \n",
    "\n",
    "            best_params, best_mean_test_perf, best_learning_curve, results_all_trials, best_trial_predictions = run_binary_trials_custom_CV(\n",
    "                set_name, features, target_variable, df, fold_dict, \n",
    "                n_cv_fold_optimization_trials, n_boost_rounds, results_folder, \n",
    "                loss=loss_function, random_seed=random_seed)\n",
    "                \n",
    "            # store the test set predictions and actuals\n",
    "            all_results[bitrate][set_name] = {\n",
    "                'best_params': best_params,\n",
    "                'all_results': results_all_trials, \n",
    "                'learning_curve': best_learning_curve,\n",
    "                'test_predictions': best_trial_predictions,\n",
    "                # 'test_rmse': best_mean_rmse,\n",
    "                'test_auc': best_mean_test_perf,\n",
    "                # 'target_cdfs': target_cdfs,\n",
    "            } \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f5a7a-9296-4c84-8c7e-084300f660a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_col = f'dkl_{concurrent}_post_{prior}R'\n",
    "target_column = 'underspecified_model_flag'\n",
    "loss_function = 'binary:logistic'\n",
    "partial_counts = False\n",
    "binary_test_results_fname = f'binary_classification_coverage_test.npy'\n",
    "if partial_counts is True:\n",
    "    binary_test_results_fname = binary_test_results_fname.replace('.npy', '_partial_counts.npy')\n",
    "binary_test_results_fpath = os.path.join(binary_results_folder, binary_test_results_fname)\n",
    "if os.path.exists(binary_test_results_fpath):\n",
    "    print('Processed and loading: ', binary_test_results_fname)\n",
    "    all_test_results = np.load(binary_test_results_fpath, allow_pickle=True).item()\n",
    "else:\n",
    "    all_test_results = predict_underspecification_from_attributes(\n",
    "        attr_df, target_column, max_centroid_distance, binary_results_folder, prior,\n",
    "        loss_function=loss_function, partial_counts=partial_counts, n_boost_rounds=n_boost_rounds, random_seed=random_seed,\n",
    "        optimize_cv_folds=optimize_cv_folds, n_cv_fold_optimization_trials=n_cv_fold_optimization_trials, cv_fold_seed=cv_fold_seed,\n",
    "    )\n",
    "    np.save(binary_test_results_fpath, all_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c84bb3-aa87-45d1-a02b-bc8578fdd921",
   "metadata": {},
   "source": [
    "### Notes for Discussion\n",
    "\n",
    "1. The binary classification balance ranges from 46% to 58%, meaning a very large proportion of the models in the dataset are underspecified, or the support of $Q(x_i)$ does not cover $P(x_i)$.  Underspecified models are senstive to the prior assumed in computing the KL divergence.  \n",
    "2. The binary classification tells us $\\forall x_i \\in X, P(x_i) > 0 \\rightarrow Q(x_i) > 0$.  This is different than knowing whether the range of values is equal, **as $Q$ could still cover a wider range**.  In other words, the support of $P(x_i)$ could be a subset of $Q(x_i)$.  Just not the other way around.  However, since the pairs are directional and both are contained in the dataset, we can create a third target variable for prediction that describes the combined case where the model $Q(x_i)$ covers the support of $P(x_i)$ **and** $P(x_i)$ covers the support of $Q(x_i)$.  This boolean combination describes matching coverage.\n",
    "3. The support coverage can be predicted as a binary variable from catchment attributes.  Predicting whether a model (proxy) $Q(x_i)$ will cover the support of the target $P(x_i)$ **based on proximity alone** does almost no better than random guessing.  However the AUC score improves substantially with the addition of climate attributes.  This is likely due to the precipitation.  Adding terrain attributes again results in a marked improvement in AUC score reaching nearly 90%. This improvement is likely due to the addition of the drainage area.  Adding land cover and soil attributes to the covariate vector does not improve the AUC.  The annual precipitation and drainage area in combination explain much of the story of what is expected in terms of the range of unit area runoff, and we can test this by training the model with those two features exclusively.  More nuanced differences can be plausibly explained by any number of physical processes, but no such tests are attempted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af2a8a-5a08-4fa3-a1c2-9feca86206db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(all_test_predictions, threshold=0.5):\n",
    "    predicted_labels = (all_test_predictions['predicted'] >= threshold).astype(int)\n",
    "    cm = confusion_matrix(all_test_predictions['actual'], predicted_labels)\n",
    "    cm_percent = cm / cm.sum() * 100  # Convert to percentages\n",
    "    # Prepare data for Bokeh plot (flatten the 2x2 matrix)\n",
    "    labels = [\"True Neg\", \"False Pos\", \"False Neg\", \"True Pos\"]\n",
    "    counts = [float(e) for e in cm_percent.flatten()]\n",
    "    text_counts = [f'{c:.1f}%' for c in counts]\n",
    "    x = [0, 1, 0, 1]  # Columns of the confusion matrix\n",
    "    y = [1, 1, 0, 0]  # Rows of the confusion matrix\n",
    "\n",
    "    # Create a data source for Bokeh\n",
    "    source = ColumnDataSource(data=dict(x=x, y=y, counts=counts, labels=labels, text_counts=text_counts))\n",
    "\n",
    "    # Set up color mapping\n",
    "    mapper = linear_cmap(field_name='counts', palette='Blues9', low=min(counts), high=max(counts))\n",
    "\n",
    "    # Create the figure\n",
    "    p = figure(\n",
    "        title=None,\n",
    "        x_axis_location=\"above\",\n",
    "        tools=\"\",\n",
    "        width=400,\n",
    "        height=400,\n",
    "        x_range=(-0.5, 1.5),\n",
    "        y_range=(-0.5, 1.5),\n",
    "    )\n",
    "    p.grid.visible = False\n",
    "\n",
    "    # Add squares for each cell in the confusion matrix\n",
    "    p.rect(x='x', y='y', width=1, height=1, source=source, fill_color=mapper, line_color=\"black\")\n",
    "\n",
    "    # Add text labels to the squares\n",
    "    p.text(\n",
    "        x='x', y='y', text='text_counts', source=source,\n",
    "        text_font_size=\"16pt\", text_align=\"center\", text_baseline=\"middle\",\n",
    "        background_fill_color=\"white\", background_fill_alpha=0.6  # White background with alpha\n",
    "    )\n",
    "    # Customize the axes\n",
    "    # Remove only the ticks from both axes\n",
    "    p.xaxis.major_tick_line_color = None  # Hide x-axis major ticks\n",
    "    p.yaxis.major_tick_line_color = None  # Hide y-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # Hide x-axis minor ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # Hide y-axis minor ticks\n",
    "    p.xaxis.major_label_overrides = {0: \"Pred False\", 1: \"Pred True\", -0.5: \"\", 0.5: \"\", 1.5: \"\"}\n",
    "    p.yaxis.major_label_overrides = {0: \"Actual True\", 1: \"Actual False\", -0.5: \"\", 0.5: \"\", 1.5: \"\"}\n",
    "\n",
    "\n",
    "    # p.xaxis.axis_label = \"Predicted Label\"\n",
    "    # p.yaxis.axis_label = \"Actual Label\"\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42fac0-faba-4dcc-a69e-37633344cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(all_test_predictions):\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(all_test_predictions['actual'], all_test_predictions['predicted'])\n",
    "    \n",
    "    # Compute Youden's Index for each threshold\n",
    "    youden_index = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_index)  # Index of the best threshold\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2abd8c-cf72-4b17-8857-3885e323844a",
   "metadata": {},
   "source": [
    "### Binary Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472d2b6-2d59-4bb9-9e3d-1e84207c3c8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "layout_dict = {}\n",
    "reg_plots_dict = {}\n",
    "res_r2_dict = {}\n",
    "\n",
    "plots = []\n",
    "reg_plots_dict[prior] = {}\n",
    "res_r2_dict[prior] = {}\n",
    "\n",
    "binary_results = np.load(binary_test_results_fpath, allow_pickle=True).item()\n",
    "\n",
    "for b, binary_result in binary_results.items():\n",
    "    test_set = 'test_auc'\n",
    "\n",
    "    y2 = [binary_result[e][test_set] for e in attribute_set_names]\n",
    "    source = ColumnDataSource({'x': attribute_set_names, 'y2': y2})\n",
    "        \n",
    "    title = f'{b} bits'\n",
    "    if len(plots) == 0:\n",
    "        fig = figure(title=title, x_range=attribute_set_names, toolbar_location='above')\n",
    "    else:\n",
    "        fig = figure(title=title, x_range=attribute_set_names, y_range=plots[0].y_range, \n",
    "                     output_backend='webgl', toolbar_location='above')\n",
    "\n",
    "    fig.line('x', 'y2', legend_label='AUC', color='dodgerblue', source=source, line_width=3)\n",
    "    fig.legend.background_fill_alpha = 0.6\n",
    "    fig.legend.location = 'bottom_right'\n",
    "    fig.yaxis.axis_label = 'AUC'\n",
    "    fig.xaxis.axis_label = 'Attribute Group (additive)'\n",
    "    plots.append(fig)\n",
    "    \n",
    "    result_df = pd.DataFrame({'set': attribute_set_names, 'auc': y2})\n",
    "    best_auc_idx = result_df['auc'].idxmax()\n",
    "    best_auc_set = result_df.loc[best_auc_idx, 'set']\n",
    "    foo = binary_result[best_auc_set]\n",
    "    \n",
    "    # plot the test set convergence for the 'best' trial\n",
    "    cfig = figure(title=f'AUC ({best_auc_set} set)',)\n",
    "    learning_curve_sets = binary_result[best_auc_set]['learning_curve']\n",
    "    train_curve_dfs, test_curve_dfs = [], []\n",
    "    n = 0\n",
    "    for lc_dict in learning_curve_sets:\n",
    "        train_df = pd.DataFrame(lc_dict['train']['auc'], columns=[n])\n",
    "        test_df = pd.DataFrame(lc_dict['eval']['auc'], columns=[n])\n",
    "        test_curve_dfs.append(test_df)\n",
    "        train_curve_dfs.append(train_df)\n",
    "        n += 1\n",
    "\n",
    "    train_curve_df = pd.concat(train_curve_dfs, axis=1)\n",
    "    train_curve_df['mean'] = train_curve_df.mean(axis=1)\n",
    "    test_curve_df = pd.concat(test_curve_dfs, axis=1)\n",
    "    test_curve_df['mean'] = test_curve_df.mean(axis=1)\n",
    "    \n",
    "    for fn in range(len(learning_curve_sets)):\n",
    "        cfig.line(train_curve_df.index, train_curve_df[fn], line_alpha=0.6, line_color='grey', line_dash='dotted')\n",
    "        cfig.line(test_curve_df.index, test_curve_df[fn], line_alpha=0.6, line_color='red', line_dash='dotted')\n",
    "    cfig.line(train_curve_df.index, train_curve_df['mean'], line_alpha=0.5, line_color='grey', \n",
    "              line_width=2, legend_label='CV Mean (Train)')\n",
    "    cfig.line(test_curve_df.index, test_curve_df['mean'], line_alpha=0.5, line_color='red', \n",
    "              line_width=2, legend_label='CV Mean (Test)')\n",
    "\n",
    "    # find the min predictive risk (optimal complexity)\n",
    "    min_pred_risk_idx = test_curve_df['mean'].idxmax()\n",
    "    if min_pred_risk_idx == max(test_curve_df['mean'].index):\n",
    "        print(f'Min prediction risk occurs at the maximum iteration, try increasing the number of boosting rounds')\n",
    "    \n",
    "    min_pred_risk = test_curve_df.loc[min_pred_risk_idx, 'mean']\n",
    "    cfig.line([min_pred_risk_idx, min_pred_risk_idx], [test_curve_df['mean'].min(), min_pred_risk], \n",
    "              legend_label='Min risk', color='green', line_width=2, line_dash='dashed')\n",
    "\n",
    "    cfig.xaxis.axis_label = r'$$\\text{Iteration}$$'\n",
    "    cfig.yaxis.axis_label = r'$$\\text{AUC} $$'\n",
    "    cfig.legend.background_fill_alpha = 0.5\n",
    "    cfig.legend.location = 'bottom_right'\n",
    "    plots.append(cfig)\n",
    "\n",
    "    all_test_predictions = pd.concat(binary_result[best_auc_set]['test_predictions'], axis=0)\n",
    "    # find a threshold to balance (minimize) false positives and false negatives\n",
    "    optimal_threshold = find_optimal_threshold(all_test_predictions)\n",
    "    # plot the confusion matrix\n",
    "    cm_plot = plot_confusion_matrix(all_test_predictions, threshold=optimal_threshold)\n",
    "    plots.append(cm_plot)\n",
    "\n",
    "    cdf_plot = figure(title=f'AUC CDF ({best_auc_set} set) Threshold={optimal_threshold:.2f}',)\n",
    "    for preds in binary_result[best_auc_set]['test_predictions']:\n",
    "        pct_true = np.sum(preds['actual']) / len(preds)\n",
    "        percentiles = np.linspace(1, 100, 1000)\n",
    "        cdf = np.percentile(preds['predicted'], percentiles)\n",
    "        cdf_plot.line(percentiles, cdf, color='grey', line_width=1.5, line_alpha=0.5, legend_label='test folds')\n",
    "    all_mean = np.percentile(all_test_predictions['predicted'], percentiles)\n",
    "    pct_true = all_test_predictions['actual'].sum() / len(all_test_predictions)\n",
    "    cdf_plot.line(percentiles, all_mean, color='red', alpha=0.5, line_width=2,\n",
    "                  legend_label=f'{100*pct_true:.0f}% True')\n",
    "    cdf_plot.legend.location = 'bottom_right'\n",
    "    plots.append(cdf_plot)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfeeadd-69e3-45a4-8ba1-35099502eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_layout = gridplot(plots, ncols=4, width=300, height=275)\n",
    "show(binary_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82614f44-66a4-4da9-8d09-57679050942e",
   "metadata": {},
   "source": [
    "### Binary Classification Problem Part 2: Feature Importance Hypothesis Test\n",
    "\n",
    "In Part 1 of the binary classification test, findings of interest included: i) proximity alone barely beats random guessing for predicting when a model catchment will provide support coverage of a target, ii) climate attributes improve the predictive performance substantially, iii) adding terrain attributes achieves an AUC performance of 0.9, iv) the misclassified samples on the out-of-sample test set were split nearly evenly between true and false positives, and v) overall the model correctly labels out of sample catchments roughly 8 out of 10 times.\n",
    "\n",
    "Intuitively it seems knowing the mean precipitation and the drainage area provides most of the picture as far as the general range of the distribution, so we re-test the model using just these two attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cbd5e-283d-4d0e-b066-8fca2c7613ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_underspecification_from_key_attributes(attr_df, target_variable, max_centroid_distance, results_folder, prior,\n",
    "                                loss_function=None, partial_counts=False, n_boost_rounds=100, random_seed=42, \n",
    "                              optimize_cv_folds=True, n_cv_fold_optimization_trials=20, cv_fold_seed=42):\n",
    "    counts_key = 'partial'\n",
    "    if partial_counts == \"False\":\n",
    "        counts_key = 'whole'\n",
    "    \n",
    "    all_results = {}\n",
    "    for bitrate in [4, 6, 8, 10]:\n",
    "        all_results[bitrate] = {}\n",
    "        t0 = time()\n",
    "        print(f'bitrate = {bitrate}')\n",
    "        input_data_fpath = os.path.join(input_folder, fname)\n",
    "        nrows = None\n",
    "        # df = pd.read_csv(input_data_fpath, nrows=nrows, low_memory=False)\n",
    "        df = result_dict[bitrate][counts_key]\n",
    "        df.dropna(subset=[target_variable], inplace=True)\n",
    "\n",
    "        # check the target variable balance\n",
    "        true_vals = len(df[df[target_variable] == True])\n",
    "        pct_true = int(100*true_vals / len(df))\n",
    "        print(f'{true_vals}/{len(df)} ({pct_true}%) of the dataset are underspecified')\n",
    "                        \n",
    "        # reduce the maximum distance separating pairs such that the \n",
    "        # graph can be more evenly separated.  Too permissible a distance\n",
    "        # filter increases the graph connectivity, making it difficult to\n",
    "        # create cross validation folds with no data leakage.\n",
    "        df = df[df['centroid_distance'] < max_centroid_distance]\n",
    "        print(f'  {len(df)} pairs remaining after filtering by max distance of {max_centroid_distance} km')        \n",
    "        # add the attributes into the input dataset\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        df = add_attributes(attr_df, df.copy(), all_attributes)\n",
    "        fold_dict = create_fold_dict(df)\n",
    "\n",
    "        # add attribute groups successively\n",
    "        predictor_attributes = []\n",
    "        # make sure the order of attributes matches the attribute_set_names list\n",
    "        group_sets = [['prcp',], ['drainage_area_km2',], ['prcp', 'drainage_area_km2',]]\n",
    "        small_set_names = ['precipitation', 'area', 'precip+area']\n",
    "        for attribute_set, set_name in zip(group_sets, small_set_names):\n",
    "            print(f'  Processing {set_name} attribute set: {target_variable}')\n",
    "            # initialize the predictor variables (features)\n",
    "            predictor_attributes = attribute_set\n",
    "            non_dist_features = [c for c in predictor_attributes if c != 'centroid_distance']\n",
    "            pair_features = format_features(non_dist_features)\n",
    "            # inter-catchment differences of attributes \n",
    "            diff_features = [f'{c}_diff' for c in non_dist_features]\n",
    "            features = pair_features + diff_features\n",
    "\n",
    "            best_params, best_mean_test_perf, best_learning_curve, results_all_trials, best_trial_predictions = run_binary_trials_custom_CV(\n",
    "                set_name, features, target_variable, df, fold_dict, \n",
    "                n_cv_fold_optimization_trials, n_boost_rounds, results_folder, \n",
    "                loss=loss_function, random_seed=random_seed)\n",
    "                \n",
    "            # store the test set predictions and actuals\n",
    "            all_results[bitrate][set_name] = {\n",
    "                'best_params': best_params,\n",
    "                'all_results': results_all_trials, \n",
    "                'learning_curve': best_learning_curve,\n",
    "                'test_predictions': best_trial_predictions,\n",
    "                # 'test_rmse': best_mean_rmse,\n",
    "                'test_auc': best_mean_test_perf,\n",
    "                # 'target_cdfs': target_cdfs,\n",
    "            } \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959e19a-d1ea-41f5-9161-7fb83fb422ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_col = f'dkl_{concurrent}_post_{prior}R'\n",
    "target_column = 'underspecified_model_flag'\n",
    "loss_function = 'binary:logistic'\n",
    "partial_counts = True\n",
    "binary_test_results_fname = f'binary_classification_coverage_test_precip_DA.npy'\n",
    "if partial_counts is True:\n",
    "    binary_test_results_fname = binary_test_results_fname.replace('.npy', '_partial_counts.npy')\n",
    "binary_test_results_fpath = os.path.join(binary_results_folder, binary_test_results_fname)\n",
    "if os.path.exists(binary_test_results_fpath):\n",
    "    print('Processed and loading: ', binary_test_results_fname)\n",
    "    all_test_results = np.load(binary_test_results_fpath, allow_pickle=True).item()\n",
    "else:\n",
    "    all_test_results = predict_underspecification_from_key_attributes(\n",
    "        attr_df, target_column, max_centroid_distance, binary_results_folder, prior,\n",
    "        loss_function=loss_function, partial_counts=partial_counts, n_boost_rounds=n_boost_rounds, random_seed=random_seed,\n",
    "        optimize_cv_folds=optimize_cv_folds, n_cv_fold_optimization_trials=n_cv_fold_optimization_trials, cv_fold_seed=cv_fold_seed,\n",
    "    )\n",
    "    np.save(binary_test_results_fpath, all_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c193f-c8bd-490a-b5b1-6104f0a09632",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dict = {}\n",
    "reg_plots_dict = {}\n",
    "res_r2_dict = {}\n",
    "\n",
    "plots = []\n",
    "reg_plots_dict[prior] = {}\n",
    "res_r2_dict[prior] = {}\n",
    "\n",
    "binary_results = np.load(binary_test_results_fpath, allow_pickle=True).item()\n",
    "small_set_names = ['precipitation', 'area', 'precip+area']\n",
    "for b, binary_result in binary_results.items():\n",
    "    test_set = 'test_auc'\n",
    "\n",
    "    y2 = [binary_result[e][test_set] for e in small_set_names]\n",
    "    source = ColumnDataSource({'x': small_set_names, 'y2': y2})\n",
    "        \n",
    "    title = f'{b} bits'\n",
    "    if len(plots) == 0:\n",
    "        fig = figure(title=title, x_range=small_set_names, toolbar_location='above')\n",
    "    else:\n",
    "        fig = figure(title=title, x_range=small_set_names, y_range=plots[0].y_range, \n",
    "                     output_backend='webgl', toolbar_location='above')\n",
    "\n",
    "    fig.line('x', 'y2', legend_label='AUC', color='dodgerblue', source=source, line_width=3)\n",
    "    fig.legend.background_fill_alpha = 0.6\n",
    "    fig.legend.location = 'bottom_right'\n",
    "    fig.yaxis.axis_label = 'AUC'\n",
    "    fig.xaxis.axis_label = 'Attribute Group (additive)'\n",
    "    plots.append(fig)\n",
    "    \n",
    "    result_df = pd.DataFrame({'set': small_set_names, 'auc': y2})\n",
    "    best_auc_idx = result_df['auc'].idxmax()\n",
    "    best_auc_set = result_df.loc[best_auc_idx, 'set']\n",
    "    foo = binary_result[best_auc_set]\n",
    "    \n",
    "    # plot the test set convergence for the 'best' trial\n",
    "    cfig = figure(title=f'AUC ({best_auc_set} set)',)\n",
    "    learning_curve_sets = binary_result[best_auc_set]['learning_curve']\n",
    "    train_curve_dfs, test_curve_dfs = [], []\n",
    "    n = 0\n",
    "    for lc_dict in learning_curve_sets:\n",
    "        train_df = pd.DataFrame(lc_dict['train']['auc'], columns=[n])\n",
    "        test_df = pd.DataFrame(lc_dict['eval']['auc'], columns=[n])\n",
    "        test_curve_dfs.append(test_df)\n",
    "        train_curve_dfs.append(train_df)\n",
    "        n += 1\n",
    "\n",
    "    train_curve_df = pd.concat(train_curve_dfs, axis=1)\n",
    "    train_curve_df['mean'] = train_curve_df.mean(axis=1)\n",
    "    test_curve_df = pd.concat(test_curve_dfs, axis=1)\n",
    "    test_curve_df['mean'] = test_curve_df.mean(axis=1)\n",
    "    \n",
    "    for fn in range(len(learning_curve_sets)):\n",
    "        cfig.line(train_curve_df.index, train_curve_df[fn], line_alpha=0.6, line_color='grey', line_dash='dotted')\n",
    "        cfig.line(test_curve_df.index, test_curve_df[fn], line_alpha=0.6, line_color='red', line_dash='dotted')\n",
    "    cfig.line(train_curve_df.index, train_curve_df['mean'], line_alpha=0.5, line_color='grey', \n",
    "              line_width=2, legend_label='CV Mean (Train)')\n",
    "    cfig.line(test_curve_df.index, test_curve_df['mean'], line_alpha=0.5, line_color='red', \n",
    "              line_width=2, legend_label='CV Mean (Test)')\n",
    "\n",
    "    # find the min predictive risk (optimal complexity)\n",
    "    min_pred_risk_idx = test_curve_df['mean'].idxmax()\n",
    "    if min_pred_risk_idx == max(test_curve_df['mean'].index):\n",
    "        print(f'Min prediction risk occurs at the maximum iteration, try increasing the number of boosting rounds')\n",
    "    \n",
    "    min_pred_risk = test_curve_df.loc[min_pred_risk_idx, 'mean']\n",
    "    cfig.line([min_pred_risk_idx, min_pred_risk_idx], [test_curve_df['mean'].min(), min_pred_risk], \n",
    "              legend_label='Min risk', color='green', line_width=2, line_dash='dashed')\n",
    "\n",
    "    cfig.xaxis.axis_label = r'$$\\text{Iteration}$$'\n",
    "    cfig.yaxis.axis_label = r'$$\\text{AUC} $$'\n",
    "    cfig.legend.background_fill_alpha = 0.5\n",
    "    cfig.legend.location = 'bottom_right'\n",
    "    plots.append(cfig)\n",
    "\n",
    "    all_test_predictions = pd.concat(binary_result[best_auc_set]['test_predictions'], axis=0)\n",
    "    # find a threshold to balance (minimize) false positives and false negatives\n",
    "    optimal_threshold = find_optimal_threshold(all_test_predictions)\n",
    "    # plot the confusion matrix\n",
    "    cm_plot = plot_confusion_matrix(all_test_predictions, threshold=optimal_threshold)\n",
    "    plots.append(cm_plot)\n",
    "\n",
    "    cdf_plot = figure(title=f'AUC CDF ({best_auc_set} set) Threshold={optimal_threshold:.2f}',)\n",
    "    for preds in binary_result[best_auc_set]['test_predictions']:\n",
    "        pct_true = np.sum(preds['actual']) / len(preds)\n",
    "        percentiles = np.linspace(1, 100, 1000)\n",
    "        cdf = np.percentile(preds['predicted'], percentiles)\n",
    "        cdf_plot.line(percentiles, cdf, color='grey', line_width=1.5, line_alpha=0.5, legend_label='test folds')\n",
    "    all_mean = np.percentile(all_test_predictions['predicted'], percentiles)\n",
    "    pct_true = all_test_predictions['actual'].sum() / len(all_test_predictions)\n",
    "    cdf_plot.line(percentiles, all_mean, color='red', alpha=0.5, line_width=2,\n",
    "                  legend_label=f'{100*pct_true:.0f}% True')\n",
    "    cdf_plot.legend.location = 'bottom_right'\n",
    "    plots.append(cdf_plot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdfb25-cc50-4136-8d70-abe6bd89d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_layout = gridplot(plots, ncols=4, width=300, height=275)\n",
    "show(binary_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0da28-95c4-4550-96cc-09f97ff9b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_layout = gridplot(plots, ncols=4, width=300, height=275)\n",
    "show(binary_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d981f83-b2a1-43c4-b134-a53156556b14",
   "metadata": {},
   "source": [
    "### Regression Problem\n",
    "\n",
    "The same problem setup applies for the regression prediction problem which is to optimize the discriminant function and the input signal quantization simultaneously to minimize the error in predicting the KL divergence from catchment attributes.  \n",
    "\n",
    "Instead of predicting a scalar measure which is a feature of a single location, the key difference in this step is the target variable describes a measure of the difference in runoff between **pairs of locations**.  This approach asks whether the **Kullback-Leibler Divergence** (KLD) of the distribution of unit area runoff between two locations can be predicted from the attributes of both catchments (and their differences) using the gradient boosted decision tree method, which is also capable of predicting continuous variables, in this case $D_{KL}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb16a8-bd98-4c6f-b786-b8d0e8bd36a7",
   "metadata": {},
   "source": [
    "### Set trial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10314ed-631c-43cf-9f7a-4ea9ea6de8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the amount of data to set aside for final testing\n",
    "# n_cv_folds = 5\n",
    "n_boost_rounds = 2500\n",
    "priors_to_test = [-2, -1, 0, 1, 2]\n",
    "random_seed = 42\n",
    "loss_function = 'reg:absoluteerror'  # L1 objective function for regression\n",
    "\n",
    "#define if testing concurrent or nonconcurrent data\n",
    "concurrent = 'concurrent'\n",
    "\n",
    "# partial counts refer to the test where observations were assigned\n",
    "# a uniform distribution to approximate error and allow fractional \n",
    "# observations in state space\n",
    "partial_counts = False\n",
    "\n",
    "# cross validation parameters\n",
    "optimize_cv_folds = False\n",
    "cv_fold_seed = 83561\n",
    "n_cv_fold_optimization_trials = 20\n",
    "# limit the maximum distance to make the network \n",
    "# graph of station pairs more separable\n",
    "max_centroid_distance = 1000\n",
    "\n",
    "attribute_set_names = ['proximity', '+climate', '+terrain', '+land_cover', '+soil']\n",
    "attribute_group_sets = [['centroid_distance'], climate, terrain, land_cover, soil]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe86f8-8454-4e81-85d6-f110346f53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_columns = [c for c in df_partial.columns if c.startswith(f'dkl_{concurrent}_post')]\n",
    "print(posterior_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea495aa3-1b0d-4d62-9715-e076a3048069",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = os.path.join(BASE_DIR, 'data', 'kld_prediction_results')\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4e2f4-a431-4140-8b41-f85c679802aa",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "The input dataset is pairwise comparisons of just over 1300 (streamflow) monitored catchments, their attributes, and the attribute differences.  After filtering for data concurrency (minimum 1 year, < 5 days missing per month) and maximum distance between basin centroids (500 km) we are left with roughly 225K pairs.  The pairwise setup means that station data appears in more than one row.  As a result, the attributes of stations can end up in both training and test sets if we simply split by randomly assigning rows to training or test sets.   We can't simply cut edges until the graph is separated because it is a [generalization of the \"keeping a subset of vertices\" problem in graph theory which is NP-hard](https://en.wikipedia.org/wiki/Independent_set_(graph_theory)).\n",
    "\n",
    "To address this issue we split the dataset spatially to create partitioned datasets for each fold and draw samples from within the \"cluster\" while filtering all edges between the fold and the rest of the set.  The end goal is to generate training folds where the official_id does not appear in both training and test set, in either proxy or target column.  One problem remains, and that is to generate training and test sets with some assurance that the target variable distributions match to some degree, but this is less critical than ensuring there is no data leakage between training and test data.  \n",
    "\n",
    "Next we create training/test splits and visualize how the target variable distributions compare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442359a3-ccf8-4257-b04a-3d102e8aae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_cdf(data):\n",
    "    sorted_data = np.sort(data)\n",
    "    cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    return sorted_data, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f696c0c-a732-4cc2-8460-55b978ff7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result_by_prior(prior, binary=False, partial_counts=False):\n",
    "    rf = os.path.join('data', 'kld_prediction_results')\n",
    "    fname = f'dkl_{concurrent}_post_{prior}R_{prior}_prior_results.npy'\n",
    "    if partial_counts == True:\n",
    "        fname = fname.replace('.npy', '_partial_counts.npy')\n",
    "    fpath = os.path.join(rf, fname)\n",
    "    return np.load(fpath, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06244546-3c48-4982-b988-bc10af184af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_model(\n",
    "    input_data, fold_no, cv_data, attributes, target, params, num_boost_rounds, loss\n",
    "):    \n",
    "    test_idxs = cv_data['test']\n",
    "    train_idxs = cv_data['train']   \n",
    "\n",
    "    test_data = input_data.iloc[test_idxs, :].copy()\n",
    "    train_data = input_data.iloc[train_idxs, :].copy()\n",
    "\n",
    "    X_train = train_data[attributes].values\n",
    "    Y_train = np.log10(train_data[target].values)\n",
    "    X_test = test_data[attributes].values\n",
    "    Y_test = np.log10(test_data[target].values)\n",
    "\n",
    "    # model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=Y_test)\n",
    "\n",
    "    eval_list = [(dtrain, \"train\"), (dtest, \"eval\")]\n",
    "    evals_result = {}\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_rounds,\n",
    "        evals=eval_list,\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=0,\n",
    "    )\n",
    "    eval_keys = list(evals_result['train'].keys())\n",
    "    if len(eval_keys) > 1:\n",
    "        print(f' setting eval key to {eval_keys[0]} from {eval_keys}')\n",
    "\n",
    "    eval_key = eval_keys[0]\n",
    "    # Convert the lists to NumPy arrays with dtype=object\n",
    "    train_perf = np.array(evals_result['train'][eval_key], dtype=object)\n",
    "    test_perf = np.array(evals_result['eval'][eval_key], dtype=object)\n",
    "    fold_progress = pd.DataFrame({\n",
    "        'train': evals_result['train'][eval_key],\n",
    "        'test': evals_result['eval'][eval_key],\n",
    "        'fold': [fold_no]*len(evals_result['train'][eval_key]),\n",
    "    })\n",
    "    predicted = bst.predict(dtest)\n",
    "\n",
    "    return train_perf, test_perf, fold_progress, predicted, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67846b89-82f2-4c00-ab87-e080df8cae09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_xgb_trials_custom_CV(\n",
    "    set_name,\n",
    "    attributes,\n",
    "    target,\n",
    "    input_data,\n",
    "    fold_dict, \n",
    "    n_optimization_rounds,\n",
    "    num_boost_rounds,\n",
    "    results_folder,\n",
    "    loss='reg:squarederror',\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom CV refers to cross validation.  Custom cross validation means the \n",
    "    held-out set must be determined in a more robust way to avoid \"data leakage\".\n",
    "    That is, the pairs making up the training, validation, and test sets must \n",
    "    be made up of pairings from unique sets of stations.\n",
    "    \"\"\"\n",
    "    # select random hyperparameters for n_optimization_rounds\n",
    "    sample_choices = np.arange(0.5, 0.9, 0.02)  # subsample and colsample percentages\n",
    "    lr_choices = np.arange(0.001, 0.1, 0.0005)  # learning rates\n",
    "    learning_rates = np.random.choice(lr_choices, n_optimization_rounds)\n",
    "    subsamples = np.random.choice(sample_choices, n_optimization_rounds)\n",
    "    colsamples = np.random.choice(sample_choices, n_optimization_rounds)\n",
    "    num_boost_rounds = num_boost_rounds\n",
    "    eval_key = loss.split(':')[1]\n",
    "\n",
    "    all_results = []\n",
    "    best_result = (None, np.inf, None)\n",
    "    best_params = None\n",
    "    best_mean_test_perf = np.inf\n",
    "    best_convergence_df = pd.DataFrame()\n",
    "    best_trial_test_predictions = None\n",
    "    output_target_cdfs = None\n",
    "    for trial in range(n_optimization_rounds):\n",
    "        lr, ss, cs = learning_rates[trial], subsamples[trial], colsamples[trial]\n",
    "        params = {\n",
    "            \"objective\": loss,\n",
    "            \"eta\": lr,\n",
    "            # \"max_depth\": 6,  # use default max_depth\n",
    "            # \"min_child_weight\": 1, # use colsample and subsample instead of min_child_weight\n",
    "            \"subsample\": ss,\n",
    "            \"colsample_bytree\": cs,\n",
    "            \"seed\": random_seed,\n",
    "            \"device\": \"cuda\",  # note, change this to 'cpu' if your system doesn't have a CUDA GPU\n",
    "            \"sampling_method\": \"gradient_based\",\n",
    "            \"tree_method\": \"hist\",\n",
    "        }\n",
    "\n",
    "        results_fname = (\n",
    "            f\"{set_name}_{bitrate}_bits_{lr:.3f}_lr_{ss:.3f}_sub_{cs:.3f}_col.csv\"\n",
    "        )\n",
    "        results_fpath = os.path.join(results_folder, results_fname)\n",
    "\n",
    "        # k-fold cross validation\n",
    "        fold_scores = []\n",
    "        n_samples = len(input_data)\n",
    "        fold_no = 0\n",
    "        best_train_fold_perf, best_test_fold_perf, best_test_rounds = [], [], []\n",
    "        all_fold_results = []\n",
    "        fold_scores, fold_arrays = [], []\n",
    "        target_cdfs = []\n",
    "        for fold_no, cv_data in fold_dict.items():\n",
    "            train_perf, test_perf, fold_progress, predicted, Y_test = train_xgb_model(\n",
    "                input_data,\n",
    "                fold_no,\n",
    "                cv_data,\n",
    "                attributes,\n",
    "                target,\n",
    "                params,\n",
    "                num_boost_rounds,\n",
    "                loss\n",
    "            )            \n",
    "            fold_arrays.append(fold_progress)\n",
    "            \n",
    "            test_ids = input_data.loc[cv_data['test'], ['proxy', 'target']].values\n",
    "            test_ids = [f'{e[0]}_{e[1]}' for e in test_ids]\n",
    "\n",
    "            test_results = pd.DataFrame(\n",
    "                {\"predicted\": predicted, \"actual\": Y_test, \"proxy_target\": test_ids}\n",
    "            )\n",
    "            \n",
    "            ordered_data, fold_cdf = compute_empirical_cdf(Y_test)\n",
    "            target_cdfs += [(ordered_data, fold_cdf)]\n",
    "            \n",
    "            # Get the round with the best validation score (out-of-sample performance)\n",
    "            best_perf_round_train, best_perf_round_test = np.argmin(train_perf), np.argmin(test_perf)\n",
    "            \n",
    "            # Store the metrics at the best round (minimum risk)\n",
    "            train_perf_best = train_perf[best_perf_round_train]\n",
    "            test_perf_best = test_perf[best_perf_round_test]\n",
    "\n",
    "            best_train_fold_perf.append(train_perf_best)\n",
    "            best_test_fold_perf.append(test_perf_best)\n",
    "            best_test_rounds.append(best_perf_round_test)\n",
    "            all_fold_results.append(test_results)\n",
    "            fold_no += 1\n",
    "\n",
    "        all_test_predictions_df = pd.concat(all_fold_results)\n",
    "        convergence_df = pd.concat(fold_arrays)    \n",
    "        \n",
    "        mean_test_perf = np.mean(best_test_fold_perf)\n",
    "        stdev_test_perf = np.std(best_test_fold_perf)\n",
    "        # # track the trial error metrics\n",
    "        results_dict = {\n",
    "            'trial': trial,\n",
    "            f'test_{eval_key}_mean': mean_test_perf,\n",
    "            f'test_{eval_key}_stdev': stdev_test_perf,\n",
    "        }\n",
    "        \n",
    "        results_cols = list(results_dict.keys())\n",
    "        results_dict.update(params)\n",
    "        all_results.append(results_dict)\n",
    "        if (trial > 0) & (trial % 10 == 0):\n",
    "            print(f\"   completed {trial}/{n_optimization_rounds}\")\n",
    "            \n",
    "        if round(mean_test_perf,2) < round(best_mean_test_perf, 2):\n",
    "            best_params = params\n",
    "            best_mean_test_perf = mean_test_perf\n",
    "            best_convergence_df = convergence_df\n",
    "            best_trial_test_predictions = all_test_predictions_df\n",
    "            output_target_cdfs = target_cdfs\n",
    "            print(f'    New best result: {eval_key}={mean_test_perf:.2f} (trial {trial})')\n",
    "    \n",
    "    # save the best trial results\n",
    "    best_trial_test_predictions.to_csv(results_fpath)\n",
    "    results_all_trials = pd.DataFrame(all_results)\n",
    "    # get the mean and standard deviation of the error metrics over all trials\n",
    "    all_trials_mean = results_all_trials[f\"test_{eval_key}_mean\"].mean()\n",
    "    all_trials_stdev = results_all_trials[f\"test_{eval_key}_mean\"].std()\n",
    "    print(\n",
    "        f\"    {all_trials_mean:.2f} ± {2*all_trials_stdev:.3f} mean (95% CI) {eval_key} (of {len(results_all_trials)} hyperparameter optimization rounds.)\"\n",
    "    )\n",
    "    return best_params, best_mean_test_perf, best_convergence_df, best_trial_test_predictions, results_all_trials, output_target_cdfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b4c45-cb1d-445c-95e2-65dd657e9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_KLD_from_attributes(attr_df, target_variable, max_centroid_distance, results_folder, prior,\n",
    "                                loss_function=None, partial_counts=False, n_boost_rounds=100, random_seed=42, \n",
    "                              optimize_cv_folds=True, n_cv_fold_optimization_trials=20, cv_fold_seed=42):\n",
    "    counts_key = 'partial'\n",
    "    if partial_counts == \"False\":\n",
    "        counts_key = 'whole'\n",
    "    \n",
    "    all_results = {}\n",
    "    for bitrate in [4, 6, 8, 10]:\n",
    "        all_results[bitrate] = {}\n",
    "        t0 = time()\n",
    "        print(f'bitrate = {bitrate} (prior=10^{prior})')\n",
    "        input_data_fpath = os.path.join(input_folder, fname)\n",
    "        nrows = None\n",
    "        # df = pd.read_csv(input_data_fpath, nrows=nrows, low_memory=False)\n",
    "        df = result_dict[bitrate][counts_key]\n",
    "        df.dropna(subset=[target_variable], inplace=True)\n",
    "                        \n",
    "        # reduce the maximum distance separating pairs such that the \n",
    "        # graph can be more evenly separated.  Too permissible a distance\n",
    "        # filter increases the graph connectivity, making it difficult to\n",
    "        # create cross validation folds with no data leakage.\n",
    "        df = df[df['centroid_distance'] < max_centroid_distance]\n",
    "        print(f'  {len(df)} pairs remaining after filtering by max distance of {max_centroid_distance} km')        \n",
    "        # add the attributes into the input dataset\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        df = add_attributes(attr_df, df.copy(), all_attributes)\n",
    "        fold_dict = create_fold_dict(df)\n",
    "\n",
    "        # add attribute groups successively\n",
    "        predictor_attributes = []\n",
    "        # make sure the order of attributes matches the attribute_set_names list\n",
    "        for attribute_set, set_name in zip(attribute_group_sets, attribute_set_names):\n",
    "            print(f'  Processing {set_name} attribute set: {target_variable}')\n",
    "            \n",
    "            # initialize the predictor variables (features)\n",
    "            predictor_attributes += attribute_set\n",
    "            if set_name == 'proximity':\n",
    "                features = ['centroid_distance']\n",
    "            else:\n",
    "                non_dist_features = [c for c in predictor_attributes if c != 'centroid_distance']\n",
    "                pair_features = format_features(non_dist_features)\n",
    "                diff_features = [f'{c}_diff' for c in non_dist_features]\n",
    "                features = ['centroid_distance'] + pair_features + diff_features                \n",
    "\n",
    "            best_params, best_mean_rmse, best_convg_df, best_trial_test_predictions, results_all_trials, target_cdfs = run_xgb_trials_custom_CV(\n",
    "                    set_name, features, target_variable, df, fold_dict, \n",
    "                    n_cv_fold_optimization_trials, n_boost_rounds, results_folder, loss=loss_function, random_seed=random_seed,\n",
    "            )\n",
    "            # store the test set predictions and actuals\n",
    "            all_results[bitrate][set_name] = {\n",
    "                'best_params': best_params,\n",
    "                'all_results': results_all_trials, \n",
    "                'convergence': best_convg_df,\n",
    "                'oos_predictions': best_trial_test_predictions,\n",
    "                # 'test_rmse': best_mean_rmse,\n",
    "                'test_mae': best_mean_rmse,\n",
    "                'target_cdfs': target_cdfs,\n",
    "            } \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9796376-b870-45b3-a9a5-3adc7ed60f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f43e5-3c86-44d1-8c7b-34349ae460ce",
   "metadata": {},
   "source": [
    "## Run Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116fb09-2489-4dcf-a6ca-8e00f73b70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_to_test = [-2, -1, 0, 1, 2]\n",
    "rev_date = '20241202'\n",
    "for prior in priors_to_test:\n",
    "    target_col = f'dkl_{concurrent}_post_{prior}R'\n",
    "    test_results_fname = f'{target_col}_{prior}_prior_results_{rev_date}.npy'\n",
    "    test_results_fname = f'{target_col}_{prior}_prior_results.npy'\n",
    "    if partial_counts is False:\n",
    "        test_results_fname = f'{target_col}_{prior}_prior_results_{rev_date}.npy'\n",
    "        test_results_fname = f'{target_col}_{prior}_prior_results.npy'\n",
    "    else:\n",
    "        test_results_fname = f'{target_col}_{prior}_prior_results_partial_counts_{rev_date}.npy'\n",
    "        test_results_fname = f'{target_col}_{prior}_prior_results_partial_counts.npy'\n",
    "    test_results_fpath = os.path.join(results_folder, test_results_fname)\n",
    "    if os.path.exists(test_results_fpath):\n",
    "        print('processed and loading: ', test_results_fname)\n",
    "        all_test_results = np.load(test_results_fpath, allow_pickle=True).item()\n",
    "    else:\n",
    "        all_test_results = predict_KLD_from_attributes(\n",
    "            attr_df, target_col, max_centroid_distance, results_folder, prior,\n",
    "            loss_function=loss_function, partial_counts=partial_counts, n_boost_rounds=n_boost_rounds, random_seed=random_seed,\n",
    "            optimize_cv_folds=optimize_cv_folds, n_cv_fold_optimization_trials=n_cv_fold_optimization_trials, cv_fold_seed=cv_fold_seed,\n",
    "        )                               \n",
    "        np.save(test_results_fpath, all_test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f72be-d958-41d3-a26b-f5235b10b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fig_fonts(fig, font_size=20, font='Bitstream Charter', legend=True):\n",
    "    fig.xaxis.axis_label_text_font_size = f'{font_size}pt'\n",
    "    fig.yaxis.axis_label_text_font_size = f'{font_size}pt'\n",
    "    fig.xaxis.major_label_text_font_size = f'{font_size-2}pt'\n",
    "    fig.yaxis.major_label_text_font_size = f'{font_size-2}pt'\n",
    "    fig.yaxis.axis_label_text_font = font\n",
    "    fig.xaxis.axis_label_text_font = font\n",
    "    fig.xaxis.major_label_text_font = font\n",
    "    fig.yaxis.major_label_text_font = font\n",
    "    if legend == True:\n",
    "        fig.legend.label_text_font_size = f'{font_size-2}pt'\n",
    "        fig.legend.label_text_font = font\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6271a5f-d3a0-4194-97e5-fc0aee52649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result_by_prior(prior, rev_date, binary=False, partial_counts=False):\n",
    "    rf = os.path.join('data', 'kld_prediction_results')\n",
    "    fname = f'dkl_{concurrent}_post_{prior}R_{prior}_prior_results_{rev_date}.npy'\n",
    "    fname = f'dkl_{concurrent}_post_{prior}R_{prior}_prior_results.npy'\n",
    "    if partial_counts == True:\n",
    "        fname = fname.replace('.npy', '_partial_counts.npy')\n",
    "    fpath = os.path.join(rf, fname)\n",
    "    return np.load(fpath, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f4545-2911-4235-9bfd-1e034821cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = -1\n",
    "bt = 6\n",
    "result_by_prior = load_result_by_prior(pt, rev_date, binary=False)[b]\n",
    "test_set = 'test_mae'\n",
    "y2 = [result[e][test_set] for e in attribute_set_names]\n",
    "\n",
    "result_df = pd.DataFrame({'set': attribute_set_names, 'mae': y2})\n",
    "# best_rmse_idx = result_df['rmse'].idxmin()\n",
    "best_mae_idx = result_df['mae'].idxmin()\n",
    "\n",
    "# best_rmse_set = result_df.loc[best_rmse_idx, 'set']\n",
    "best_mae_set = result_df.loc[best_mae_idx, 'set']\n",
    "foo = result[best_mae_set]\n",
    "best_result = result[best_mae_set]['oos_predictions']\n",
    "xx, yy = best_result['actual'], best_result['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4163b92-bd66-4095-9a93-237aeec8e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_min, x_max = np.min(np.log10(xx)), np.max(np.log10(xx))\n",
    "edges = np.linspace(x_min, x_max, 20)\n",
    "edges = np.power(10, edges)\n",
    "hist, edges = np.histogram(xx, bins=edges)\n",
    "# Prepare data for Bokeh\n",
    "source = ColumnDataSource(data={\n",
    "    \"top\": hist,\n",
    "    \"left\": edges[:-1],\n",
    "    \"right\": edges[1:]\n",
    "})\n",
    "p = figure(title=f'{bt} bits', toolbar_location='above', x_axis_type='log', height=400, width=550)\n",
    "# Add quad glyph for the histogram\n",
    "p.xaxis.axis_label = r'$$\\text{Observed } D_\\text{KL}(P||Q)$$'\n",
    "p.yaxis.axis_label = r'$$\\text{Pr}(X)$$'\n",
    "p.quad(top=\"top\", bottom=0, left=\"left\", right=\"right\", source=source, \n",
    "       fill_color=\"blue\", line_color=\"black\", alpha=0.4)\n",
    "p = dpf.format_fig_fonts(p, font_size=14)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36c533-34fa-41d5-bd86-f07b8f230950",
   "metadata": {},
   "source": [
    "## Plot Results of $D_{KL}$ Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e405798-b88a-4b3a-87d0-957eb9532ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layout_dict = {}\n",
    "reg_plots_dict = {}\n",
    "res_r2_dict = {}\n",
    "for prior in priors_to_test:\n",
    "    print(prior)\n",
    "    plots = []\n",
    "    reg_plots_dict[prior] = {}\n",
    "    res_r2_dict[prior] = {}\n",
    "    result_by_prior = load_result_by_prior(prior, rev_date, binary=False)\n",
    "    for b in result_by_prior.keys():\n",
    "        result = result_by_prior[b]\n",
    "\n",
    "        test_rmse, test_mae = [], []\n",
    "        test_set = 'test_mae'\n",
    "\n",
    "        y2 = [result[e][test_set] for e in attribute_set_names]\n",
    "        source = ColumnDataSource({'x': attribute_set_names, 'y2': y2})\n",
    "            \n",
    "        title = f'{b} bits (Q(θ|D)∼Dirichlet(α=10^{prior}))'\n",
    "        if len(plots) == 0:\n",
    "            fig = figure(title=title, x_range=attribute_set_names, toolbar_location='above',\n",
    "                        output_backend='webgl')\n",
    "        else:\n",
    "            fig = figure(title=title, x_range=attribute_set_names, y_range=plots[0].y_range, \n",
    "                         output_backend='webgl', toolbar_location='above',\n",
    "                        )\n",
    "        # fig.line('x', 'y1', legend_label='rmse', color='green', source=source, line_width=3)\n",
    "        fig.line('x', 'y2', legend_label='mae', color='dodgerblue', source=source, line_width=3)\n",
    "        fig.legend.background_fill_alpha = 0.6\n",
    "        fig.yaxis.axis_label = 'Error'\n",
    "        fig.xaxis.axis_label = 'Attribute Group (additive)'\n",
    "        plots.append(fig)\n",
    "        \n",
    "        result_df = pd.DataFrame({'set': attribute_set_names, 'mae': y2})\n",
    "        # best_rmse_idx = result_df['rmse'].idxmin()\n",
    "        best_mae_idx = result_df['mae'].idxmin()\n",
    "        # best_rmse_set = result_df.loc[best_rmse_idx, 'set']\n",
    "        best_mae_set = result_df.loc[best_mae_idx, 'set']\n",
    "        foo = result[best_mae_set]\n",
    "        best_result = result[best_mae_set]['oos_predictions']\n",
    "        xx, yy = best_result['actual'], best_result['predicted']\n",
    "        xmin, ymin = np.nanmin(xx), np.nanmin(yy)\n",
    "        print(xmin, ymin)\n",
    "        # print(asdf)\n",
    "        slope, intercept, r, p, se = linregress(xx, yy)\n",
    "        \n",
    "        # sfig = figure(title=f'Test: {b} bits best model {best_rmse_set} (N={len(best_result)})', toolbar_location='above')\n",
    "        sfig = figure(title=f'{b} bits', toolbar_location='above')#, x_axis_type='log', y_axis_type='log')\n",
    "        # sfig.scatter(xx, yy, size=1, alpha=0.6)\n",
    "        # Create hexbin plot\n",
    "        binsize=0.05\n",
    "        hex_renderer, hex_data = sfig.hexbin(xx, yy, size=binsize, hover_color=\"pink\", hover_alpha=0.8)\n",
    "        \n",
    "        # Add color mapping based on bin counts\n",
    "        counts = hex_data['counts']  # Extract the counts from the source\n",
    "        mapper = linear_cmap(field_name='counts', palette=Greys256[::-1], low=min(counts), high=max(counts))\n",
    "        \n",
    "        # Plot the hex tiles using the color mapping\n",
    "        sfig.hex_tile(q=\"q\", r=\"r\", size=binsize, line_color=None, source=hex_data, fill_color=mapper)\n",
    "        \n",
    "        xpred = np.linspace(min(xx), max(xx), 100)\n",
    "        ybf = [slope * e + intercept for e in xpred]\n",
    "        sfig.line(xpred, ybf, color='red', line_width=3, line_dash='dashed', legend_label=f'R²={r**2:.2f}')   \n",
    "        # plot a 1:1 line\n",
    "        sfig.line([min(yy), max(yy)], [min(yy), max(yy)], color='black', line_dash='dotted', \n",
    "                  line_width=2, legend_label='1:1')\n",
    "        sfig.xaxis.axis_label = r'Actual $$D_{KL}$$ [bits/sample]'\n",
    "        sfig.yaxis.axis_label = r'Predicted $$D_{KL}$$ [bits/sample]'\n",
    "        sfig.legend.background_fill_alpha = 0.6\n",
    "        sfig.legend.location = 'top_left'\n",
    "        reg_plots_dict[prior][b] = sfig\n",
    "        res_r2_dict[prior][b] = r**2\n",
    "        \n",
    "        plots.append(sfig)   \n",
    "    \n",
    "        # plot the test set convergence for the 'best' trial\n",
    "        cfig = figure(title=f'Loss Curve ({best_mae_set} set)',)\n",
    "        convergence_df = result[best_mae_set]['convergence']\n",
    "    \n",
    "        # Pivot the data to get separate columns for each fold\n",
    "        train_pivot = convergence_df.pivot(columns='fold', values='train')\n",
    "        test_pivot = convergence_df.pivot(columns='fold', values='test')\n",
    "    \n",
    "        # Rename the columns to indicate folds\n",
    "        train_pivot.columns = [f'fold_{col}' for col in train_pivot.columns]\n",
    "        test_pivot.columns = [f'fold_{col}' for col in test_pivot.columns]\n",
    "        train_pivot['mean'] = train_pivot.mean(axis=1)\n",
    "        test_pivot['mean'] = test_pivot.mean(axis=1)\n",
    "        fold_nos = sorted(list(set(convergence_df['fold'])))\n",
    "        \n",
    "        for fn in fold_nos:\n",
    "            cfig.line(test_pivot.index, test_pivot[f'fold_{fn}'], line_alpha=0.6, line_color='red', line_dash='dotted')\n",
    "            cfig.line(train_pivot.index, train_pivot[f'fold_{fn}'], line_alpha=0.6, line_color='grey', line_dash='dotted')\n",
    "        cfig.line(train_pivot.index, train_pivot['mean'], line_alpha=0.5, line_color='grey', \n",
    "                  line_width=2, legend_label='CV Mean (Train)')\n",
    "        cfig.line(test_pivot.index, test_pivot['mean'], line_alpha=0.5, line_color='red', \n",
    "                  line_width=2, legend_label='CV Mean (Test)')\n",
    "    \n",
    "        # find the minimum predictive risk (optimal complexity)\n",
    "        min_pred_risk_idx = test_pivot['mean'].idxmin()\n",
    "        if min_pred_risk_idx == max(test_pivot['mean'].index):\n",
    "            print(f'Min prediction risk occurs at the maximum iteration, try increasing the number of boosting rounds')\n",
    "        \n",
    "        min_pred_risk = test_pivot.loc[min_pred_risk_idx, 'mean']\n",
    "        cfig.line([min_pred_risk_idx, min_pred_risk_idx], [train_pivot['mean'].min(), min_pred_risk], \n",
    "                  legend_label='Min risk', color='green', line_width=2, line_dash='dashed')\n",
    "    \n",
    "        cfig.xaxis.axis_label = r'$$\\text{Iteration}$$'\n",
    "        cfig.yaxis.axis_label = r'$$\\text{|x-y|} $$'\n",
    "        cfig.legend.background_fill_alpha = 0.5\n",
    "        cfig.legend.location = 'top_right'\n",
    "        plots.append(cfig)\n",
    "        # plot a 1:1 line\n",
    "        sfig.line([min(ybf), max(ybf)], [min(ybf), max(ybf)], color='black', line_dash='dotted', \n",
    "                  line_width=2, legend_label='1:1')\n",
    "    \n",
    "        # plot the cdfs of the target variables in each fold to compare\n",
    "        cdffig = figure(title=f'Target Variable CDFs by fold', x_axis_type='log')\n",
    "        cdf_arrays = result[best_mae_set]['target_cdfs']\n",
    "        for (cdfx, cdfy) in cdf_arrays:\n",
    "            cdffig.line(cdfx, cdfy, color='black', line_alpha=0.6, line_width=2)\n",
    "        plots.append(cdffig)\n",
    "        cdffig.xaxis.axis_label = r'$$\\text{Observed Values } \\text{[bits/sample]}$$'\n",
    "        cdffig.yaxis.axis_label = r'$$\\text{Pr}(x\\leq X)$$'\n",
    "        \n",
    "    layout_dict[prior] = gridplot(plots, ncols=4, width=300, height=275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016f803-cafa-4721-8e83-1768e89e70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(layout_dict[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81056b-c05a-4e60-a53b-e50cc6c8a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(layout_dict[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5502684-7eca-4565-bb7f-6a966909ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(layout_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7082d28-2f23-4947-971a-a3342830d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(layout_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9099e77-470b-4283-97b9-b3e071b0ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(layout_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945ca57-c961-426b-84a9-f5d20f179b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_plots = []\n",
    "prior = 0\n",
    "for b in [4, 6, 8, 10, 12]:\n",
    "    plot = reg_plots_dict[prior][b]\n",
    "    sample_plots.append(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f741e-ce59-4ff3-b04c-7534c4116b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_layout = gridplot(sample_plots, ncols=5, width=250, height=250)\n",
    "show(sample_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c9819-40b2-47aa-806b-967e143dcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.models import ColorBar, ColumnDataSource\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.palettes import Viridis256, gray, magma, Category20\n",
    "\n",
    "# Convert the nested dict into a DataFrame\n",
    "df = pd.DataFrame(res_r2_dict).T  # Transpose to get priors as columns\n",
    "df.index.name = 'Prior'\n",
    "df.columns.name = 'Bitrate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a062077-04c0-40e3-b367-29d9f7db4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to a long format\n",
    "df_melted = df.reset_index().melt(id_vars='Prior', var_name='Bitrate', value_name='Value')\n",
    "# Ensure the Bitrate values are ordered correctly (increasing order)\n",
    "df_melted['Bitrate'] = pd.Categorical(df_melted['Bitrate'], categories=sorted(df_melted['Bitrate'].unique(), reverse=False), ordered=True)\n",
    "df_melted['Value'] = df_melted['Value'].round(2)\n",
    "# Create a Bokeh ColumnDataSource\n",
    "source = ColumnDataSource(df_melted)\n",
    "\n",
    "# Create a figure for the heatmap\n",
    "p = figure(title=\"KL divergence from attributes: R² of test set by Prior and Bitrate\",width=600, height=500,\n",
    "           tools=\"hover\", tooltips=[('Value', '@Value{0.00}')], toolbar_location=None)\n",
    "\n",
    "# Create a color mapper\n",
    "mapper = linear_cmap(field_name='Value', palette=magma(256), low=df_melted.Value.min(), high=df_melted.Value.max())\n",
    "\n",
    "# Add rectangles to the plot\n",
    "p.rect(x=\"Prior\", y=\"Bitrate\", width=1, height=1, source=source,\n",
    "       line_color=None, fill_color=mapper)\n",
    "\n",
    "# Add color bar\n",
    "color_bar = ColorBar(color_mapper=mapper['transform'], width=8, location=(0,0))\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "# Format plot\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.xaxis.axis_label = r'$$Q(θ|D)∼\\text{Dirichlet}(\\alpha = 10^{a})$$'\n",
    "p.yaxis.axis_label = r'$$\\text{Quantization Bitrate (dictionary size)}$$'\n",
    "p.axis.major_label_text_font_size = \"10pt\"\n",
    "p.axis.major_label_standoff = 0\n",
    "p.xaxis.major_label_orientation = 1.0\n",
    "\n",
    "# Output the plot to an HTML file and display it\n",
    "# output_file(\"heatmap.html\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f21fc-62e0-47d9-a0bb-9aafde797ff8",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Since KL divergence $D_{KL}(P||Q) = \\sum_{i=1}^{2^b} p_i\\log(\\frac{p_i}{q_i}) = +\\infty \\text{ when any } q_i \\rightarrow 0$, the simulated $Q$ is treated as a posterior distribution by assuming a uniform (Dirichlet) prior $\\alpha = [a_1, \\dots, a_n]$. The prior $\\alpha$ is an additive array of uniform pseudo-counts used to address the (commonly occurring) case where $q_i = 0$, that is the model does not predict an observed state $i$.  In this experiment we tested a wide range of priors on an exponential scale, $\\alpha = 10^a, a \\in [-2, -1, 0, 1, 2]$.  \n",
    "\n",
    "The scale of the pseudo-count can be interpreted as a strength of belief in the model. Small $a$ represents strong belief that the model produces a distribution $Q$ that is representative of the \"true\" (observed posterior) distribution $P$, and for a fixed $p_i$ the effect of a decreasing $a$ on the discriminant function $D_{KL}$ yields a stronger penalty for a model that predicts an observed state with 0 probability.  Loss functions penalize overconfidence in incorrect predictions, and a prediction of 0 probability of a state which is actually observed should perhaps be thought of as confidence in an incorrect prediction and penalized as such.  A large $a$ represents weak belief that the model produces a distribution $Q$ that is representative of $P$, since $Q$ approaches the uniform distribution $\\mathbb{U}$ as $a$ increases.  \n",
    "\n",
    "Adding pseudo-counts has the effect of diluting the signal for the gradient boosting model to exploit in minimizing prediction error.  Analogously, varying the bitrate, or the size of the dictionary used to quantize continuous streamflow into discrete states, also adds quantization noise since the original streamflow signals are stored in three decimal precision and they are quantized into as few as 4 bits (16 symbol dictionary) and as many as 12 bits (4096 symbol dictionary).  The range of dictionary sizes is set to cover the expected range of rating curve uncertainty, which is generally considered multiplicative and expressed as a \\% of the observed value.\n",
    "\n",
    "As shown by the results, priors representing the addition of $10^1 \\text{ to } 10^2$ pseudo-counts diminishes the performance of the gradient boosted decision tree model, regardless of the dictionary size, or the number of possible values provided by the quantization.  Heavily penalizing unpredicted states does not have as great an impact as anticipated, perhaps as a result of the corresponding $p_i$ values also being small.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f46d97-0f45-4184-9347-3563c410ddf5",
   "metadata": {},
   "source": [
    "How do the prior and the birate affect the distribution of \"actual\" $D_{KL}$?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1541ca-c276-4055-98ad-59af4c81a67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871369af-b30d-4b73-bbc5-caac5bd93a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fdc7bcd-88cd-45b1-8c47-3a3b0a204046",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc03c6-d41f-4891-bd96-e2a6a7c6e935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
