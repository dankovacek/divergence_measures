{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec6f101-baba-407b-bbd0-da0b48b55d58",
   "metadata": {},
   "source": [
    "# Update HYSETS Catchment Attributes\n",
    "\n",
    "After checking for updated catchment polygons, the attributes associated with monitored station polygons are updated.  The majority of the preprocessing work is detailed in {cite}`kovacek2024bcub` and example notebooks are provided in the [jupyter book associated with that publication](https://dankovacek.github.io/bcub_demo/0_intro.html) detailing all of these steps. Where more recent catchment boundary information was found for monitoring stations in the preceding chapter, the updated polygons are used to revise catchment attributes.  The effect is most pronounced where the HYSETS \"artificial boundaries\" flagged catchments represented attributes with the nearest raster pixel or where the polygon was simply a square of area equal to that reported in official sources centred at the reported station location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a177c0f-f2aa-4de6-b7b0-157fc24a1f49",
   "metadata": {},
   "source": [
    "## Compare updated results with HYSETS attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307468b6-9e51-4a62-ab31-c2502e35fb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"e47f7627-1e24-477b-b121-cddb779d5f50\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"e47f7627-1e24-477b-b121-cddb779d5f50\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.5.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"e47f7627-1e24-477b-b121-cddb779d5f50\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"e47f7627-1e24-477b-b121-cddb779d5f50\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.5.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.5.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e47f7627-1e24-477b-b121-cddb779d5f50\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import Point\n",
    "from time import time\n",
    "from attribute_processing_functions import *\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ca7c9-f759-4664-a1c8-947ea056d212",
   "metadata": {},
   "source": [
    "### Load the original hysets data and the pre-processed results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16ab0d0-e47c-440e-b93b-2a552f7dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_attributes = ['prcp', 'tmin', 'tmax', 'vp', 'swe', 'srad', 'low_prcp_duration', 'low_prcp_freq', 'high_prcp_duration', 'high_prcp_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9e2c85-078c-4b19-a1b7-eb3735f1221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633/14425 HYSETS boundaries have ab flag\n"
     ]
    }
   ],
   "source": [
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "ab_flag_stns = hs_df[hs_df['Flag_Artificial_Boundaries'] == 1]['Official_ID'].values\n",
    "print(f'{len(ab_flag_stns)}/{len(hs_df)} HYSETS boundaries have ab flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36f026-9eb2-4c69-852f-d822a018d37f",
   "metadata": {},
   "source": [
    "Below we look ahead at the results of this step to compare the updated values with the HYSETS attributes.  The remainder of this chapter computes the updated catchment attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add90d08-ebac-4c68-8b21-9263d2711495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "data/BCUB_watershed_attributes_updated.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     attributes_df\u001b[38;5;241m.\u001b[39mhead()    \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_catchment_attribute_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     attributes_df \u001b[38;5;241m=\u001b[39m results_df[[c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      9\u001b[0m     attributes_df\u001b[38;5;241m.\u001b[39mto_csv(attributes_fpath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/code/data_analysis/lib/python3.10/site-packages/geopandas/io/file.py:294\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[0;32m~/Documents/code/data_analysis/lib/python3.10/site-packages/geopandas/io/file.py:547\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[0;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    544\u001b[0m     )\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/data_analysis/lib/python3.10/site-packages/pyogrio/geopandas.py:261\u001b[0m, in \u001b[0;36mread_dataframe\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_as_string\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[1;32m    281\u001b[0m     meta, table \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/Documents/code/data_analysis/lib/python3.10/site-packages/pyogrio/raw.py:196\u001b[0m, in \u001b[0;36mread\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/data_analysis/lib/python3.10/site-packages/pyogrio/_io.pyx:1239\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/code/data_analysis/lib/python3.10/site-packages/pyogrio/_io.pyx:219\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDataSourceError\u001b[0m: data/BCUB_watershed_attributes_updated.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "updated_catchment_attribute_path = os.path.join('data/BCUB_watershed_attributes_updated.geojson')\n",
    "attributes_fpath = updated_catchment_attribute_path.replace('.geojson', '.csv')\n",
    "if os.path.exists(attributes_fpath):\n",
    "    attributes_df = pd.read_csv(attributes_fpath)\n",
    "    attributes_df.head()    \n",
    "else:\n",
    "    results_df = gpd.read_file(updated_catchment_attribute_path)\n",
    "    attributes_df = results_df[[c for c in results_df.columns if c != 'geometry']].copy()\n",
    "    attributes_df.to_csv(attributes_fpath, index=False)\n",
    "\n",
    "attributes_df.set_index('Official_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c45dc-e4d8-44ad-b255-c9683d7f6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the unaltered hysets attributes for stations in the results dataframe\n",
    "# og_df = hs_df[hs_df['Official_ID'].#.isin(attributes_df.index)].copy()\n",
    "# og_df.set_index('Official_ID', inplace=True)\n",
    "# # the soil permeability and porosity column names need to be updated\n",
    "# og_df.rename({'Permeability_logk_m2': 'logk_ice_x100', 'Porosity_frac': 'porosity_x100'}, axis=1, inplace=True)\n",
    "# og_df.columns\n",
    "# len(og_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc98a086-2da0-4b80-b801-db0e9273a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the attributes of interest (climate are not included in original as such)\n",
    "attributes = [\n",
    "    'logk_ice_x100', 'porosity_x100',\n",
    "    'Slope_deg', 'Aspect_deg', 'Elevation_m', 'Drainage_Area_km2', \n",
    "    'Land_Use_Forest_frac', 'Land_Use_Shrubs_frac', 'Land_Use_Grass_frac',\n",
    "    'Land_Use_Wetland_frac', 'Land_Use_Crops_frac', 'Land_Use_Urban_frac',\n",
    "    'Land_Use_Water_frac', 'Land_Use_Snow_Ice_frac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ae59a3-1e26-4b79-a283-4d32166c817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df, a, ab_flag_stns):\n",
    "    min_val, max_val = df.min().min(), df.max().max()\n",
    "    # Create a new plot with a title and axis labels\n",
    "    p = figure(title=a)\n",
    "    if a.lower() == 'drainage_area_km2':\n",
    "        p = figure(title=a, x_axis_type='log', y_axis_type='log')\n",
    "        \n",
    "    df['stn_id'] = df.index  # Make sure the index column is available for tooltips\n",
    "    df['ab_flag'] = [True if e in ab_flag_stns else False for e in df.index]\n",
    "    flag_df = df[df['ab_flag'] == True].copy()\n",
    "    noflag_df = df[df['ab_flag'] == False].copy()\n",
    "    flag_source = ColumnDataSource(flag_df)\n",
    "    noflag_source = ColumnDataSource(noflag_df)\n",
    "    # Add a scatter renderer with circle markers\n",
    "    p.scatter(\n",
    "        x='original', y='revised', size=3, color=\"dodgerblue\", alpha=0.6, source=noflag_source,\n",
    "        legend_label='no_flag'\n",
    "    )\n",
    "    p.scatter(\n",
    "        x='original', y='revised', size=3, color=\"orange\", alpha=0.6, source=flag_source,\n",
    "        legend_label='ab_flag'\n",
    "    )\n",
    "\n",
    "    # Add a HoverTool to show the index\n",
    "    hover = HoverTool()\n",
    "    hover.tooltips = [\n",
    "        (\"ID\", \"@stn_id\"),\n",
    "    ]\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    \n",
    "    x = np.linspace(min_val, max_val, 1000)\n",
    "    y = x\n",
    "    p.line(x, y, legend_label='1:1', color='red', line_width=3, line_dash='dashed')\n",
    "    \n",
    "    # Set axis labels\n",
    "    p.xaxis.axis_label = 'original'\n",
    "    p.yaxis.axis_label = 'updated'\n",
    "    p.legend.click_policy = 'hide'\n",
    "    p.legend.location = 'top_left'\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca62b81d-b4d9-43ec-8e6c-bddfdb2d4b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'og_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLand_Use\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     result_a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2010\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m og_vals \u001b[38;5;241m=\u001b[39m \u001b[43mog_df\u001b[49m[[a]]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mrename({a: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m}, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m revised_vals \u001b[38;5;241m=\u001b[39m attributes_df[[result_a]]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mrename({result_a: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevised\u001b[39m\u001b[38;5;124m'\u001b[39m}, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m comp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([og_vals, revised_vals], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'og_df' is not defined"
     ]
    }
   ],
   "source": [
    "plots = []\n",
    "for a in attributes:\n",
    "    result_a = a    \n",
    "    if a.startswith('Land_Use'):\n",
    "        result_a += '_2010'\n",
    "    og_vals = og_df[[a]].copy().rename({a: 'original'}, axis=1)\n",
    "    \n",
    "    revised_vals = attributes_df[[result_a]].copy().rename({result_a: 'revised'}, axis=1)\n",
    "    comp_df = pd.concat([og_vals, revised_vals], axis=1)\n",
    "    comp_df.dropna(inplace=True, how='any')\n",
    "\n",
    "    if a in ['logk_ice_x100', 'porosity_x100']:\n",
    "        comp_df['revised'] /=  100\n",
    "    plot = scatter_plot(comp_df, a, ab_flag_stns)\n",
    "    plots.append(plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53277b89-c829-469a-9205-4dd4f5b8d037",
   "metadata": {},
   "source": [
    "### View scatter plots of HYSETS vs. updated attributes\n",
    "\n",
    "Approximately 25% of the stations we evaluated had an \"artificial bounds\" flag, meaning that catchment geometries were not available from official sources.  These catchment boundaries were approximated by a square centred at the \"centroid\" coordinates which were stated in the HYSET paper to reflect the reported station location.  Below we see the attributes based on updated values are quite different, in particular those which were updated from revised official sources (ab_flag).\n",
    "\n",
    "The soil attributes describe a marked difference between studies.  This may be because this study uses the GLHYMPS 2.0 version {cite}`huscroft2018compiling` [DOI: https://doi.org/10.1002/2017GL075860](https://doi.org/10.1002/2017GL075860).  The source used in HYSETS (https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP2/DLGXYO) is {cite}'gleeson2018' [DOI: https://doi.org/10.5683/SP2/DLGXYO](https://doi.org/10.5683/SP2/DLGXYO)\n",
    "\n",
    "\n",
    "```{note}\n",
    "ab_flag = Artificial boundaries \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb50734-8878-4403-a1ed-e0b79774360a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"eb22b1ee-678d-4e3b-90e3-131d75901441\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"75170337-c320-41cc-95bd-705a55bfefec\":{\"version\":\"3.5.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"p1002\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1001\"}}}]}};\n",
       "  const render_items = [{\"docid\":\"75170337-c320-41cc-95bd-705a55bfefec\",\"roots\":{\"p1002\":\"eb22b1ee-678d-4e3b-90e3-131d75901441\"},\"root_ids\":[\"p1002\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layout = gridplot(plots, ncols=3, width=350, height=325)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15319a-251b-4963-8a52-c40154aae3e3",
   "metadata": {},
   "source": [
    "## Load updated catchment polygons\n",
    "\n",
    "The data processing below is optional if you use the pre-processed (revised) attributes `BCUB_watershed_attributes_updated.csv`\n",
    "\n",
    "The file `BCUB_watershed_bounds_updated.geojson` is the end result of the preceding chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7f68b1-3290-46c3-9276-36b6f69331f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n"
     ]
    }
   ],
   "source": [
    "revised_catchment_geometry_fpath = 'data/BCUB_watershed_bounds_updated.geojson'\n",
    "bcub_gdf = gpd.read_file(revised_catchment_geometry_fpath)\n",
    "geom_updated_stns = bcub_gdf[bcub_gdf['geometry_updated'] == 1]['Official_ID'].values\n",
    "print(len(geom_updated_stns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa56ba6-452e-40d1-8f09-ef64209bd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_pts = bcub_gdf.copy()\n",
    "bcub_pts['geometry'] = bcub_pts.apply(lambda row: Point(row['Centroid_Lon_deg_E'], row['Centroid_Lat_deg_N']), axis=1)\n",
    "# we are overwriting the polygon geometry which is 3005\n",
    "bcub_pts = bcub_pts.set_crs(4326, allow_override=True)\n",
    "bcub_pts[['geometry']].head()\n",
    "if 'index_right' in bcub_pts.columns:\n",
    "    bcub_pts.drop('index_right', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2dd054-5fbf-4267-a090-b4a61351bf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Watershed_ID', 'Source', 'Name', 'Official_ID', 'Centroid_Lat_deg_N',\n",
       "       'Centroid_Lon_deg_E', 'Drainage_Area_km2', 'Drainage_Area_GSIM_km2',\n",
       "       'Flag_GSIM_boundaries', 'Flag_Artificial_Boundaries', 'Elevation_m',\n",
       "       'Slope_deg', 'Gravelius', 'Perimeter', 'Flag_Shape_Extraction',\n",
       "       'Aspect_deg', 'Flag_Terrain_Extraction', 'Land_Use_Forest_frac',\n",
       "       'Land_Use_Grass_frac', 'Land_Use_Wetland_frac', 'Land_Use_Water_frac',\n",
       "       'Land_Use_Urban_frac', 'Land_Use_Shrubs_frac', 'Land_Use_Crops_frac',\n",
       "       'Land_Use_Snow_Ice_frac', 'Flag_Land_Use_Extraction',\n",
       "       'Permeability_logk_m2', 'Porosity_frac', 'Flag_Subsoil_Extraction',\n",
       "       'region_code', 'geometry_updated', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcub_pts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61ff8fc-df02-4c58-8739-a3f75d18ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize the stations by the region they are contained in to reduce the number of raster loads.  \n",
    "# import the BCUB (study) region boundary\n",
    "# region_gdf = gpd.read_file('data/BCUB_regions_4326.geojson')\n",
    "# region_gdf = region_gdf.to_crs(3005)\n",
    "# # simplify the geometries (100m threshold) and add a small buffer (250m) to capture HYSETS station points recorded with low accuracy near boundaries\n",
    "# region_gdf.geometry = region_gdf.simplify(100).buffer(500)\n",
    "# region_gdf = region_gdf.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92528d9-2379-48ba-8cc3-a472a4688db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # organize the stations by their containing study sub-region polygon\n",
    "# assert region_gdf.crs == bcub_pts.crs\n",
    "\n",
    "# for i, row in region_gdf.iterrows():\n",
    "#     rc = row['region_code']\n",
    "    \n",
    "#     region_polygon = region_gdf.loc[[i]].copy()\n",
    "#     region_polygon.to_crs(3005, inplace=True)\n",
    "#     region_polygon.geometry = region_polygon.simplify(100).buffer(400)\n",
    "#     region_polygon.to_crs(4326, inplace=True)\n",
    "#     contained = gpd.sjoin(bcub_pts, region_polygon, how='inner', predicate='intersects')\n",
    "#     if contained.empty:\n",
    "#         continue\n",
    "    \n",
    "#     bcub_pts.loc[bcub_pts['Official_ID'].isin(contained['Official_ID'].values), 'region_code'] = rc\n",
    "#     bcub_df.loc[bcub_df['Official_ID'].isin(contained['Official_ID'].values), 'region_code'] = rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c4654f-384f-4909-84fa-b31c6036d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609 1609\n"
     ]
    }
   ],
   "source": [
    "# update the two added stations 08AG003 (YKR), 10ED002 (LRD)\n",
    "bcub_pts.loc[bcub_pts['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_pts.loc[bcub_pts['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "# assert len(bcub_pts[bcub_pts['region_code'] == None]) == 0\n",
    "\n",
    "region_codes = sorted(list(set(bcub_pts['region_code'])))\n",
    "print(len(bcub_pts), len(bcub_gdf))\n",
    "# make sure all rows have an associated region_code\n",
    "assert len([e for e in region_codes if e is None]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccc13c-6082-4e98-844c-003fbeb664c7",
   "metadata": {},
   "source": [
    "### Extract terrain, climate, land cover, and soil attributes\n",
    "\n",
    "Terrain attributes are extracted from 1-arc-second DEM available at the USGS [National Map Downloader](https://apps.nationalmap.gov/downloader/#/).\n",
    "\n",
    "In the GLHYMPS dataset, the attributes are truncated (.shp truncates at 10 symbols):\n",
    "* porosity: `Porosity_x`,\n",
    "* permeability: `logK_Ice_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1643d2f3-46ef-4a17-bff1-69aac4d004c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcub_data_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub'\n",
    "dem_folder = '/home/danbot/Documents/code/23/bcub/processed_data/processed_dem/'\n",
    "local_data_folder = 'data/geospatial_layers/'\n",
    "glhymps_path = os.path.join(local_data_folder, 'glhymps/GLHYMPS_clipped_3005.geojson')\n",
    "nalcms_folder = os.path.join(local_data_folder, 'nalcms')\n",
    "daymet_folder = os.path.join(local_data_folder, 'daymet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e895232-cbc6-4253-91ae-c860043a7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nalcms_dict = {}\n",
    "for y in [2010, 2015, 2020]:\n",
    "    nalcms_fpath = os.path.join(nalcms_folder, f'NA_NALCMS_landcover_{y}_3005_clipped.tif')\n",
    "    nalcms_dict[y] = rxr.open_rasterio(nalcms_fpath, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24b1db69-7ff5-4533-9ceb-6536cb3ef9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_dict = {}\n",
    "for c in daymet_attributes:\n",
    "    fpath = os.path.join(daymet_folder, f'{c}_mosaic_3005.tiff')\n",
    "    climate_dict[c] = rxr.open_rasterio(fpath, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9418592-0745-42c5-94bf-ed1b8e7700c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps_data = gpd.read_file(glhymps_path)\n",
    "glhymps_data.geometry = glhymps_data.geometry.make_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd06500-7445-425b-8d52-0ff9feac0e4f",
   "metadata": {},
   "source": [
    "### Re-process catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44703394-a7d7-4bc4-bc9e-345840d3a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "def get_merged_dem(basin_geom):\n",
    "    r1_path = os.path.join(dem_folder, f'10E_USGS_3DEP_3005.tif')\n",
    "    r2_path = os.path.join(dem_folder, f'LRD_USGS_3DEP_3005.tif')\n",
    "    r1_dem, dem_crs, dem_affine = retrieve_raster(r1_path)\n",
    "    r2_dem, dem_crs, dem_affine = retrieve_raster(r2_path)\n",
    "\n",
    "    merged_raster = merge_arrays([r1_dem, r2_dem])\n",
    "    masked_raster = merged_raster.rio.clip(basin_geom.geometry, merged_raster.rio.crs)\n",
    "    \n",
    "    return masked_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f767a7fd-d162-4b47-989c-c24f8eef3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_catchment_attributes(rc, row, region_dem, crs):\n",
    "\n",
    "    stn_id = row['Official_ID']\n",
    "    \n",
    "    t0 = time.time()\n",
    "    basin_data = {}\n",
    "    basin_data['region'] = rc\n",
    "    basin_data['Official_ID'] = stn_id\n",
    "    basin_data['geometry'] = row['geometry']\n",
    "    basin_data['Drainage_Area_km2'] = round(row['geometry'].area / 1e6, 1)\n",
    "    basin_data['Centroid_Lon_deg_E'] = row['Centroid_Lon_deg_E']\n",
    "    basin_data['Centroid_Lat_deg_N'] = row['Centroid_Lat_deg_N']\n",
    "        \n",
    "    basin_polygon = gpd.GeoDataFrame(geometry=[row['geometry']], crs=crs)  \n",
    "    basin_polygon.geometry = basin_polygon.geometry.buffer(0)\n",
    "    \n",
    "    if not basin_polygon.is_valid.all():\n",
    "        basin_polygon.geometry = basin_polygon.geometry.make_valid()\n",
    "        if not basin_polygon.is_valid.all():\n",
    "            raise Exception('arg')\n",
    "        else:\n",
    "            print(f'Fixed invalid basin polygon geometry for {stn_id}.')\n",
    "\n",
    "    # process soil attributes\n",
    "    soil_masked = gpd.clip(glhymps_data, mask=basin_polygon)\n",
    "    soil_masked = soil_masked[soil_masked.geometry.area > 1.0]   \n",
    "    soil_masked.geometry = soil_masked.geometry.buffer(0)\n",
    "    soil_masked.geometry = soil_masked.geometry.make_valid()\n",
    "    \n",
    "    assert all(soil_masked.is_valid)    \n",
    "    porosity = get_soil_properties(soil_masked, 'Porosity_x')\n",
    "    permeability = get_soil_properties(soil_masked, 'logK_Ice_x')\n",
    "    basin_data['logk_ice_x100'] = round(permeability, 2)\n",
    "    basin_data['porosity_x100'] = round(porosity, 5)\n",
    "    del soil_masked\n",
    "    \n",
    "    # process NALCMS land cover\n",
    "    for y in [2010, 2015, 2020]:\n",
    "        # nalcms_fpath = os.path.join(nalcms_folder, f'NA_NALCMS_landcover_{y}_3005_clipped.tif')\n",
    "        # clipped_land_cover = rxr.open_rasterio(nalcms_fpath, masked=True).rio.clip(basin_polygon.geometry, all_touched=True)\n",
    "        clip_ok, clipped_nalcms = clip_raster_to_basin(basin_polygon, nalcms_dict[y])\n",
    "        land_cover = process_lulc(i, basin_polygon, clipped_nalcms, y)\n",
    "        land_cover = land_cover.to_dict('records')[0]\n",
    "        basin_data.update(land_cover)\n",
    "\n",
    "    # process terrain\n",
    "    # make a special case for 10ED002 where we need to load and merge\n",
    "    # the rasters for LRD and 10E and merge\n",
    "    del clipped_nalcms\n",
    "    if stn_id == '10ED002':\n",
    "        print(f'processing special case: {stn_id}')\n",
    "        clipped_dem = get_merged_dem(basin_polygon)\n",
    "    else:\n",
    "        dem_fpath = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005.tif')\n",
    "        assert os.path.exists(dem_fpath)\n",
    "        clip_ok, clipped_dem = clip_raster_to_basin(basin_polygon, region_dem)\n",
    "\n",
    "        slope, aspect = calculate_slope_and_aspect(clipped_dem)\n",
    "        # print(f'aspect, slope: {aspect:.1f} {slope:.2f} ')\n",
    "        basin_data['Slope_deg'] = slope\n",
    "        basin_data['Aspect_deg'] = aspect\n",
    "    \n",
    "        mean_el, median_el, min_el, max_el = process_basin_elevation(clipped_dem)\n",
    "        basin_data['median_el'] = median_el\n",
    "        basin_data['mean_el'] = mean_el\n",
    "        basin_data['max_el'] = max_el\n",
    "        basin_data['min_el'] = min_el\n",
    "        basin_data['Elevation_m'] = mean_el\n",
    "\n",
    "    # process climate params\n",
    "    del clipped_dem\n",
    "    for climate_param in daymet_attributes:\n",
    "        clip_ok, clipped_data = clip_raster_to_basin(basin_polygon, climate_dict[climate_param])\n",
    "        # Check if the clipped raster is empty or has no data\n",
    "        if clipped_data is None:\n",
    "            print(f'clip is empty, finding nearest point from polygon centroid')\n",
    "            # If the clipped raster is empty or contains only NaN, find the nearest value\n",
    "            spatial_mean = find_nearest_raster_value(climate_dict[climate_param], basin_polygon)            \n",
    "        else:\n",
    "            spatial_mean = round(clipped_data.mean(dim=['y', 'x']).item(), 1)\n",
    "            \n",
    "        basin_data[climate_param] = spatial_mean\n",
    "            # basin_polygon.to_file(f'{stn_id}_error.geojson')\n",
    "            # raise Exception(f'issue with {climate_param}')\n",
    "        \n",
    "    return basin_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7210fb-25c0-4e00-8a0e-115f8518aa89",
   "metadata": {},
   "source": [
    "### Load the updated catchment geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d63c43f0-72eb-4c09-8132-059692159ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revisit the preceding chapter to generate revised catchment geometries.\n"
     ]
    }
   ],
   "source": [
    "all_basin_data = []\n",
    "t0 = time.time()\n",
    "\n",
    "results_df, processed_ids = pd.DataFrame(), []\n",
    "if os.path.exists(updated_catchment_attribute_path):\n",
    "    print(f'{updated_catchment_attribute_path.split(\"/\")[-1]} exists, loading existing file.')\n",
    "    results_df = gpd.read_file(updated_catchment_attribute_fpath)\n",
    "    print(f'{len(results_df)} existing results loaded')\n",
    "    processed_ids = results_df['Official_ID'].values.tolist()\n",
    "else:\n",
    "    print('Revisit the preceding chapter to generate revised catchment geometries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c016a-af27-4b52-a9df-4226c85eb70e",
   "metadata": {},
   "source": [
    "### Process monitored catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823c516-155b-4e48-b7f8-bb43cc23d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 08A region catchments\n",
      "Processing 08B region catchments\n",
      "Processing 08C region catchments\n",
      "Processing 08D region catchments\n",
      "Processing 08E region catchments\n",
      "Processing 08F region catchments\n",
      "Processing 08G region catchments\n",
      "Processing 10E region catchments\n",
      "     ...saving output file.\n",
      "    ...saved 400 results file (200 unique station ids).\n",
      "processing special case: 10ED002\n"
     ]
    }
   ],
   "source": [
    "batch_results = []\n",
    "for rc in region_codes:\n",
    "    print(f'Processing {rc} region catchments')\n",
    "    batch_df = bcub_gdf[bcub_gdf['region_code'] == rc].copy()\n",
    "    # batch_df = batch_df[~batch_df['Official_ID'].isin(processed_ids)].copy()\n",
    "    dem_fpath = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005.tif')\n",
    "    assert os.path.exists(dem_fpath)\n",
    "    region_dem = rxr.open_rasterio(dem_fpath, mask_and_scale=True)\n",
    "    for i, row in batch_df.iterrows():\n",
    "        stn_id = row['Official_ID']\n",
    "\n",
    "        result = process_catchment_attributes(rc, row, region_dem, bcub_gdf.crs)\n",
    "        \n",
    "        batch_results.append(result)\n",
    "        processed_ids.append(stn_id)\n",
    "        if (len(batch_results) % 200 == 0) | (len(processed_ids) >= len(bcub_gdf) - 1):\n",
    "            new_results = gpd.GeoDataFrame(batch_results, crs='EPSG:3005')\n",
    "            results_df = gpd.GeoDataFrame(pd.concat([results_df, new_results]), crs='EPSG:3005')    \n",
    "            batch_results = []\n",
    "            print('     ...saving output file.')\n",
    "            results_df.to_file(updated_catchment_attribute_path, index=False)\n",
    "            n_unique = len(list(set(results_df['Official_ID'])))\n",
    "            print(f'    ...saved {len(results_df)} results file ({n_unique} unique station ids).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c61f5c-5df5-41f5-908b-b81fac43ea1f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71186c1a-6368-4978-a0a3-48321811552a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
