{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec6f101-baba-407b-bbd0-da0b48b55d58",
   "metadata": {},
   "source": [
    "# Update HYSETS Catchment Attributes\n",
    "\n",
    "After checking for updated catchment polygons, the attributes associated with monitored station polygons are updated.  The majority of the preprocessing work is detailed in {cite}`kovacek2024bcub` and example notebooks are provided in the [jupyter book associated with that publication](https://dankovacek.github.io/bcub_demo/0_intro.html) detailing all of these steps. Where more recent catchment boundary information was found for monitoring stations in the preceding chapter, the updated polygons are used to revise catchment attributes.  The effect is most pronounced where the HYSETS \"artificial boundaries\" flagged catchments represented attributes with the nearest raster pixel or where the polygon was simply a square of area equal to that reported in official sources centred at the reported station location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a177c0f-f2aa-4de6-b7b0-157fc24a1f49",
   "metadata": {},
   "source": [
    "## Compare updated results with HYSETS attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307468b6-9e51-4a62-ab31-c2502e35fb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"faf7bf87-b3f5-491d-9880-d2ad3a3789e2\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"faf7bf87-b3f5-491d-9880-d2ad3a3789e2\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"faf7bf87-b3f5-491d-9880-d2ad3a3789e2\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"faf7bf87-b3f5-491d-9880-d2ad3a3789e2\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"faf7bf87-b3f5-491d-9880-d2ad3a3789e2\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import Point\n",
    "from time import time\n",
    "from attribute_processing_functions import *\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ca7c9-f759-4664-a1c8-947ea056d212",
   "metadata": {},
   "source": [
    "### Load the original hysets data and the pre-processed results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16ab0d0-e47c-440e-b93b-2a552f7dcdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_attributes = ['prcp', 'tmin', 'tmax', 'vp', 'swe', 'srad', 'low_prcp_duration', 'low_prcp_freq', 'high_prcp_duration', 'high_prcp_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9e2c85-078c-4b19-a1b7-eb3735f1221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1633/14425 HYSETS boundaries have ab flag\n"
     ]
    }
   ],
   "source": [
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "ab_flag_stns = hs_df[hs_df['Flag_Artificial_Boundaries'] == 1]['Official_ID'].values\n",
    "print(f'{len(ab_flag_stns)}/{len(hs_df)} HYSETS boundaries have ab flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36f026-9eb2-4c69-852f-d822a018d37f",
   "metadata": {},
   "source": [
    "Look ahead at the results of this chapter to compare the updated values with the HYSETS attributes.  The remainder of this chapter following the plots below computes the updated catchment attributes that we see compared here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd314d8-851f-492d-92fc-7b3740df1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_catchment_attribute_path = os.path.join(os.getcwd(), 'data/BCUB_watershed_attributes_updated.geojson')\n",
    "attributes_fpath = updated_catchment_attribute_path.replace('.geojson', '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add90d08-ebac-4c68-8b21-9263d2711495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/BCUB_watershed_attributes_updated.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     attributes_df\u001b[38;5;241m.\u001b[39mhead()    \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_catchment_attribute_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     attributes_df \u001b[38;5;241m=\u001b[39m results_df[[c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m results_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m     attributes_df\u001b[38;5;241m.\u001b[39mto_csv(attributes_fpath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/geopandas/io/file.py:294\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/geopandas/io/file.py:547\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[0;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    544\u001b[0m     )\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/pyogrio/geopandas.py:239\u001b[0m, in \u001b[0;36mread_dataframe\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_as_string\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[1;32m    259\u001b[0m     meta, table \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/pyogrio/raw.py:194\u001b[0m, in \u001b[0;36mread\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/pyogrio/_io.pyx:1124\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/pyogrio/_io.pyx:171\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDataSourceError\u001b[0m: /home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/BCUB_watershed_attributes_updated.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "if os.path.exists(attributes_fpath):\n",
    "    attributes_df = pd.read_csv(attributes_fpath)\n",
    "    attributes_df.head()    \n",
    "else:\n",
    "    results_df = gpd.read_file(updated_catchment_attribute_path)\n",
    "    attributes_df = results_df[[c for c in results_df.columns if c != 'geometry']].copy()\n",
    "    attributes_df.to_csv(attributes_fpath, index=False)\n",
    "\n",
    "attributes_df.set_index('official_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2c45dc-e4d8-44ad-b255-c9683d7f6862",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# filter the unaltered hysets attributes for stations in the results dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m og_df \u001b[38;5;241m=\u001b[39m hs_df[hs_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOfficial_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43mattributes_df\u001b[49m\u001b[38;5;241m.\u001b[39mindex)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      3\u001b[0m og_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOfficial_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# the soil permeability and porosity column names need to be updated\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes_df' is not defined"
     ]
    }
   ],
   "source": [
    "# filter the unaltered hysets attributes for stations in the results dataframe\n",
    "og_df = hs_df[hs_df['Official_ID'].isin(attributes_df.index)].copy()\n",
    "og_df.set_index('Official_ID', inplace=True)\n",
    "# the soil permeability and porosity column names need to be updated\n",
    "og_df.rename({'Permeability_logk_m2': 'logk_ice_x100', 'Porosity_frac': 'porosity_x100'}, axis=1, inplace=True)\n",
    "len(og_df)\n",
    "og_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc98a086-2da0-4b80-b801-db0e9273a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the attributes of interest (climate are not included in original as such)\n",
    "attributes = [\n",
    "    'logk_ice_x100', 'porosity_x100',\n",
    "    'Slope_deg', 'Aspect_deg', 'Elevation_m', 'Drainage_Area_km2', \n",
    "    'Land_Use_Forest_frac', 'Land_Use_Shrubs_frac', 'Land_Use_Grass_frac',\n",
    "    'Land_Use_Wetland_frac', 'Land_Use_Crops_frac', 'Land_Use_Urban_frac',\n",
    "    'Land_Use_Water_frac', 'Land_Use_Snow_Ice_frac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ae59a3-1e26-4b79-a283-4d32166c817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df, a, ab_flag_stns):\n",
    "    min_val, max_val = df.min().min(), df.max().max()\n",
    "    # Create a new plot with a title and axis labels\n",
    "    p = figure(title=a)\n",
    "\n",
    "    if a.lower() == 'drainage_area_km2':\n",
    "        p = figure(title=a, x_axis_type='log', y_axis_type='log')\n",
    "        \n",
    "    df['stn_id'] = df.index  # Make sure the index column is available for tooltips\n",
    "    df['ab_flag'] = [True if e in ab_flag_stns else False for e in df.index]\n",
    "    flag_df = df[df['ab_flag'] == True].copy()\n",
    "    noflag_df = df[df['ab_flag'] == False].copy()\n",
    "\n",
    "    flag_source = ColumnDataSource(flag_df)\n",
    "    noflag_source = ColumnDataSource(noflag_df)\n",
    "    # Add a scatter renderer with circle markers\n",
    "    p.scatter(\n",
    "        x='original', y='revised', size=3, color=\"dodgerblue\", alpha=0.6, source=noflag_source,\n",
    "        legend_label='no_flag'\n",
    "    )\n",
    "    p.scatter(\n",
    "        x='original', y='revised', size=3, color=\"orange\", alpha=0.6, source=flag_source,\n",
    "        legend_label='ab_flag'\n",
    "    )\n",
    "\n",
    "    # Add a HoverTool to show the index\n",
    "    hover = HoverTool()\n",
    "    hover.tooltips = [\n",
    "        (\"ID\", \"@stn_id\"),\n",
    "    ]\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    \n",
    "    x = np.linspace(min_val, max_val, 1000)\n",
    "    y = x\n",
    "    p.line(x, y, legend_label='1:1', color='red', line_width=3, line_dash='dashed')\n",
    "    \n",
    "    # Set axis labels\n",
    "    p.xaxis.axis_label = 'original'\n",
    "    p.yaxis.axis_label = 'updated'\n",
    "    p.legend.click_policy = 'hide'\n",
    "    p.legend.location = 'top_left'\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53277b89-c829-469a-9205-4dd4f5b8d037",
   "metadata": {},
   "source": [
    "### View scatter plots of HYSETS vs. updated attributes\n",
    "\n",
    "Approximately 25% of the stations we evaluated had an \"artificial bounds\" flag, meaning that catchment geometries were not available from official sources.  These catchment boundaries were approximated by a square centred at the \"centroid\" coordinates which were stated in the HYSET paper to reflect the reported station location.  Below we see the attributes based on updated values are quite different, in particular those which were updated from revised official sources (ab_flag).\n",
    "\n",
    "The soil attributes describe a marked difference between studies.  This may be because this study uses the GLHYMPS 2.0 version {cite}`huscroft2018compiling` [DOI: https://doi.org/10.1002/2017GL075860](https://doi.org/10.1002/2017GL075860).  The source used in HYSETS (https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP2/DLGXYO) is {cite}'gleeson2018' [DOI: https://doi.org/10.5683/SP2/DLGXYO](https://doi.org/10.5683/SP2/DLGXYO)\n",
    "\n",
    "\n",
    "```{note}\n",
    "ab_flag = Artificial boundaries \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca62b81d-b4d9-43ec-8e6c-bddfdb2d4b31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'og_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLand_Use\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     result_a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2010\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m og_vals \u001b[38;5;241m=\u001b[39m \u001b[43mog_df\u001b[49m[[a]]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mrename({a: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m}, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)    \n\u001b[1;32m      7\u001b[0m revised_vals \u001b[38;5;241m=\u001b[39m attributes_df[[result_a\u001b[38;5;241m.\u001b[39mlower()]]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mrename({result_a\u001b[38;5;241m.\u001b[39mlower(): \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevised\u001b[39m\u001b[38;5;124m'\u001b[39m}, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m comp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([og_vals, revised_vals], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'og_df' is not defined"
     ]
    }
   ],
   "source": [
    "plots = []\n",
    "for a in attributes:\n",
    "    result_a = a    \n",
    "    if a.startswith('Land_Use'):\n",
    "        result_a += '_2010'\n",
    "    og_vals = og_df[[a]].copy().rename({a: 'original'}, axis=1)    \n",
    "    revised_vals = attributes_df[[result_a.lower()]].copy().rename({result_a.lower(): 'revised'}, axis=1)\n",
    "    comp_df = pd.concat([og_vals, revised_vals], axis=1)\n",
    "    comp_df.dropna(inplace=True, how='any')\n",
    "    if a in ['logk_ice_x100', 'porosity_x100']:\n",
    "        comp_df['revised'] /=  100\n",
    "\n",
    "    plot = scatter_plot(comp_df, a, ab_flag_stns)\n",
    "    plots.append(plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb50734-8878-4403-a1ed-e0b79774360a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"d9cf2836-aee1-4f34-8c90-93c3c2dd9dd3\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"4606b902-df06-4cd9-8b21-7d778ab12b09\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"p1002\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1001\"}}}]}};\n",
       "  const render_items = [{\"docid\":\"4606b902-df06-4cd9-8b21-7d778ab12b09\",\"roots\":{\"p1002\":\"d9cf2836-aee1-4f34-8c90-93c3c2dd9dd3\"},\"root_ids\":[\"p1002\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layout = gridplot(plots, ncols=3, width=350, height=325)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15319a-251b-4963-8a52-c40154aae3e3",
   "metadata": {},
   "source": [
    "## Load updated catchment polygons\n",
    "\n",
    "The data processing below is optional if you use the pre-processed (revised) attributes `BCUB_watershed_attributes_updated.csv`\n",
    "\n",
    "The file `BCUB_watershed_bounds_updated.geojson` is the end result of the preceding chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7f68b1-3290-46c3-9276-36b6f69331f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n"
     ]
    }
   ],
   "source": [
    "revised_catchment_geometry_fpath = 'data/BCUB_watershed_bounds_updated.geojson'\n",
    "if os.path.exists(revised_catchment_geometry_fpath):\n",
    "    bcub_gdf = gpd.read_file(revised_catchment_geometry_fpath)\n",
    "    geom_updated_stns = bcub_gdf[bcub_gdf['geometry_updated'] == 1]['Official_ID'].values\n",
    "    print(len(geom_updated_stns))\n",
    "else:\n",
    "    print('Revisit the preceding chapter to generate revised catchment geometries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa56ba6-452e-40d1-8f09-ef64209bd461",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_pts = bcub_gdf.copy()\n",
    "bcub_pts['geometry'] = bcub_pts.apply(lambda row: Point(row['Centroid_Lon_deg_E'], row['Centroid_Lat_deg_N']), axis=1)\n",
    "# we are overwriting the polygon geometry which is 3005\n",
    "bcub_pts = bcub_pts.set_crs(4326, allow_override=True)\n",
    "bcub_pts[['geometry']].head()\n",
    "if 'index_right' in bcub_pts.columns:\n",
    "    bcub_pts.drop('index_right', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2dd054-5fbf-4267-a090-b4a61351bf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Watershed_ID', 'Source', 'Name', 'Official_ID', 'Centroid_Lat_deg_N',\n",
       "       'Centroid_Lon_deg_E', 'Drainage_Area_km2', 'Drainage_Area_GSIM_km2',\n",
       "       'Flag_GSIM_boundaries', 'Flag_Artificial_Boundaries', 'Elevation_m',\n",
       "       'Slope_deg', 'Gravelius', 'Perimeter', 'Flag_Shape_Extraction',\n",
       "       'Aspect_deg', 'Flag_Terrain_Extraction', 'Land_Use_Forest_frac',\n",
       "       'Land_Use_Grass_frac', 'Land_Use_Wetland_frac', 'Land_Use_Water_frac',\n",
       "       'Land_Use_Urban_frac', 'Land_Use_Shrubs_frac', 'Land_Use_Crops_frac',\n",
       "       'Land_Use_Snow_Ice_frac', 'Flag_Land_Use_Extraction',\n",
       "       'Permeability_logk_m2', 'Porosity_frac', 'Flag_Subsoil_Extraction',\n",
       "       'region_code', 'geometry_updated', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcub_pts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c4654f-384f-4909-84fa-b31c6036d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609 1609\n"
     ]
    }
   ],
   "source": [
    "# update the two added stations 08AG003 (YKR), 10ED002 (LRD)\n",
    "bcub_pts.loc[bcub_pts['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_pts.loc[bcub_pts['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "# assert len(bcub_pts[bcub_pts['region_code'] == None]) == 0\n",
    "\n",
    "region_codes = sorted(list(set(bcub_pts['region_code'])))\n",
    "print(len(bcub_pts), len(bcub_gdf))\n",
    "# make sure all rows have an associated region_code\n",
    "assert len([e for e in region_codes if e is None]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccc13c-6082-4e98-844c-003fbeb664c7",
   "metadata": {},
   "source": [
    "### Extract terrain, climate, land cover, and soil attributes\n",
    "\n",
    "Terrain attributes are extracted from 1-arc-second DEM available at the USGS [National Map Downloader](https://apps.nationalmap.gov/downloader/#/).\n",
    "\n",
    "In the GLHYMPS dataset, the attributes are truncated (.shp truncates at 10 symbols):\n",
    "* porosity: `Porosity_x`,\n",
    "* permeability: `logK_Ice_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1643d2f3-46ef-4a17-bff1-69aac4d004c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcub_data_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub'\n",
    "dem_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub/processed_data/processed_dem/'\n",
    "# dem_folder = '/home/danbot/Documents/code/23/bcub/processed_data/processed_dem/'\n",
    "local_data_folder = 'data/geospatial_layers/'\n",
    "glhymps_folder = os.path.join(os.getcwd(), local_data_folder, 'glhymps')\n",
    "nalcms_folder = os.path.join(os.getcwd(), local_data_folder, 'nalcms')\n",
    "daymet_folder = os.path.join(os.getcwd(), local_data_folder, 'daymet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e895232-cbc6-4253-91ae-c860043a7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nalcms_dict = {}\n",
    "for y in [2010, 2015, 2020]:\n",
    "    nalcms_fpath = os.path.join(nalcms_folder, f'NA_NALCMS_landcover_{y}_3005_clipped.tif')\n",
    "    nalcms_dict[y] = rxr.open_rasterio(nalcms_fpath, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24b1db69-7ff5-4533-9ceb-6536cb3ef9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_dict = {}\n",
    "for c in daymet_attributes:\n",
    "    fpath = os.path.join(daymet_folder, f'{c}_mosaic_3005.tiff')\n",
    "    climate_dict[c] = rxr.open_rasterio(fpath, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9418592-0745-42c5-94bf-ed1b8e7700c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps_data = gpd.read_file(os.path.join(glhymps_folder, 'GLHYMPS_clipped_3005.geojson'))\n",
    "glhymps_data.geometry = glhymps_data.geometry.make_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd06500-7445-425b-8d52-0ff9feac0e4f",
   "metadata": {},
   "source": [
    "### Re-process catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44703394-a7d7-4bc4-bc9e-345840d3a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "def get_merged_dem(basin_geom):\n",
    "    \"\"\"\n",
    "    There is one case (10ED002) where the catchment covers two regions,\n",
    "    so we need to load and merge the two region dem files to process terrain attributes.    \n",
    "    \"\"\"\n",
    "    r1_path = os.path.join(dem_folder, f'10E_USGS_3DEP_3005.tif')\n",
    "    r2_path = os.path.join(dem_folder, f'LRD_USGS_3DEP_3005.tif')\n",
    "    r1_dem, dem_crs, dem_affine = retrieve_raster(r1_path)\n",
    "    r2_dem, dem_crs, dem_affine = retrieve_raster(r2_path)\n",
    "\n",
    "    merged_raster = merge_arrays([r1_dem, r2_dem])\n",
    "    masked_raster = merged_raster.rio.clip(basin_geom.geometry, merged_raster.rio.crs)\n",
    "    \n",
    "    return masked_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f767a7fd-d162-4b47-989c-c24f8eef3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_catchment_attributes(rc, row, region_dem, crs):\n",
    "\n",
    "    stn_id = row['Official_ID']\n",
    "    \n",
    "    t0 = time.time()\n",
    "    basin_data = {}\n",
    "    basin_data['region'] = rc\n",
    "    basin_data['Official_ID'] = stn_id\n",
    "    basin_data['geometry'] = row['geometry']\n",
    "    basin_data['Drainage_Area_km2'] = round(row['geometry'].area / 1e6, 1)\n",
    "    basin_data['Centroid_Lon_deg_E'] = row['Centroid_Lon_deg_E']\n",
    "    basin_data['Centroid_Lat_deg_N'] = row['Centroid_Lat_deg_N']\n",
    "        \n",
    "    basin_polygon = gpd.GeoDataFrame(geometry=[row['geometry']], crs=crs)  \n",
    "    basin_polygon.geometry = basin_polygon.geometry.buffer(0)\n",
    "    \n",
    "    if not basin_polygon.is_valid.all():\n",
    "        basin_polygon.geometry = basin_polygon.geometry.make_valid()\n",
    "        if not basin_polygon.is_valid.all():\n",
    "            raise Exception('arg')\n",
    "        else:\n",
    "            print(f'Fixed invalid basin polygon geometry for {stn_id}.')\n",
    "\n",
    "    # process soil attributes\n",
    "    soil_masked = gpd.clip(glhymps_data, mask=basin_polygon)\n",
    "    soil_masked = soil_masked[soil_masked.geometry.area > 1.0]   \n",
    "    soil_masked.geometry = soil_masked.geometry.buffer(0)\n",
    "    soil_masked.geometry = soil_masked.geometry.make_valid()\n",
    "    \n",
    "    assert all(soil_masked.is_valid)    \n",
    "    porosity = get_soil_properties(soil_masked, 'Porosity_x')\n",
    "    permeability = get_soil_properties(soil_masked, 'logK_Ice_x')\n",
    "    basin_data['logk_ice_x100'] = round(permeability, 2)\n",
    "    basin_data['porosity_x100'] = round(porosity, 5)\n",
    "    del soil_masked\n",
    "    \n",
    "    # process NALCMS land cover\n",
    "    for y in [2010, 2015, 2020]:\n",
    "        # nalcms_fpath = os.path.join(nalcms_folder, f'NA_NALCMS_landcover_{y}_3005_clipped.tif')\n",
    "        # clipped_land_cover = rxr.open_rasterio(nalcms_fpath, masked=True).rio.clip(basin_polygon.geometry, all_touched=True)\n",
    "        clip_ok, clipped_nalcms = clip_raster_to_basin(basin_polygon, nalcms_dict[y])\n",
    "        land_cover = process_lulc(i, basin_polygon, clipped_nalcms, y)\n",
    "        land_cover = land_cover.to_dict('records')[0]\n",
    "        basin_data.update(land_cover)\n",
    "\n",
    "    # process terrain\n",
    "    # make a special case for 10ED002 where we need to load and merge\n",
    "    # the rasters for LRD and 10E and merge\n",
    "    del clipped_nalcms\n",
    "    if stn_id == '10ED002':\n",
    "        print(f'processing special case: {stn_id}')\n",
    "        clipped_dem = get_merged_dem(basin_polygon)\n",
    "    else:\n",
    "        dem_fpath = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005.tif')\n",
    "        assert os.path.exists(dem_fpath)\n",
    "        clip_ok, clipped_dem = clip_raster_to_basin(basin_polygon, region_dem)\n",
    "\n",
    "        slope, aspect = calculate_slope_and_aspect(clipped_dem)\n",
    "        # print(f'aspect, slope: {aspect:.1f} {slope:.2f} ')\n",
    "        basin_data['Slope_deg'] = slope\n",
    "        basin_data['Aspect_deg'] = aspect\n",
    "    \n",
    "        mean_el, median_el, min_el, max_el = process_basin_elevation(clipped_dem)\n",
    "        basin_data['median_el'] = median_el\n",
    "        basin_data['mean_el'] = mean_el\n",
    "        basin_data['max_el'] = max_el\n",
    "        basin_data['min_el'] = min_el\n",
    "        basin_data['Elevation_m'] = mean_el\n",
    "\n",
    "    # process climate params\n",
    "    del clipped_dem\n",
    "    for climate_param in daymet_attributes:\n",
    "        clip_ok, clipped_data = clip_raster_to_basin(basin_polygon, climate_dict[climate_param])\n",
    "        # Check if the clipped raster is empty or has no data\n",
    "        if clipped_data is None:\n",
    "            print(f'clip is empty, finding nearest point from polygon centroid')\n",
    "            # If the clipped raster is empty or contains only NaN, find the nearest value\n",
    "            spatial_mean = find_nearest_raster_value(climate_dict[climate_param], basin_polygon)            \n",
    "        else:\n",
    "            spatial_mean = round(clipped_data.mean(dim=['y', 'x']).item(), 1)\n",
    "            \n",
    "        basin_data[climate_param] = spatial_mean\n",
    "            # basin_polygon.to_file(f'{stn_id}_error.geojson')\n",
    "            # raise Exception(f'issue with {climate_param}')\n",
    "        \n",
    "    return basin_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7210fb-25c0-4e00-8a0e-115f8518aa89",
   "metadata": {},
   "source": [
    "### Load existing results, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d63c43f0-72eb-4c09-8132-059692159ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing results found.\n"
     ]
    }
   ],
   "source": [
    "all_basin_data = []\n",
    "t0 = time.time()\n",
    "\n",
    "results_df, processed_ids = pd.DataFrame(), []\n",
    "if os.path.exists(updated_catchment_attribute_path):\n",
    "    print(f'{updated_catchment_attribute_path.split(\"/\")[-1]} exists, loading existing file.')\n",
    "    results_df = gpd.read_file(updated_catchment_attribute_path)\n",
    "    print(f'{len(results_df)} existing results loaded')\n",
    "    processed_ids = results_df['Official_ID'].values.tolist()\n",
    "else:\n",
    "    print('No existing results found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1c016a-af27-4b52-a9df-4226c85eb70e",
   "metadata": {},
   "source": [
    "### Process monitored catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cd48034-8a2e-4def-b79a-eb0d4dfea430",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_folder = os.path.join(os.getcwd(), 'data/temp/')\n",
    "if not os.path.exists(temp_batch_folder):\n",
    "    os.makedirs(temp_batch_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9823c516-155b-4e48-b7f8-bb43cc23d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 08A region catchments\n",
      "Processing 08B region catchments\n",
      "Processing 08C region catchments\n",
      "Processing 08D region catchments\n",
      "Processing 08E region catchments\n",
      "Processing 08F region catchments\n",
      "Processing 08G region catchments\n",
      "Processing 10E region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "processing special case: 10ED002\n",
      "Processing CLR region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "Processing ERK region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "Processing FRA region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "At least one of the clipped raster x,y coordinates has only one point.\n",
      "clip is empty, finding nearest point from polygon centroid\n",
      "Processing HAY region catchments\n",
      "Processing HGW region catchments\n",
      "Processing LRD region catchments\n",
      "Processing PCR region catchments\n",
      "Processing VCI region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "Processing WWA region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "Processing YKR region catchments\n",
      "     ...saving batch output file.\n",
      "    ...saved 200 results file (200 unique station ids).\n",
      "     ...saving batch output file.\n",
      "    ...saved 8 results file (8 unique station ids).\n",
      "     ...saving batch output file.\n",
      "    ...saved 1 results file (1 unique station ids).\n"
     ]
    }
   ],
   "source": [
    "batch_results = []\n",
    "batch_fpaths = []\n",
    "batch_no = 0\n",
    "if not os.path.exists(updated_catchment_attribute_path):\n",
    "    for rc in region_codes:\n",
    "        print(f'Processing {rc} region catchments')\n",
    "        batch_df = bcub_gdf[bcub_gdf['region_code'] == rc].copy()\n",
    "        dem_fpath = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005.tif')\n",
    "        assert os.path.exists(dem_fpath), f'{dem_fpath} not found'\n",
    "        region_dem = rxr.open_rasterio(dem_fpath, mask_and_scale=True)\n",
    "        for i, row in batch_df.iterrows():\n",
    "            stn_id = row['Official_ID']    \n",
    "            result = process_catchment_attributes(rc, row, region_dem, bcub_gdf.crs)            \n",
    "            batch_results.append(result)\n",
    "            processed_ids.append(stn_id)\n",
    "            if (len(batch_results) % 200 == 0) | (len(processed_ids) >= len(bcub_gdf) - 1):\n",
    "                batch_no += 1\n",
    "                \n",
    "                new_results = gpd.GeoDataFrame(batch_results, crs='EPSG:3005')\n",
    "                print('     ...saving batch output file.')\n",
    "                batch_path = os.path.join(temp_batch_folder, f'updated_attributes_batch_{batch_no}.geojson')\n",
    "                new_results.to_file(batch_path)\n",
    "                batch_fpaths.append(batch_path)\n",
    "                batch_results = []\n",
    "                n_unique = len(list(set(new_results['Official_ID'])))\n",
    "                print(f'    ...saved {len(new_results)} results file ({n_unique} unique station ids).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1242836b-c9ac-4c0e-ae7c-6a85b37dbdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_1.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_2.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_3.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_4.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_5.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_6.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_7.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_8.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_9.geojson',\n",
       " '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/temp/updated_attributes_batch_10.geojson']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_fpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58065a9c-361b-4e5f-b12d-301718aa0e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the result batches\n",
    "all_results_dataframes = pd.concat([gpd.read_file(f) for f in batch_fpaths])\n",
    "updated_gdf = gpd.GeoDataFrame(all_results_dataframes, crs='EPSG:3005')\n",
    "# updated_gdf.to_file(updated_catchment_attribute_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a92dbe99-c4d3-4e49-b6b9-a93c7befe379",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_df = updated_gdf[[c for c in updated_gdf.columns if c != 'geometry']].copy()\n",
    "attributes_df.to_csv(attributes_fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "539ec356-bfce-42ec-b3a5-bab74f4f02f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>logk_ice_x100</th>\n",
       "      <th>porosity_x100</th>\n",
       "      <th>Land_Use_Forest_frac_2010</th>\n",
       "      <th>Land_Use_Shrubs_frac_2010</th>\n",
       "      <th>Land_Use_Grass_frac_2010</th>\n",
       "      <th>...</th>\n",
       "      <th>prcp</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>vp</th>\n",
       "      <th>swe</th>\n",
       "      <th>srad</th>\n",
       "      <th>low_prcp_duration</th>\n",
       "      <th>low_prcp_freq</th>\n",
       "      <th>high_prcp_duration</th>\n",
       "      <th>high_prcp_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08A</td>\n",
       "      <td>08AA004</td>\n",
       "      <td>695.3</td>\n",
       "      <td>60.490103</td>\n",
       "      <td>-137.436239</td>\n",
       "      <td>-1338.24</td>\n",
       "      <td>11.70636</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>691.6</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>346.5</td>\n",
       "      <td>280.5</td>\n",
       "      <td>215.4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08A</td>\n",
       "      <td>08AA008</td>\n",
       "      <td>1226.8</td>\n",
       "      <td>61.446754</td>\n",
       "      <td>-137.752162</td>\n",
       "      <td>-1108.36</td>\n",
       "      <td>0.65119</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.51</td>\n",
       "      <td>...</td>\n",
       "      <td>433.1</td>\n",
       "      <td>-10.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>329.7</td>\n",
       "      <td>101.4</td>\n",
       "      <td>193.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08A</td>\n",
       "      <td>08AA009</td>\n",
       "      <td>184.6</td>\n",
       "      <td>61.211074</td>\n",
       "      <td>-136.863365</td>\n",
       "      <td>-1068.02</td>\n",
       "      <td>0.05309</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>430.1</td>\n",
       "      <td>-10.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>337.9</td>\n",
       "      <td>116.0</td>\n",
       "      <td>200.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08A</td>\n",
       "      <td>08AB001</td>\n",
       "      <td>15352.7</td>\n",
       "      <td>60.807985</td>\n",
       "      <td>-137.677689</td>\n",
       "      <td>-1324.77</td>\n",
       "      <td>2.82491</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>666.9</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>338.1</td>\n",
       "      <td>307.0</td>\n",
       "      <td>206.8</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08A</td>\n",
       "      <td>08AB002</td>\n",
       "      <td>27320.5</td>\n",
       "      <td>60.373054</td>\n",
       "      <td>-137.607298</td>\n",
       "      <td>-1327.49</td>\n",
       "      <td>3.90509</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1164.0</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>370.7</td>\n",
       "      <td>651.4</td>\n",
       "      <td>212.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YKR</td>\n",
       "      <td>09AE003</td>\n",
       "      <td>3391.8</td>\n",
       "      <td>59.922386</td>\n",
       "      <td>-131.242037</td>\n",
       "      <td>-1216.43</td>\n",
       "      <td>1.87940</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>624.8</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>351.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>224.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YKR</td>\n",
       "      <td>09AE004</td>\n",
       "      <td>1865.2</td>\n",
       "      <td>59.688497</td>\n",
       "      <td>-132.921383</td>\n",
       "      <td>-1229.33</td>\n",
       "      <td>2.20238</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>722.3</td>\n",
       "      <td>-8.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>383.1</td>\n",
       "      <td>291.2</td>\n",
       "      <td>222.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>YKR</td>\n",
       "      <td>09AE006</td>\n",
       "      <td>1576.5</td>\n",
       "      <td>60.254980</td>\n",
       "      <td>-131.786971</td>\n",
       "      <td>-1248.97</td>\n",
       "      <td>2.15092</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>615.8</td>\n",
       "      <td>-9.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>352.9</td>\n",
       "      <td>252.6</td>\n",
       "      <td>222.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YKR</td>\n",
       "      <td>09AF001</td>\n",
       "      <td>35368.0</td>\n",
       "      <td>60.315025</td>\n",
       "      <td>-132.345961</td>\n",
       "      <td>-1191.15</td>\n",
       "      <td>2.42053</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>564.9</td>\n",
       "      <td>-8.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>365.8</td>\n",
       "      <td>208.5</td>\n",
       "      <td>214.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YKR</td>\n",
       "      <td>09AG003</td>\n",
       "      <td>475.5</td>\n",
       "      <td>61.270392</td>\n",
       "      <td>-134.186904</td>\n",
       "      <td>-1257.45</td>\n",
       "      <td>3.94565</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>445.1</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>343.1</td>\n",
       "      <td>141.7</td>\n",
       "      <td>197.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1609 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   region Official_ID  Drainage_Area_km2  Centroid_Lon_deg_E  \\\n",
       "0     08A     08AA004              695.3           60.490103   \n",
       "1     08A     08AA008             1226.8           61.446754   \n",
       "2     08A     08AA009              184.6           61.211074   \n",
       "3     08A     08AB001            15352.7           60.807985   \n",
       "4     08A     08AB002            27320.5           60.373054   \n",
       "..    ...         ...                ...                 ...   \n",
       "4     YKR     09AE003             3391.8           59.922386   \n",
       "5     YKR     09AE004             1865.2           59.688497   \n",
       "6     YKR     09AE006             1576.5           60.254980   \n",
       "7     YKR     09AF001            35368.0           60.315025   \n",
       "0     YKR     09AG003              475.5           61.270392   \n",
       "\n",
       "    Centroid_Lat_deg_N  logk_ice_x100  porosity_x100  \\\n",
       "0          -137.436239       -1338.24       11.70636   \n",
       "1          -137.752162       -1108.36        0.65119   \n",
       "2          -136.863365       -1068.02        0.05309   \n",
       "3          -137.677689       -1324.77        2.82491   \n",
       "4          -137.607298       -1327.49        3.90509   \n",
       "..                 ...            ...            ...   \n",
       "4          -131.242037       -1216.43        1.87940   \n",
       "5          -132.921383       -1229.33        2.20238   \n",
       "6          -131.786971       -1248.97        2.15092   \n",
       "7          -132.345961       -1191.15        2.42053   \n",
       "0          -134.186904       -1257.45        3.94565   \n",
       "\n",
       "    Land_Use_Forest_frac_2010  Land_Use_Shrubs_frac_2010  \\\n",
       "0                        0.30                       0.11   \n",
       "1                        0.20                       0.21   \n",
       "2                        0.27                       0.23   \n",
       "3                        0.33                       0.13   \n",
       "4                        0.28                       0.12   \n",
       "..                        ...                        ...   \n",
       "4                        0.55                       0.13   \n",
       "5                        0.51                       0.14   \n",
       "6                        0.67                       0.13   \n",
       "7                        0.67                       0.13   \n",
       "0                        0.53                       0.18   \n",
       "\n",
       "    Land_Use_Grass_frac_2010  ...    prcp  tmin  tmax     vp    swe   srad  \\\n",
       "0                       0.47  ...   691.6  -9.7   1.6  346.5  280.5  215.4   \n",
       "1                       0.51  ...   433.1 -10.9   1.0  329.7  101.4  193.9   \n",
       "2                       0.44  ...   430.1 -10.1   2.0  337.9  116.0  200.2   \n",
       "3                       0.33  ...   666.9 -10.4   1.2  338.1  307.0  206.8   \n",
       "4                       0.33  ...  1164.0  -9.4   1.4  370.7  651.4  212.7   \n",
       "..                       ...  ...     ...   ...   ...    ...    ...    ...   \n",
       "4                       0.27  ...   624.8  -9.7   2.4  351.0  257.0  224.2   \n",
       "5                       0.27  ...   722.3  -8.1   2.0  383.1  291.2  222.1   \n",
       "6                       0.15  ...   615.8  -9.6   2.3  352.9  252.6  222.9   \n",
       "7                       0.15  ...   564.9  -8.8   2.5  365.8  208.5  214.5   \n",
       "0                       0.25  ...   445.1 -10.0   0.9  343.1  141.7  197.9   \n",
       "\n",
       "    low_prcp_duration  low_prcp_freq  high_prcp_duration  high_prcp_freq  \n",
       "0                 5.1            0.7                 1.0             0.1  \n",
       "1                 6.3            0.8                 1.0             0.1  \n",
       "2                 5.7            0.8                 1.0             0.1  \n",
       "3                 5.4            0.7                 1.0             0.1  \n",
       "4                 4.6            0.7                 1.0             0.1  \n",
       "..                ...            ...                 ...             ...  \n",
       "4                 3.9            0.7                 1.0             0.1  \n",
       "5                 4.0            0.7                 1.1             0.1  \n",
       "6                 4.0            0.7                 1.0             0.1  \n",
       "7                 4.3            0.7                 1.0             0.1  \n",
       "0                 5.1            0.7                 1.0             0.1  \n",
       "\n",
       "[1609 rows x 51 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c61f5c-5df5-41f5-908b-b81fac43ea1f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a4a3a-3dfe-40f9-9ce0-d91528e5517c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26811713-11c5-4b80-97b7-1e74688f10af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
