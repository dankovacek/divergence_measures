{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5ce998-903d-4b91-9dbf-e9e69e9f2f42",
   "metadata": {},
   "source": [
    "# Update HYSETS Catchment Polygons\n",
    "\n",
    "The HYSETS dataset {cite}`arsenault2020comprehensive` contains an \"artificial boundaries\" flag to indicate where the catchment boundary for the monitoring location is approximated due to either missing data or uncertainty in catchment delineation, in general due to small drainage area.  In July 2022, the Water Survey of Canada published updated polygons for over 8000 catchments in Canada.  We use updated catchment boundaries where available and additionally check the USGS database for updated polygons.  Polygons updated from USGS and WSC official sources cover 64% of the \"artificial bounds\" flagged catchments, and the remaining flags are updated using the closest matching station in the [BCUB dataset](https://doi.org/10.5683/SP3/JNKZVT). \n",
    "\n",
    "To start, download the HYSETS catchment polygons (`HYSETS_watershed_boundaries.zip`) from [that dataset's open data repository](https://osf.io/rpc3w/).\n",
    "\n",
    "The resulting updated polygons are used in the next chapter/section to extract and validate attributes as part of the supporting information for technical validation of the associated publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331b3ee-44ba-4bc0-9d19-07f3a05ca2a1",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4c4e3-dbac-4cee-9765-50ac29902287",
   "metadata": {},
   "source": [
    "### Import HYSETS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3129c-b011-4ba1-8b7d-e64fadbad4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627071f7-2d43-4d42-884b-ea9e853312d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filename where the updated results will be stored\n",
    "updated_fpath = 'data/BCUB_watershed_bounds_updated.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d3f33-da10-4237-8c7c-15c4b8ccd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the HYSETS attributes data\n",
    "hysets_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hysets_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634249dd-bd4f-4a1c-b260-c7b61cb9d41b",
   "metadata": {},
   "source": [
    "### Import (BCUB) study region bounds\n",
    "\n",
    "Get the region bounds from the BCUB dataset [https://doi.org/10.5683/SP3/JNKZVT](https://doi.org/10.5683/SP3/JNKZVT) or just skip this step and use the pre-processed file (`data/data/study_region_stations.geojson`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca6dc1-cae6-40d9-a1a5-86821e8cc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BCUB (study) region boundary\n",
    "region_gdf = gpd.read_file('data/BCUB_regions_4326.geojson')\n",
    "region_gdf = region_gdf.to_crs(3005)\n",
    "# simplify the geometries (100m threshold) and add a small buffer (250m) to capture HYSETS station points recorded with low accuracy near boundaries\n",
    "region_gdf.geometry = region_gdf.simplify(100).buffer(500)\n",
    "# region_gdf = region_gdf.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9040e7-32ef-42f0-8fdc-53715f829085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stations contained in the study region\n",
    "centroids = hysets_df.apply(lambda x: Point(x['Centroid_Lon_deg_E'], x['Centroid_Lat_deg_N']), axis=1)\n",
    "hysets_points = gpd.GeoDataFrame(hysets_df, geometry=centroids, crs='EPSG:4326')\n",
    "hysets_points.to_crs(3005, inplace=True)\n",
    "hysets_points.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292928ee-aca4-4d1f-b8b0-de84c9c551c6",
   "metadata": {},
   "source": [
    "Note that these are just the artificial bounds flagged rows, below we check for other corrections/updates from official sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c739937-1d9b-4e54-9012-04ada424834e",
   "metadata": {},
   "source": [
    "### Load the original HYSETS polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13af91-986f-49ac-85c3-bc441c6a5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_path = 'data/catchment_polygons/HYSETS_watershed_boundaries/HYSETS_watershed_boundaries_20200730.shp'\n",
    "hs_polygons = gpd.read_file(hs_path)\n",
    "hs_polygons = hs_polygons.set_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d18b5-21b8-488c-817c-57a39bcfa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_gdf = gpd.read_file('data/study_region_stations.geojson')\n",
    "print(bcub_gdf.crs)\n",
    "# the bcub geometries are (centroid) points from the HYSETS properties, \n",
    "# set the geometry to the HYSETS polygons instead\n",
    "catchment_geometries = bcub_gdf.apply(lambda x: hs_polygons.loc[hs_polygons['OfficialID'] == x['Official_ID'], 'geometry'].values[0], axis=1)\n",
    "bcub_gdf['geometry'] = catchment_geometries\n",
    "bcub_gdf.to_crs(3005, inplace=True)\n",
    "hs_polygons.to_crs(3005, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97508d3d-e8ac-4bfe-b680-311d7af9df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df = bcub_gdf.loc[bcub_gdf['Flag_Artificial_Boundaries']== 1, :].copy()\n",
    "print(f'{len(ab_df)}/{len(bcub_gdf)} catchment geometries are flagged \"artificial bounds\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879db7d-519b-4687-9111-d86421c7a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc_ab = ab_df.loc[ab_df['Source'] == 'HYDAT', :].copy()\n",
    "usgs_ab = ab_df.loc[ab_df['Source'] == 'USGS', :].copy()\n",
    "print(f'{len(wsc_ab)}/{len(usgs_ab)} WSC/USGS artificial bounds flags ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9539f5-a40d-4d90-a601-131e89c7fda4",
   "metadata": {},
   "source": [
    "### Check for updated WSC polygons\n",
    "\n",
    "The updated WSC catchments can be accessed at the Environment and Climate Change Canada (ECCC) [hydrometrics page](https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/HydrometricNetworkBasinPolygons/).  We only need to download the region files associated with the study region, corresponding to the first two digits of the station identifier (official id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde59e4-b53c-4c33-b0d9-3b3e34f18b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc_stn_df = bcub_gdf.loc[bcub_gdf['Source'] == 'HYDAT'].copy()\n",
    "wsc_stns = wsc_stn_df['Official_ID']\n",
    "prefixes = list(set([e[:2] for e in wsc_stns]))\n",
    "prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5271c-413d-438f-aaa7-0f35699d1961",
   "metadata": {},
   "source": [
    "Download the above files `<2-digit identifier>.zip` and extract them in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f964b91-5e06-477a-8a60-10639ab4c18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find updated catchment polygons\n",
    "def retrieve_and_update_WSC_polygon(stn_id):\n",
    "    \"\"\"\n",
    "    Returns an updated WSC polygon if it exists or returns an empty (geo)dataframe.\n",
    "    \"\"\"\n",
    "    stn_id = row['Official_ID']\n",
    "    stn_prefix = stn_id[:2]\n",
    "    catchment_path = f'data/catchment_polygons/{stn_prefix}/{stn_id}/{stn_id}_DrainageBasin_BassinDeDrainage.shp'\n",
    "    if os.path.exists(catchment_path):\n",
    "        print(f'Updated polygon found for {stn_id}')\n",
    "        updated_polygon = gpd.read_file(catchment_path)\n",
    "        updated_polygon.to_crs(3005, inplace=True)\n",
    "        return updated_polygon\n",
    "    return gpd.GeoDataFrame()\n",
    "\n",
    "def retrieve_and_update_WSC_station_location(stn_id):\n",
    "    \"\"\"\n",
    "    Returns an updated WSC station location if it exists or returns an empty (geo)dataframe.\n",
    "    \"\"\"\n",
    "    stn_id = row['Official_ID']\n",
    "    stn_prefix = stn_id[:2]\n",
    "    file_path = f'data/catchment_polygons/{stn_prefix}/{stn_id}/{stn_id}_Station.shp'\n",
    "    if os.path.exists(file_path):\n",
    "        print(f'Updated station location found for {stn_id}')\n",
    "        updated_pt = gpd.read_file(file_path)\n",
    "        updated_pt.to_crs(3005, inplace=True)\n",
    "        return updated_pt\n",
    "    return gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c500c31-1e60-4f78-a42c-ee9ebd6f093a",
   "metadata": {},
   "source": [
    "### Check for updated USGS polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42498fa-f699-4683-b457-e220ac13b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api url for nwis sites\n",
    "usgs_api_url = 'https://labs.waterdata.usgs.gov/api/nldi/linked-data/nwissite/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c1dfd-5426-4c35-bc53-1fb584cd4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_usgs_stn_data(stn):\n",
    "    # query the NWIS with the station number to get the station coordinates    \n",
    "    try:\n",
    "        query_url = usgs_api_url + f'USGS-{stn}'\n",
    "        usgs_data = pd.read_json(query_url)\n",
    "        usgs_stn_loc = usgs_data['features'][0]['geometry']['coordinates']\n",
    "        stn_pt = Point(*usgs_stn_loc)\n",
    "        return gpd.GeoDataFrame(geometry=[stn_pt], crs=4326)\n",
    "    except Exception as ex:\n",
    "        msg = f'USGS station query failed for {stn}. {ex}'\n",
    "        print(msg)\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce0b8b-6b2e-4232-9ff5-47b2bd7a8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usgs_basin_polygon_query(url):\n",
    "    \"\"\"USGS polygons are in EPSG:4326 crs\"\"\"\n",
    "    response = urlopen(url)\n",
    "    json_data = response.read().decode('utf-8', 'replace')\n",
    "    d = json.loads(json_data)\n",
    "    return gpd.GeoDataFrame.from_features(d['features'], crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397b462-e777-49d4-a189-99b04229268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_usgs_basin_data(stn):\n",
    "    \"\"\"Retrieve the USGS basin polygon and station location from the NLDI API. \n",
    "    If there is no basin for the station, use the NLDI to retrieve upstream \n",
    "    and downstream boundaries.  \n",
    "    Pick the one closest in (HYSETS published) area to the station location.\"\"\"    \n",
    "    \n",
    "    # query the basin polygon from USGS\n",
    "    basin_query = usgs_api_url + f'USGS-{stn}/basin?simplified=false&splitCatchment=false'    \n",
    "    try:\n",
    "        usgs_basin_df = usgs_basin_polygon_query(basin_query)\n",
    "        # dissolve the basin polygons\n",
    "        usgs_basin_df = usgs_basin_df.dissolve()\n",
    "        usgs_basin_df = usgs_basin_df.to_crs(3005)\n",
    "        # check if geometry is multipolygon\n",
    "        if usgs_basin_df.geometry.type.values[0] == 'MultiPolygon':\n",
    "            print(f'   ...MultiPolygon detected, attemping to make geometry valid.')\n",
    "            usgs_basin_df = usgs_basin_df.explode()\n",
    "            usgs_basin_df['area'] = usgs_basin_df.geometry.area / 1E6\n",
    "            usgs_basin_df['area_pct'] = usgs_basin_df['area'] / usgs_basin_df['area'].sum()\n",
    "            usgs_basin_df = usgs_basin_df[usgs_basin_df['area_pct'] > 0.95]\n",
    "            if len(usgs_basin_df) > 1:\n",
    "                raise Exception('USGS basin polygon query returned multiple polygons.')\n",
    "\n",
    "        return usgs_basin_df\n",
    "    except Exception as ex:\n",
    "        print(f'USGS basin polygon query failed for {stn}.  {ex}')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382731fc-9f79-4a70-b252-aee083a0bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_gdf['geometry_updated'] = False\n",
    "# set a variable to prevent jupyter book from processing\n",
    "# during book building step\n",
    "process_step = False\n",
    "if process_step:\n",
    "    for i, row in bcub_gdf.iterrows():\n",
    "        stn_id = row['Official_ID']\n",
    "        source = row['Source']\n",
    "        if source == 'HYDAT':\n",
    "            # pt = retrieve_and_update_WSC_station_location(stn_id)\n",
    "            polygon = retrieve_and_update_WSC_polygon(stn_id)            \n",
    "        elif source == 'USGS':\n",
    "            # pt = retrieve_usgs_stn_data(stn_id)\n",
    "            polygon = retrieve_usgs_basin_data(stn_id)    \n",
    "        if not polygon.empty:\n",
    "            assert polygon.crs == 'EPSG:3005'\n",
    "            pt = polygon.geometry.centroid\n",
    "            pt = pt.to_crs(4326)\n",
    "            lat, lon = pt.geometry.x[0], pt.geometry.y[0]\n",
    "    \n",
    "            bcub_gdf.loc[i, 'Centroid_Lat_deg_N'] = lat\n",
    "            bcub_gdf.loc[i, 'Centroid_Lon_deg_E'] = lon\n",
    "            bcub_gdf.loc[i, 'geometry'] = polygon.geometry.values[0]\n",
    "            bcub_gdf.loc[i, 'geometry_updated'] = True\n",
    "            bcub_gdf.loc[i, 'Flag_Artificial_Boundaries'] = False    \n",
    "        else:\n",
    "            polygon = hs_polygons.loc[hs_polygons['OfficialID'] == stn_id, 'geometry'].copy()\n",
    "            bcub_gdf.loc[i, 'geometry'] = polygon.geometry.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629d070-adf2-417a-ad2b-3f275e595563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df_remaining = bcub_gdf.loc[bcub_gdf['Flag_Artificial_Boundaries'] == 1, :].copy()\n",
    "print(f'{len(ab_df_remaining)}/{len(bcub_gdf)} catchment geometries remain flagged \"artificial bounds\" ({len(ab_df) - len(ab_df_remaining)} updated.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151f419-4d73-431c-bcab-4171083ef224",
   "metadata": {},
   "source": [
    "### For the remaining 'artificial bounds' catchments, derive them from USGS 3DEP DEM.\n",
    "\n",
    "For all of the remaining rows labeled \"FLAG_Artificial_Boundaries\", follow the methodology in the [BCUB dataset demo](https://dankovacek.github.io/bcub_demo/notebooks/2_DEM_Preprocessing.html) and reprocess the catchment bounds.\n",
    "\n",
    "First, find which regions have flagged geometries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5527587-da40-46ee-9813-8d7e969c98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to ensure all geometries are polygon type\n",
    "assert all(ab_df_remaining.geometry.geom_type == 'Polygon')\n",
    "if 'index_right' in ab_df_remaining.columns:\n",
    "    ab_df_remaining.drop(columns=['index_right'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083a640-fe38-4a98-908f-5fce78fe11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_step:\n",
    "    ab_df_remaining['bcub_region_code'] = None\n",
    "    for i, row in ab_df_remaining.iterrows():\n",
    "        # region_polygon = region_gdf.loc[i, 'geometry']\n",
    "        listed_da = row['Drainage_Area_km2']\n",
    "        centroid = row['geometry'].centroid\n",
    "        stn_id = row['Official_ID']\n",
    "            \n",
    "        # get the intersecting region\n",
    "        intersecting_region = gpd.sjoin(ab_df_remaining.loc[[i]], region_gdf, how='inner', predicate='intersects')\n",
    "        rc = intersecting_region['region_code_left']\n",
    "        if rc.empty:\n",
    "            foo = gpd.GeoDataFrame([stn_id], geometry=[centroid], crs='EPSG:3005')\n",
    "            foo.to_file(f'data/catchment_polygons/{stn_id}_centroid.geojson', driver='GeoJSON')\n",
    "            raise Exception('region search failed')\n",
    "        else:\n",
    "            ab_df_remaining.loc[i, 'bcub_region_code'] = intersecting_region['region_code_left'].values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214cbb1-33f2-4228-a7e0-5653e04201a4",
   "metadata": {},
   "source": [
    "### Set DEM folder -- need processed stream raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee86eb-5e14-41ca-a39a-4c64aa745ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_folder = '/home/danbot2/code_5820/large_sample_hydrology/bcub/processed_data/processed_dem/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da956d0-e124-4767-89f9-24b73b24d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "from rasterio.features import dataset_features\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def retrieve_raster(filename):\n",
    "    \"\"\"\n",
    "    Take in a file name and return the raster data, \n",
    "    the coordinate reference system, and the affine transform.\n",
    "    \"\"\"\n",
    "    raster = rxr.open_rasterio(filename, mask_and_scale=True)\n",
    "    crs = raster.rio.crs\n",
    "    affine = raster.rio.transform(recalc=False)\n",
    "    return raster, crs.to_epsg(), affine\n",
    "\n",
    "def nearest_stream_pixel(dem_filepath, points_gdf):\n",
    "    # Read the DEM raster\n",
    "    with rio.open(dem_filepath) as src:\n",
    "        dem_data = src.read(1)  # Read the first band\n",
    "        transform = src.transform\n",
    "        no_data = src.nodata\n",
    "        \n",
    "        # Get the coordinates of the stream pixels (assuming stream pixels are not no_data)\n",
    "        stream_indices = np.column_stack(np.where(dem_data != no_data))\n",
    "        \n",
    "        # Convert raster indices to centroids of the pixels\n",
    "        stream_coords = []\n",
    "        for row, col in stream_indices:\n",
    "            x_center, y_center = rio.transform.xy(transform, row, col, offset='center')\n",
    "            stream_coords.append((x_center, y_center))\n",
    "\n",
    "        # Build a KDTree for fast nearest-neighbor lookup\n",
    "        stream_tree = cKDTree(stream_coords)\n",
    "    \n",
    "    # Function to find nearest stream pixel for a given point\n",
    "    def find_nearest_stream(point):\n",
    "        point_coords = np.array(point.coords[0])\n",
    "        distance, idx = stream_tree.query(point_coords)\n",
    "        nearest_stream_coord = stream_coords[idx]\n",
    "        return nearest_stream_coord, distance\n",
    "    \n",
    "    # Apply the function to each point in the GeoDataFrame\n",
    "    results = points_gdf.geometry.apply(find_nearest_stream)\n",
    "    \n",
    "    # Create a new GeoDataFrame with the results\n",
    "    nearest_coords = [result[0] for result in results]\n",
    "    distances = [result[1] for result in results]\n",
    "    nearest_points_gdf = gpd.GeoDataFrame({\n",
    "        'geometry': [Point(coord) for coord in nearest_coords],\n",
    "        'distance_to_stream': distances,\n",
    "        'Official_ID': points_gdf['Official_ID']\n",
    "    }, crs=points_gdf.crs)\n",
    "    \n",
    "    return nearest_points_gdf    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac68c4e-6e86-4b9b-b0db-4b16b047356f",
   "metadata": {},
   "source": [
    "### Find the nearest pixel in the stream raster corresponding to the point geometry\n",
    "\n",
    "Note that where the catchment geometry is missing in HYSETS, the centroid point is just the station location, and the geometry is simply a square centred at the station location and with an area equal to the drainage area reported by the official source {cite}`arsenault2020comprehensive`.  We then find the nearest stream pixel to the \"centroid\" which is not actually the centroid but the reported station location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d77c2-82b8-487c-b79d-3d38639a1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df_remaining = ab_df_remaining.sort_values(by=['bcub_region_code'])\n",
    "\n",
    "pour_pt_filenames = []\n",
    "for rc in list(set(ab_df_remaining['bcub_region_code'])):\n",
    "    print(f'Processing {rc}')\n",
    "    to_check = ab_df_remaining[ab_df_remaining['bcub_region_code'] == rc].copy()[['Official_ID', 'Centroid_Lat_deg_N', 'Centroid_Lon_deg_E']]\n",
    "    to_check['geometry'] = to_check.apply(lambda x: Point(x['Centroid_Lon_deg_E'], x['Centroid_Lat_deg_N']), axis=1)\n",
    "    check_pts = gpd.GeoDataFrame(to_check, crs='4326')\n",
    "    check_pts = check_pts.to_crs(3005)\n",
    "    # load the stream raster\n",
    "    stream_dem_path = os.path.join(dem_folder, f'{rc}_USGS_3DEP_3005_stream.tif')\n",
    "    temp_fpath = f'data/catchment_polygons/updated_polygon_set/{rc}_pour_points.shp'\n",
    "    if not os.path.exists(temp_fpath):\n",
    "        nearest_px = nearest_stream_pixel(stream_dem_path, check_pts)\n",
    "        nearest_px.to_file(temp_fpath)\n",
    "    print(f'   ...processed {rc}')\n",
    "    pour_pt_filenames.append(temp_fpath)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c123c-02e7-4ce9-a3b8-95b0907301de",
   "metadata": {},
   "source": [
    "### Delineate catchments with Whiteboxtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab3d14-9eac-4a0e-b5b6-48ae7fd17dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whitebox \n",
    "wbt = whitebox.WhiteboxTools()\n",
    "wbt.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e817eb1-c086-4c5a-b2f5-903560e7ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if process_step:\n",
    "    for f in pour_pt_filenames:\n",
    "        rc = f.split('/')[-1].split('_')[0]\n",
    "        print(f'   ...processing {rc} pour points')\n",
    "        d8_path = f'/home/danbot2/code_5820/large_sample_hydrology/bcub/processed_data/processed_dem/{rc}_USGS_3DEP_3005_fdir.tif'\n",
    "        assert os.path.exists(d8_path)\n",
    "        assert os.path.exists(f)\n",
    "        output_fpath = os.path.join(os.getcwd(), f.replace('_pour_points.shp', '_basins.tif'))\n",
    "        if not os.path.exists(output_fpath):\n",
    "            wbt.unnest_basins(\n",
    "                d8_path, \n",
    "                os.path.join(os.getcwd(), f), \n",
    "                output_fpath, \n",
    "                esri_pntr=False, \n",
    "            )\n",
    "        print(f'   ...completed processing of {rc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a87586-453f-4470-963e-a1ab7d9e9860",
   "metadata": {},
   "source": [
    "### Convert raster to vector polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd6097-00d6-4f01-a87d-743730f6ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_explode_geoms(gdf, min_area):\n",
    "    gdf.geometry = gdf.apply(lambda row: make_valid(row.geometry) if not row.geometry.is_valid else row.geometry, axis=1)    \n",
    "    gdf = gdf.explode(index_parts=True)\n",
    "    gdf['area'] = gdf.geometry.area / 1E6\n",
    "    gdf = gdf[gdf['area'] >= min_area * 0.95]    \n",
    "    return gdf\n",
    "\n",
    "    \n",
    "def raster_to_vector_batch(rc, raster_fname, raster_crs, resolution, min_area, temp_folder):\n",
    "    \"\"\"\n",
    "    If we send too many pour points per batch to the Whitebox \"unnest\" function, \n",
    "    we generate a huge number of temporary raster files that could easily \n",
    "    exceed current SSD disk capacities.\n",
    "    \"\"\"\n",
    "    raster_path = os.path.join(temp_folder, raster_fname)\n",
    "    raster_no = int(raster_fname.split('_')[-1].split('.')[0])\n",
    "    \n",
    "    polygon_path = os.path.join(temp_folder, f'{rc}_temp_polygons_{raster_no:05}.shp')\n",
    "    \n",
    "    output_path = polygon_path.replace('.shp', '.geojson')\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'{output_path.split(\"/\")[-1]} exists')\n",
    "        existing_df = gpd.read_file(output_path)\n",
    "        return existing_df\n",
    "    else:\n",
    "        # this function creates rasters of ordered \n",
    "        # sets of non-overlapping basins\n",
    "        wbt.raster_to_vector_polygons(\n",
    "            raster_path,\n",
    "            polygon_path,\n",
    "        )\n",
    "    \n",
    "        gdf = gpd.read_file(polygon_path, crs=raster_crs)\n",
    "    \n",
    "        # simplify the polygon geometry to avoid self-intersecting polygons\n",
    "        simplify_dim = 0.5 * np.sqrt(resolution[0]**2 + resolution[1]**2)\n",
    "        simplify_dim = abs(resolution[0])\n",
    "        buffer_dim = 10\n",
    "        gdf.geometry = gdf.geometry.buffer(buffer_dim)\n",
    "        gdf.geometry = gdf.geometry.simplify(simplify_dim)\n",
    "        gdf.geometry = gdf.geometry.buffer(-1.0 * buffer_dim)\n",
    "        \n",
    "        gdf = filter_and_explode_geoms(gdf, min_area)\n",
    "    \n",
    "        assert (gdf.geometry.geom_type == 'Polygon').all()\n",
    "            \n",
    "        gdf.drop(labels=['area'], inplace=True, axis=1)\n",
    "        gdf.to_file(output_path)    \n",
    "        return gdf\n",
    "    \n",
    "\n",
    "def format_batch_geometries(all_polygons, ppt_batch, region_raster_crs):\n",
    "    \n",
    "    ppt_batch['ppt_x'] = ppt_batch.geometry.x.astype(int)\n",
    "    ppt_batch['ppt_y'] = ppt_batch.geometry.y.astype(int)\n",
    "    ppt_batch.drop(inplace=True, labels=['geometry'], axis=1)\n",
    "\n",
    "    # concatenate the polygon batches\n",
    "    for f in all_polygons:\n",
    "        print(len(f))\n",
    "\n",
    "    batch_polygons = gpd.GeoDataFrame(pd.concat(all_polygons), crs=region_raster_crs)\n",
    "    print(len(all_polygons))\n",
    "    batch_polygons.sort_values(by='VALUE', inplace=True)\n",
    "    batch_polygons.reset_index(inplace=True, drop=True)\n",
    "    # print(batch_polygons)\n",
    "    # print(asdf)\n",
    "    \n",
    "    batch = gpd.GeoDataFrame(pd.concat([batch_polygons, ppt_batch], axis=1), crs=region_raster_crs)\n",
    "    # print(batch)\n",
    "    # print(asdf)\n",
    "    batch['centroid_x'] = batch.geometry.centroid.x.astype(int)\n",
    "    batch['centroid_y'] = batch.geometry.centroid.y.astype(int)\n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e2c4c-e643-48d9-96fb-73acfddae462",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_area = 1.0 # km^2\n",
    "\n",
    "if process_step:\n",
    "    for f in pour_pt_filenames:\n",
    "        rc = f.split('/')[-1].split('_')[0]\n",
    "        output_fpath = os.path.join(os.getcwd(), f'data/catchment_polygons/{rc}_basins_BCUB.geojson')\n",
    "        catchment_folder = 'data/catchment_polygons/updated_polygon_set'\n",
    "        temp_folder = os.path.join(os.getcwd(), catchment_folder, 'temp')\n",
    "        # batch_output_fpath = output_fpath.replace('.geojson', f'_final.geojson')\n",
    "        \n",
    "        if not os.path.exists(temp_folder):\n",
    "            os.makedirs(temp_folder)\n",
    "    \n",
    "        ppt_batch = gpd.read_file(f)\n",
    "        print(f'{len(ppt_batch)} ppts in pour point batch')\n",
    "        print('')\n",
    "            \n",
    "        basin_raster_files = [e for e in os.listdir(catchment_folder) if e.startswith(f'{rc}_basins')]\n",
    "        print(f'   ...processing {rc} basin rasters to vector.')\n",
    "        catchment_batch = []\n",
    "        for bf in sorted(basin_raster_files):\n",
    "            print(f'processing {bf}')\n",
    "            raster_fpath = os.path.join(os.getcwd(), catchment_folder, bf)\n",
    "            assert os.path.exists(raster_fpath)\n",
    "            raster, raster_crs, _ = retrieve_raster(raster_fpath)\n",
    "            resolution = raster.rio.resolution()\n",
    "            \n",
    "            cdf = raster_to_vector_batch(rc, raster_fpath, raster_crs, resolution, min_area, temp_folder)\n",
    "            \n",
    "            catchment_batch.append(cdf)\n",
    "    \n",
    "        batch_df = format_batch_geometries(catchment_batch, ppt_batch, raster_crs)\n",
    "        batch_df.to_file(output_fpath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536b7b3-3097-4838-b133-c9cb64609150",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_files = sorted([e for e in os.listdir('data/catchment_polygons') if e.endswith('basins_BCUB.geojson')])\n",
    "all_dfs = []\n",
    "if process_step:\n",
    "    for f in updated_files:\n",
    "        udf = gpd.read_file(os.path.join('data/catchment_polygons', f))\n",
    "        udf['region_code'] = f.split('/')[-1].split('_')[0]\n",
    "        \n",
    "        for i, row in udf.iterrows():\n",
    "            geom = row['geometry']\n",
    "            \n",
    "            stn_id = row['Official_I']\n",
    "            if stn_id not in bcub_gdf['Official_ID'].values:\n",
    "                raise Exception(f'   ...{stn_id} not found in hysets set')\n",
    "            # update the geometries\n",
    "            bcub_gdf.loc[bcub_gdf['Official_ID'] == stn_id, 'geometry'] = geom\n",
    "            pt = geom.centroid\n",
    "            pt_reproj = gpd.GeoDataFrame(geometry=[pt], crs=3005).to_crs(4326)\n",
    "            lat, lon = pt_reproj.geometry.x[0], pt_reproj.geometry.y[0]\n",
    "            \n",
    "            bcub_gdf.loc[bcub_gdf['Official_ID'] == stn_id, 'Centroid_Lat_deg_N'] = lat\n",
    "            bcub_gdf.loc[bcub_gdf['Official_ID'] == stn_id, 'Centroid_Lon_deg_E'] = lon\n",
    "            bcub_gdf.loc[bcub_gdf['Official_ID'] == stn_id, 'geometry'] = geom\n",
    "            bcub_gdf.loc[bcub_gdf['Official_ID'] == stn_id, 'geometry_updated'] = True\n",
    "            bcub_gdf.loc[bcub_gdf['Official_ID'] == stn_id, 'Flag_Artificial_Boundaries'] = False\n",
    "\n",
    "    ab_df_remaining = bcub_gdf.loc[bcub_gdf['Flag_Artificial_Boundaries'] == 1, :].copy()\n",
    "    print(f'{len(ab_df_remaining)}/{len(bcub_gdf)} catchment geometries remain flagged \"artificial bounds\" ({len(ab_df) - len(ab_df_remaining)} updated.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656d3ef-2558-4969-88b0-3940fcbac160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated_fpath\n",
    "# bcub_gdf.to_file(updated_fpath)\n",
    "print(f'saved updated polygon geometries to {updated_fpath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7bf5c-a2f3-4249-a340-2bd2e9d4b933",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
