{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "```{figure} ../images/figure_1_study_region.png\n",
    "---\n",
    "alt: Study region polygons and HYSETS monitored catchment polygons.\n",
    "name: study-region-fig\n",
    "width: 500px\n",
    "align: center\n",
    "---\n",
    "Study region polygons and WSC + USGS active (green triangles) and historical (yellow triangles) streamflow monitoring stations.  \n",
    "```\n",
    "\n",
    "```{note}\n",
    "Before proceeding with the computations in the notebook, the streamflow time series and (optionally) catchment boundaries from the HYSETS dataset must be downloaded from the [HYSETS open data repository](https://osf.io/rpc3w/).  Some data are provided in the `data/` folder as part of this repository.  Data pre-processing can be skipped by downloading the input data files from (add dataset repository link)\n",
    "```\n",
    "\n",
    "The data used in this study comes from *The Hydrometeorological Sandbox École de Technologie Supérieure* (HYSETS) {cite}`arsenault2020comprehensive`.  The HYSETS data, including streamflow time series and attributes for 14,425 catchments can be accessed at [https://osf.io/rpc3w/](https://osf.io/rpc3w/).  We use a subset of approximately 1620 catchments contained in major basins covering and bounding British Columbia, as shown in {numref}`Figure {number} <study-region-fig>`.  Ten climate indices were processed from [Daymet](https://daymet.ornl.gov/) for this subset, for details see *BCUB - A large sample ungauged basin attribute dataset for British Columbia, Canada* {cite}`kovacek2024bcub` ([https://doi.org/10.5194/essd-2023-508](https://doi.org/10.5194/essd-2023-508)).  \n",
    "\n",
    "\n",
    "For this experiment we use the following files:\n",
    "\n",
    "* **Daily average streamflow time series**: filenames follow the convention `<official_id>.csv`.  These should be downloaded from the open data repository linked above and saved under `data/hysets_streamflow_timeseries/`\n",
    "* **Catchment attributes**: filename: `BCUB_watershed_attributes_updated.csv`.   This file is provided in the `data/` folder, and it was modified from the original file `HYSETS_watershed_properties.txt` in the HYSETS dataset with ten added climate indices as described in {cite}`kovacek2024bcub`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing overview\n",
    "\n",
    "Note that these steps are optional and the end results of these pre-processing steps are provided in the open data repository.\n",
    "\n",
    "### get updated data sources and validate catchment attributes\n",
    "\n",
    "1) Extract catchment attributes using updated catchment geometries where available (optional, updated catchment geometries are saved in `data/BCUB_watershed_bounds_updated.geojson`).\n",
    "2) Process climate indices for HYSETS catchments in the study region (optional, pre-processed attributes are contained in `BCUB_watershed_attributes_updated.csv`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pre-process streamflow data based on key assumptions\n",
    "3) Define sensitivity test parameters:\n",
    "    * **Bitrate:** the number of quantization levels to encode the streamflow time series $N_s = 2^b$.  This is done to test the sensitivity of the predictive model to information loss.   \n",
    "    * **Prior**: when computing the Kullback-Leibler divergence ($D_{KL} (P||Q) = P \\text{log}\\frac{P}{Q}$), the simulated distribution Q can't contain zero probabilities, in other words we must prevent the model from saying observed states are impossible.  This is achieved by applying a prior distribution to q (the simulated series) in order to avoid division by zero.  The KL divergence is then computed on the posterior $Q'$.\n",
    "    * **Minimum record length:** we set the minimum record length to 1 year in order to see the sensitivity of the model to record length,\n",
    "    * **Record \"completeness\":** a (hydrological) year must be at least 90% complete in terms of daily mean observations.\n",
    "    * **Partial counts**: another approach to incorporating measurement uncertainty is to assign a (relative) error interval around each observation.\n",
    "    \n",
    "### Compute f-divergence measures for catchment pairs\n",
    "The quantization step then computes counts based on the proportion of the error interval covered by each bin.  This has a smoothing effect on the discrete PMF, with an increasing effect as the bitrate (number of bins / quantization levels) increases and bin intervals shrink.  \n",
    "\n",
    "4) Compute the (Shannon) entropy of the streamflow time series for each bitrate and prior.  \n",
    "5) Compute the KL divergence (KLD), Earth Mover's Distance (EMD), and total variation distance (TVD) for each pair of stations meeting the minimum record / minimum concurrency criteria.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import HYSETS catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the HYSETS attributes data\n",
    "hysets_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the study region polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BCUB (study) region boundary\n",
    "region_gdf = gpd.read_file('data/BCUB_regions_4326.geojson')\n",
    "region_gdf = region_gdf.to_crs(3005)\n",
    "# simplify the geometries (100m threshold) and add a small buffer (250m) \n",
    "# to capture HYSETS station points recorded with low accuracy near boundaries\n",
    "region_gdf.geometry = region_gdf.simplify(100).buffer(500)\n",
    "region_gdf = region_gdf.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stations contained in the study region\n",
    "centroids = hysets_df.apply(lambda x: Point(x['Centroid_Lon_deg_E'], x['Centroid_Lat_deg_N']), axis=1)\n",
    "hysets_gdf = gpd.GeoDataFrame(hysets_df, geometry=centroids, crs='EPSG:4326')\n",
    "hysets_gdf.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the stations within the study region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hysets_gdf.crs == region_gdf.crs\n",
    "\n",
    "bcub_gdf = gpd.sjoin(hysets_gdf, region_gdf, how='inner', predicate='intersects')\n",
    "print(len(bcub_gdf), len(set(bcub_gdf['Official_ID'])))\n",
    "\n",
    "# Because of the buffer (to capture stations along the coast), \n",
    "# there's a duplicated 08GA065 that should be in 08G\n",
    "bcub_gdf = bcub_gdf.drop_duplicates(subset=['Official_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in two stations in the far north just outside the study region but \n",
    "# important to include since the northern region is so sparsely monitored\n",
    "to_include = ['10ED002', '09AG003']\n",
    "added_stns = hysets_df[hysets_df['Official_ID'].isin(to_include)]\n",
    "added_centroids = added_stns.apply(lambda x: Point(x['Centroid_Lon_deg_E'], x['Centroid_Lat_deg_N']), axis=1)\n",
    "added_gdf = gpd.GeoDataFrame(added_stns, geometry=added_centroids, crs='EPSG:4326')\n",
    "bcub_gdf = gpd.GeoDataFrame(pd.concat([bcub_gdf, added_gdf]), crs='4326')\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '10ED002', 'region_code'] = '10E'\n",
    "bcub_gdf.loc[bcub_gdf['Official_ID'] == '09AG003', 'region_code'] = 'YKR'\n",
    "bcub_gdf.to_file('data/study_region_stations.geojson')\n",
    "\n",
    "# get the number of unique stations in the dataset\n",
    "unique_stations = np.unique(bcub_gdf['Official_ID'])\n",
    "print(f'{len(unique_stations)} unique monitored catchments in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove excluded stations\n",
    "\n",
    "Catchments without geometry published by official sources (WSC, USGS) are validated in the next chapter.  The following stations are excluded because they could not be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_stns = ['15087200', '07FD913', '07FD912', '12113349', '15052475',\n",
    "                '12110400', '15053200', '12212430', '08EG013']\n",
    "bcub_gdf = bcub_gdf[~bcub_gdf['Official_ID'].isin(excluded_stns)]\n",
    "print(len(bcub_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the locations (centroids) of the catchments\n",
    "# convert to geodataframe\n",
    "# convert coordinate reference system to 3857 for plotting\n",
    "gdf = bcub_gdf.copy().to_crs(3857)\n",
    "bbox = gdf.geometry.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the catchment centroid locations\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import Colorblind, Sunset10\n",
    "output_notebook()\n",
    "\n",
    "# range bounds supplied in web mercator coordinates\n",
    "p = figure(x_axis_type=\"mercator\", y_axis_type=\"mercator\", width=700, height=400,\n",
    "          x_range=(bbox[0], bbox[2]), y_range=(bbox[1], bbox[3]))\n",
    "p.add_tile(\"CartoDB Positron\", retina=True)\n",
    "p.scatter(x=gdf.geometry.x, y=gdf.geometry.y, color='orange', size=4)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import streamflow timeseries\n",
    "\n",
    "\n",
    "```{note}\n",
    "At the top of `data_processing_functions.py`, update the `STREAMFLOW_DIR` variable to match where the HYSETS streamflow time series are stored.  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing_functions as dpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading streamflow time series for a pair of monitoring stations\n",
    "s1, s2 = unique_stations[0], unique_stations[1]\n",
    "\n",
    "test_df = dpf.retrieve_nonconcurrent_data(s1, s2)\n",
    "flow_fig = figure(width=700, height=350, x_axis_type='datetime')\n",
    "flow_fig.line(test_df.index, test_df[unique_stations[0]], color='navy', legend_label=unique_stations[0])\n",
    "flow_fig.line(test_df.index, test_df[unique_stations[1]], color='dodgerblue', legend_label=unique_stations[1])\n",
    "flow_fig.yaxis.axis_label = r'$$\\text{Flow } \\frac{m^3}{s}$$'\n",
    "flow_fig.xaxis.axis_label = r'$$\\text{Date}$$'\n",
    "show(flow_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In the subsequent chapters, the monitoring network station catchments are updated, and the catchment attributes are re-extracted and compared against the HYSETS values.  The target variables are then derived from the streamflow time series before training the gradient boosting models to test their predictability from catchment attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
